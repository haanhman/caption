{"0":{"dur":4,"text":"Welcome to another Skymind screen cast"},"4":{"dur":2,"text":"Our screen cast consists of"},"6":{"dur":2,"text":"me performing, writing some codes"},"8":{"dur":2,"text":"and demonstrating some examples"},"11":{"dur":2,"text":"with voice."},"13":{"dur":4,"text":"In this case I am going demonstrate using the Datavec toolkit"},"17":{"dur":4,"text":"to read some comma-separated data"},"21":{"dur":3,"text":"to select fields from the comma-separated data"},"25":{"dur":3,"text":"and transform the comma-separated data"},"28":{"dur":2,"text":"into a numerical format"},"31":{"dur":4,"text":"from a string format. So that it can be used in a neural net"},"35":{"dur":2,"text":"The data file and the Java code"},"37":{"dur":4,"text":"are available at this github repo"},"41":{"dur":2,"text":"Thank you for watching. Enjoy."},"45":{"dur":4,"text":"In this screen cast, I am going to demonstrate using datavec"},"49":{"dur":7,"text":"to parse a comma-separated file, and extract values for use in deep learning"},"58":{"dur":5,"text":"The file I am going to be working with, and I will share this with you, if you would like to work through this example yourself"},"63":{"dur":4,"text":"is reports.csv"},"67":{"dur":3,"text":"and this is just slightly reformatted data"},"71":{"dur":6,"text":"from the storm prediction center with the National Weather Service here in the U.S."},"78":{"dur":6,"text":"the format is year, month, day"},"84":{"dur":7,"text":"dash, hour hour, minute minute, the severity of the report"},"91":{"dur":3,"text":"the location of the report"},"94":{"dur":5,"text":"the county of the report, the state of the report"},"99":{"dur":3,"text":"the latitude and the longitude"},"102":{"dur":3,"text":"a comment, and the type of the report"},"105":{"dur":3,"text":"whether it was a tornado, whether it was"},"108":{"dur":3,"text":"if you scroll down, whether it was a wind report"},"111":{"dur":4,"text":"and if you scroll down further, whether it was a hail report"},"115":{"dur":3,"text":"the severity field, the second field"},"118":{"dur":3,"text":"if it was a hail report, it would be the size of the hail stone"},"121":{"dur":4,"text":"if it is a wind report, it will be the speed of the wind"},"125":{"dur":5,"text":"if it was a tornado report , it will be the Fujita rating"},"130":{"dur":2,"text":"for the report"},"132":{"dur":2,"text":"The data I would like to extract out of this"},"134":{"dur":2,"text":"for deep learning and for this example"},"136":{"dur":4,"text":"is I want to extract the"},"140":{"dur":2,"text":"latitude and longitude"},"144":{"dur":3,"text":"and I also want to extract"},"147":{"dur":6,"text":"the type of the report. Whether it is tornado, wind or hail"},"153":{"dur":3,"text":"and the tool I am going to use to do that is datavec"},"156":{"dur":4,"text":"and I am gonna move to writting some java, and we will demonstrate this"},"160":{"dur":2,"text":"so I gonna take the data from the original format"},"163":{"dur":4,"text":"to this format, using a datavec transform"},"167":{"dur":7,"text":"using the instruction that you can find here in the deeplearning4j website, in the quick start guide"},"175":{"dur":6,"text":"will help you setup an inteliJ environment that you could following along and build this example"},"181":{"dur":6,"text":"so what I am going to do is in the dl4j-spark-local example"},"187":{"dur":5,"text":"I gonna go ahead and add a new class"},"194":{"dur":9,"text":"so I gonna add a new java class, and I gonna call this storm reports record reader"},"207":{"dur":4,"text":"lets go ahead and start writing our storm report record reader"},"211":{"dur":5,"text":"I gonna switch to presentation mode so that the code is a little bit easier to read"},"218":{"dur":2,"text":"we will need our main"},"224":{"dur":6,"text":"the data I showed you in that report.csv file, it did not have a header"},"230":{"dur":2,"text":"so when I load the data"},"232":{"dur":6,"text":"I gonna specify the numLinesToSkip"},"238":{"dur":6,"text":"is 0. I think I will just write here at the top"},"244":{"dur":3,"text":"the data was comma-delimited"},"261":{"dur":3,"text":"the next step to write some codes that specify"},"265":{"dur":2,"text":"where we gonna read our data from"},"267":{"dur":3,"text":"and where we gonna write our data to"},"270":{"dur":2,"text":"so this just specify the baseDir"},"272":{"dur":4,"text":"and once again this is the line you gonna have to change"},"276":{"dur":3,"text":"if you do this exercise at home"},"279":{"dur":2,"text":"specify the file name"},"281":{"dur":4,"text":"combine it to get an input path"},"285":{"dur":5,"text":"so we can run them multiple times and not having files collide"},"291":{"dur":3,"text":"I am going to append a time stamp"},"294":{"dur":3,"text":"to each of the output directory"},"297":{"dur":2,"text":"so we gonna write to our baseDir"},"300":{"dur":2,"text":"a file called reports_processed"},"302":{"dur":4,"text":"and then the time stamp of when this is executed"},"307":{"dur":3,"text":"I have added a comment to the code"},"310":{"dur":5,"text":"that shows what the data looks like as a sample record"},"315":{"dur":5,"text":"and notes the field datetime, severity, location etc."},"320":{"dur":6,"text":"so I can refer to that as I build the tools to parse it"},"326":{"dur":2,"text":"so"},"328":{"dur":4,"text":"in order to parse this data, we start by creating a Schema"},"332":{"dur":4,"text":"I gonna call this inputDataSchema"},"338":{"dur":6,"text":"and I gonna use schema Schema.Builder to build it"},"346":{"dur":4,"text":"the first thing we have is a collection of string columns"},"350":{"dur":5,"text":"so I gonna use this addColumnsString"},"355":{"dur":2,"text":"here it is"},"357":{"dur":3,"text":"so we have"},"360":{"dur":3,"text":"datetime, which we are going to treat it as a String"},"363":{"dur":4,"text":"severity, location, county"},"367":{"dur":3,"text":"and state, are all String"},"371":{"dur":2,"text":"so we gonna use"},"375":{"dur":5,"text":"addColumnsSgtring to specify that"},"394":{"dur":5,"text":"the next two columns, are the columns that will actually be extracting"},"399":{"dur":6,"text":"the latitude and the longitude in those can be represented as Double"},"413":{"dur":3,"text":"the next column is a String column"},"416":{"dur":4,"text":"so we will add columns of String"},"422":{"dur":2,"text":"and that is that comment field"},"424":{"dur":6,"text":"and then the last column is the type whether that is a tornado report, hail report"},"431":{"dur":2,"text":"or a wind report"},"433":{"dur":4,"text":"and that is actually categorical column"},"437":{"dur":2,"text":"so I gonna specify"},"439":{"dur":6,"text":"that the field we gonna call type, can be one of the following String"},"445":{"dur":7,"text":"\"TOR\" , \"WIND\", or \"HAIL\""},"453":{"dur":4,"text":"and then we can go ahead and build"},"457":{"dur":3,"text":"so that is our inputDataSchema"},"460":{"dur":3,"text":"I have written a comment that defines our next step"},"464":{"dur":5,"text":"we define our input schema, now we need to define a transform process"},"469":{"dur":3,"text":"that extracts the latitude and longitude"},"472":{"dur":2,"text":"and converts this type"},"474":{"dur":7,"text":"to either a  \"0\", \"1\" or \"3\""},"481":{"dur":4,"text":"so let's go ahead and write that process"},"485":{"dur":4,"text":"so the tool to do that is we define a new"},"489":{"dur":2,"text":"TransformProcess from datavec"},"491":{"dur":4,"text":"I gonna call that \"tp\""},"497":{"dur":3,"text":"and we gonna use trasnformProcess.Builder"},"500":{"dur":4,"text":"and we gonna start with inputDataSchema"},"504":{"dur":4,"text":"the first thing I need to do is specify that I want to remove some columns"},"508":{"dur":3,"text":"and the columns that I want to remove"},"511":{"dur":9,"text":"would be \"datetime\" , \"severity\" , \" location\", \"county\", \"state\" and \"comment\""},"523":{"dur":5,"text":"so we gonna remove those, and lets add comment to that"},"531":{"dur":3,"text":"and then I need to specify"},"534":{"dur":2,"text":"what remains"},"536":{"dur":3,"text":"so \"latitude\" and \"longitude\" are already Double"},"539":{"dur":3,"text":"those are fine. We need numeric data for our neural net"},"543":{"dur":3,"text":"I need to specify that I want to"},"546":{"dur":4,"text":"convert categorical to integer"},"552":{"dur":4,"text":"and that would be the type field"},"560":{"dur":2,"text":"and that's it"},"562":{"dur":2,"text":"and then build"},"566":{"dur":4,"text":"so that is our transform process"},"570":{"dur":2,"text":"at this point"},"572":{"dur":3,"text":"we have described the input scheme"},"575":{"dur":3,"text":"the way the data stored on disc"},"578":{"dur":4,"text":"and then our transform process that removes columns"},"582":{"dur":2,"text":"maintains some columns as they are"},"585":{"dur":4,"text":"and then converts a column to a numeric type"},"589":{"dur":5,"text":"this little snippet of code is going to step through our transform process"},"594":{"dur":2,"text":"however many steps they are"},"596":{"dur":4,"text":"and for each step it's gonna show the schema after step"},"600":{"dur":4,"text":"so we just gonna print the output of our before and after schema"},"604":{"dur":3,"text":"this would be a good time to pause and run the code"},"607":{"dur":3,"text":"make sure that everything up till now is good"},"610":{"dur":6,"text":"and now this gonna do is to print the before and after schema"},"619":{"dur":2,"text":"here we see. After step 0"},"621":{"dur":6,"text":"we have gotten rid all the columns except for latitude, longitude and type"},"627":{"dur":2,"text":"and then the next step"},"629":{"dur":3,"text":"takes that categorical field type"},"633":{"dur":2,"text":"and converts it into integer"},"635":{"dur":4,"text":"of either \"0\", \"1\", or \"2\""},"639":{"dur":5,"text":"so our transform process looks good"},"646":{"dur":4,"text":"once we have written and printed out"},"650":{"dur":4,"text":"our transformation steps. It is time to execute them"},"654":{"dur":4,"text":"and when we do that, the current implementation is spark only."},"659":{"dur":3,"text":"so we will need to use spark"},"662":{"dur":4,"text":"spark is rather straightforward to setup. We are going to run local spark"},"666":{"dur":6,"text":"so you do not need to execute it in a cluster. You certainly could, but in this case, we are not"},"672":{"dur":4,"text":"so I need to set up a SparkConf"},"693":{"dur":4,"text":"Once I have setup a sparkConf, I need to set the master."},"698":{"dur":5,"text":"I am call a setter on the master"},"707":{"dur":4,"text":"we need to set the master to \"local\""},"716":{"dur":2,"text":"when a spark application is running"},"718":{"dur":1,"text":"we can set an application name"},"720":{"dur":5,"text":"so when we are look at the spark user interface, we can see that this job is running"},"725":{"dur":4,"text":"and see what resources that it was using"},"729":{"dur":7,"text":"and we are going to call this the storm record reader transform"},"740":{"dur":2,"text":"now that we have set"},"743":{"dur":5,"text":"local configuration execute on local spark master, and that we have given the application a name"},"748":{"dur":4,"text":"we need to create a spark context"},"752":{"dur":5,"text":"so we create a new context, and pass it our configuration"},"758":{"dur":3,"text":"once we specify our spark configuration"},"762":{"dur":6,"text":"then we need to work with spark by creating these JavaRDDs"},"768":{"dur":5,"text":"it is a in memory representation of our data. So the first one we create called lines"},"773":{"dur":3,"text":"that's gonna contain String data"},"776":{"dur":4,"text":"and its gonna be generated by reading the text file from our input path"},"780":{"dur":3,"text":"so that reads our data in"},"784":{"dur":2,"text":"then we need to convert that data"},"786":{"dur":2,"text":"to collection of writable"},"788":{"dur":4,"text":"for our record reader to process"},"794":{"dur":4,"text":"so this RDD, stormReports, is going to be generated"},"798":{"dur":5,"text":"by running this map function, a \"StringToWritables\""},"804":{"dur":2,"text":"across the data set"},"806":{"dur":5,"text":"so we are reading this String, convert them to writables, and that is at this step"},"812":{"dur":3,"text":"now that we have them in datavec writables"},"815":{"dur":5,"text":"we are gonna create a new RDD, that is gonna contain a list of writables"},"821":{"dur":3,"text":"we gonna call that \"processed\""},"824":{"dur":5,"text":"and it is gonna generated by running our transform executor"},"829":{"dur":5,"text":"against this last RDD, against  stormReports"},"834":{"dur":6,"text":"and we gonna pass that transform executor, our transform process that we defined up above"},"842":{"dur":4,"text":"right there"},"847":{"dur":3,"text":"so at this point, we have converted our data"},"850":{"dur":3,"text":"to writable, so that we could pass them"},"853":{"dur":5,"text":"onto a deeplearning neural network, building in the array from them"},"858":{"dur":4,"text":"but in our case, we gonna go ahead and write them"},"862":{"dur":2,"text":"back to disc"},"864":{"dur":5,"text":"so I gonna create new RDD, call \"toSave\""},"869":{"dur":2,"text":"and we are gonna map"},"871":{"dur":4,"text":"the Writables back to String. So we convert the String to Writables"},"876":{"dur":3,"text":"run our transformer across those writables"},"879":{"dur":7,"text":"and then convert our Writables back to String delineate them with commas"},"886":{"dur":3,"text":"then all that left to do"},"889":{"dur":3,"text":"is to take our toSave"},"892":{"dur":3,"text":"our last RDD"},"895":{"dur":2,"text":"and save that as text file"},"897":{"dur":4,"text":"and then specify our output path"},"909":{"dur":2,"text":"all that is left to do is to run our code"},"912":{"dur":6,"text":"and when I went to do that, I realized I had a typo here in my path"},"921":{"dur":4,"text":"so I fixed that typo and will go ahead and run that code"},"926":{"dur":2,"text":"so it should read our data"},"930":{"dur":2,"text":"should output our schema"},"934":{"dur":4,"text":"and get a bunch of info"},"938":{"dur":3,"text":"log notices from spark"},"941":{"dur":3,"text":"and it looks like it is completed"},"944":{"dur":2,"text":"let's take a look"},"947":{"dur":4,"text":"so when this code executed, it created a directory"},"952":{"dur":5,"text":"instead of a file, spark's distributed framework can have multiple workers write into the same file"},"957":{"dur":3,"text":"so each write their output file into a directory"},"960":{"dur":4,"text":"so when you specify an output path in spark, it is always a directory"},"965":{"dur":2,"text":"we can see that there"},"969":{"dur":6,"text":"and we see 0 length success file, and then two part files"},"977":{"dur":5,"text":"so let's go ahead and take a look at one of the part files"},"983":{"dur":4,"text":"and then we see what we had hoped for, we see latitude"},"987":{"dur":2,"text":"longitude"},"989":{"dur":3,"text":"\"0\" for tornadoes"},"992":{"dur":5,"text":"and so we see the first 20 records are the tornadoes"},"997":{"dur":4,"text":"and then we see the beginning of the wind reports"},"1001":{"dur":2,"text":"\"1\" for wind"},"1003":{"dur":4,"text":"and then the second part file"},"1007":{"dur":4,"text":"we will see the rest of the wind reports"},"1011":{"dur":2,"text":"and then the hail reports"},"1016":{"dur":5,"text":"so congratulations, we have successfully transformed a data set"},"1021":{"dur":3,"text":"into a format that can be ingested"},"1024":{"dur":2,"text":"into our neural net"},"1026":{"dur":2,"text":"Thank you"}}