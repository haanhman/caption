{"0.00":{"start":"0","dur":"4","text":"Welcome to another Skymind screen cast"},"4.00":{"start":"4","dur":"2.8","text":"Our screen cast consists of"},"6.80":{"start":"6.8","dur":"2","text":"me performing, writing some codes"},"8.80":{"start":"8.8","dur":"2.5","text":"and demonstrating some examples"},"11.30":{"start":"11.3","dur":"2","text":"with voice."},"13.30":{"start":"13.3","dur":"4.5","text":"In this case I am going demonstrate using the Datavec toolkit"},"17.80":{"start":"17.8","dur":"4","text":"to read some comma-separated data"},"21.80":{"start":"21.8","dur":"3.5","text":"to select fields from the comma-separated data"},"25.30":{"start":"25.3","dur":"3.5","text":"and transform the comma-separated data"},"28.80":{"start":"28.8","dur":"2.22","text":"into a numerical format"},"31.02":{"start":"31.02","dur":"4.5","text":"from a string format. So that it can be used in a neural net"},"35.52":{"start":"35.52","dur":"2","text":"The data file and the Java code"},"37.52":{"start":"37.52","dur":"4","text":"are available at this github repo"},"41.52":{"start":"41.52","dur":"2.3","text":"Thank you for watching. Enjoy."},"45.32":{"start":"45.32","dur":"4.5","text":"In this screen cast, I am going to demonstrate using datavec"},"49.82":{"start":"49.82","dur":"7.5","text":"to parse a comma-separated file, and extract values for use in deep learning"},"58.32":{"start":"58.32","dur":"5.5","text":"The file I am going to be working with, and I will share this with you, if you would like to work through this example yourself"},"63.82":{"start":"63.82","dur":"4","text":"is reports.csv"},"67.82":{"start":"67.82","dur":"3.3","text":"and this is just slightly reformatted data"},"71.12":{"start":"71.12","dur":"6.9","text":"from the storm prediction center with the National Weather Service here in the U.S."},"78.02":{"start":"78.02","dur":"6","text":"the format is year, month, day"},"84.02":{"start":"84.02","dur":"7","text":"dash, hour hour, minute minute, the severity of the report"},"91.02":{"start":"91.02","dur":"3","text":"the location of the report"},"94.02":{"start":"94.02","dur":"5","text":"the county of the report, the state of the report"},"99.02":{"start":"99.02","dur":"3","text":"the latitude and the longitude"},"102.02":{"start":"102.02","dur":"3.5","text":"a comment, and the type of the report"},"105.52":{"start":"105.52","dur":"3","text":"whether it was a tornado, whether it was"},"108.52":{"start":"108.52","dur":"3","text":"if you scroll down, whether it was a wind report"},"111.52":{"start":"111.52","dur":"4","text":"and if you scroll down further, whether it was a hail report"},"115.52":{"start":"115.52","dur":"3","text":"the severity field, the second field"},"118.52":{"start":"118.52","dur":"3","text":"if it was a hail report, it would be the size of the hail stone"},"121.52":{"start":"121.52","dur":"4","text":"if it is a wind report, it will be the speed of the wind"},"125.52":{"start":"125.52","dur":"5","text":"if it was a tornado report , it will be the Fujita rating"},"130.52":{"start":"130.52","dur":"2","text":"for the report"},"132.52":{"start":"132.52","dur":"2.3","text":"The data I would like to extract out of this"},"134.82":{"start":"134.82","dur":"2","text":"for deep learning and for this example"},"136.82":{"start":"136.82","dur":"4","text":"is I want to extract the"},"140.82":{"start":"140.82","dur":"2","text":"latitude and longitude"},"144.82":{"start":"144.82","dur":"3","text":"and I also want to extract"},"147.82":{"start":"147.82","dur":"6","text":"the type of the report. Whether it is tornado, wind or hail"},"153.82":{"start":"153.82","dur":"3","text":"and the tool I am going to use to do that is datavec"},"156.82":{"start":"156.82","dur":"4","text":"and I am gonna move to writting some java, and we will demonstrate this"},"160.82":{"start":"160.82","dur":"2.4","text":"so I gonna take the data from the original format"},"163.22":{"start":"163.22","dur":"4","text":"to this format, using a datavec transform"},"167.72":{"start":"167.72","dur":"7.6","text":"using the instruction that you can find here in the deeplearning4j website, in the quick start guide"},"175.32":{"start":"175.32","dur":"6","text":"will help you setup an inteliJ environment that you could following along and build this example"},"181.32":{"start":"181.32","dur":"6","text":"so what I am going to do is in the dl4j-spark-local example"},"187.32":{"start":"187.32","dur":"5","text":"I gonna go ahead and add a new class"},"194.32":{"start":"194.32","dur":"9","text":"so I gonna add a new java class, and I gonna call this storm reports record reader"},"207.32":{"start":"207.32","dur":"4","text":"lets go ahead and start writing our storm report record reader"},"211.32":{"start":"211.32","dur":"5","text":"I gonna switch to presentation mode so that the code is a little bit easier to read"},"218.32":{"start":"218.32","dur":"2","text":"we will need our main"},"224.32":{"start":"224.32","dur":"6","text":"the data I showed you in that report.csv file, it did not have a header"},"230.32":{"start":"230.32","dur":"2","text":"so when I load the data"},"232.32":{"start":"232.32","dur":"6","text":"I gonna specify the numLinesToSkip"},"238.32":{"start":"238.32","dur":"6","text":"is 0. I think I will just write here at the top"},"244.32":{"start":"244.32","dur":"3","text":"the data was comma-delimited"},"261.32":{"start":"261.32","dur":"3.9","text":"the next step to write some codes that specify"},"265.22":{"start":"265.22","dur":"2","text":"where we gonna read our data from"},"267.22":{"start":"267.22","dur":"3","text":"and where we gonna write our data to"},"270.22":{"start":"270.22","dur":"2.5","text":"so this just specify the baseDir"},"272.72":{"start":"272.72","dur":"4","text":"and once again this is the line you gonna have to change"},"276.72":{"start":"276.72","dur":"3","text":"if you do this exercise at home"},"279.72":{"start":"279.72","dur":"2","text":"specify the file name"},"281.72":{"start":"281.72","dur":"4","text":"combine it to get an input path"},"285.72":{"start":"285.72","dur":"5.5","text":"so we can run them multiple times and not having files collide"},"291.22":{"start":"291.22","dur":"3","text":"I am going to append a time stamp"},"294.22":{"start":"294.22","dur":"3.5","text":"to each of the output directory"},"297.72":{"start":"297.72","dur":"2.4","text":"so we gonna write to our baseDir"},"300.12":{"start":"300.12","dur":"2.5","text":"a file called reports_processed"},"302.62":{"start":"302.62","dur":"4.5","text":"and then the time stamp of when this is executed"},"307.12":{"start":"307.12","dur":"3.4","text":"I have added a comment to the code"},"310.52":{"start":"310.52","dur":"5","text":"that shows what the data looks like as a sample record"},"315.52":{"start":"315.52","dur":"5","text":"and notes the field datetime, severity, location etc."},"320.52":{"start":"320.52","dur":"6","text":"so I can refer to that as I build the tools to parse it"},"326.52":{"start":"326.52","dur":"2","text":"so"},"328.52":{"start":"328.52","dur":"4","text":"in order to parse this data, we start by creating a Schema"},"332.52":{"start":"332.52","dur":"4","text":"I gonna call this inputDataSchema"},"338.52":{"start":"338.52","dur":"6","text":"and I gonna use schema Schema.Builder to build it"},"346.52":{"start":"346.52","dur":"4","text":"the first thing we have is a collection of string columns"},"350.52":{"start":"350.52","dur":"5","text":"so I gonna use this addColumnsString"},"355.52":{"start":"355.52","dur":"2","text":"here it is"},"357.52":{"start":"357.52","dur":"3","text":"so we have"},"360.52":{"start":"360.52","dur":"3","text":"datetime, which we are going to treat it as a String"},"363.52":{"start":"363.52","dur":"4","text":"severity, location, county"},"367.52":{"start":"367.52","dur":"3.5","text":"and state, are all String"},"371.02":{"start":"371.02","dur":"2","text":"so we gonna use"},"375.02":{"start":"375.02","dur":"5","text":"addColumnsSgtring to specify that"},"394.02":{"start":"394.02","dur":"5","text":"the next two columns, are the columns that will actually be extracting"},"399.02":{"start":"399.02","dur":"6","text":"the latitude and the longitude in those can be represented as Double"},"413.02":{"start":"413.02","dur":"3.3","text":"the next column is a String column"},"416.32":{"start":"416.32","dur":"4","text":"so we will add columns of String"},"422.32":{"start":"422.32","dur":"2.4","text":"and that is that comment field"},"424.72":{"start":"424.72","dur":"6.5","text":"and then the last column is the type whether that is a tornado report, hail report"},"431.22":{"start":"431.22","dur":"2","text":"or a wind report"},"433.22":{"start":"433.22","dur":"4","text":"and that is actually categorical column"},"437.22":{"start":"437.22","dur":"2","text":"so I gonna specify"},"439.22":{"start":"439.22","dur":"6.5","text":"that the field we gonna call type, can be one of the following String"},"445.72":{"start":"445.72","dur":"7.5","text":"&quot;TOR&quot; , &quot;WIND&quot;, or &quot;HAIL&quot;"},"453.22":{"start":"453.22","dur":"4","text":"and then we can go ahead and build"},"457.22":{"start":"457.22","dur":"3.5","text":"so that is our inputDataSchema"},"460.72":{"start":"460.72","dur":"3.5","text":"I have written a comment that defines our next step"},"464.22":{"start":"464.22","dur":"5","text":"we define our input schema, now we need to define a transform process"},"469.22":{"start":"469.22","dur":"3","text":"that extracts the latitude and longitude"},"472.22":{"start":"472.22","dur":"2","text":"and converts this type"},"474.22":{"start":"474.22","dur":"7","text":"to either a  &quot;0&quot;, &quot;1&quot; or &quot;3&quot;"},"481.22":{"start":"481.22","dur":"4","text":"so let&#39;s go ahead and write that process"},"485.22":{"start":"485.22","dur":"4","text":"so the tool to do that is we define a new"},"489.22":{"start":"489.22","dur":"2","text":"TransformProcess from datavec"},"491.22":{"start":"491.22","dur":"4","text":"I gonna call that &quot;tp&quot;"},"497.22":{"start":"497.22","dur":"3","text":"and we gonna use trasnformProcess.Builder"},"500.22":{"start":"500.22","dur":"4","text":"and we gonna start with inputDataSchema"},"504.22":{"start":"504.22","dur":"4","text":"the first thing I need to do is specify that I want to remove some columns"},"508.22":{"start":"508.22","dur":"3.5","text":"and the columns that I want to remove"},"511.72":{"start":"511.72","dur":"9.5","text":"would be &quot;datetime&quot; , &quot;severity&quot; , &quot; location&quot;, &quot;county&quot;, &quot;state&quot; and &quot;comment&quot;"},"523.22":{"start":"523.22","dur":"5","text":"so we gonna remove those, and lets add comment to that"},"531.22":{"start":"531.22","dur":"3","text":"and then I need to specify"},"534.22":{"start":"534.22","dur":"2","text":"what remains"},"536.22":{"start":"536.22","dur":"3","text":"so &quot;latitude&quot; and &quot;longitude&quot; are already Double"},"539.22":{"start":"539.22","dur":"3.8","text":"those are fine. We need numeric data for our neural net"},"543.02":{"start":"543.02","dur":"3.5","text":"I need to specify that I want to"},"546.52":{"start":"546.52","dur":"4","text":"convert categorical to integer"},"552.52":{"start":"552.52","dur":"4","text":"and that would be the type field"},"560.52":{"start":"560.52","dur":"2","text":"and that&#39;s it"},"562.52":{"start":"562.52","dur":"2","text":"and then build"},"566.52":{"start":"566.52","dur":"4","text":"so that is our transform process"},"570.52":{"start":"570.52","dur":"2","text":"at this point"},"572.52":{"start":"572.52","dur":"3","text":"we have described the input scheme"},"575.52":{"start":"575.52","dur":"3","text":"the way the data stored on disc"},"578.52":{"start":"578.52","dur":"4","text":"and then our transform process that removes columns"},"582.52":{"start":"582.52","dur":"2.5","text":"maintains some columns as they are"},"585.02":{"start":"585.02","dur":"4","text":"and then converts a column to a numeric type"},"589.02":{"start":"589.02","dur":"5.3","text":"this little snippet of code is going to step through our transform process"},"594.32":{"start":"594.32","dur":"2","text":"however many steps they are"},"596.32":{"start":"596.32","dur":"4.5","text":"and for each step it&#39;s gonna show the schema after step"},"600.82":{"start":"600.82","dur":"4","text":"so we just gonna print the output of our before and after schema"},"604.82":{"start":"604.82","dur":"3","text":"this would be a good time to pause and run the code"},"607.82":{"start":"607.82","dur":"3","text":"make sure that everything up till now is good"},"610.82":{"start":"610.82","dur":"6","text":"and now this gonna do is to print the before and after schema"},"619.32":{"start":"619.32","dur":"2.5","text":"here we see. After step 0"},"621.82":{"start":"621.82","dur":"6","text":"we have gotten rid all the columns except for latitude, longitude and type"},"627.82":{"start":"627.82","dur":"2","text":"and then the next step"},"629.82":{"start":"629.82","dur":"3.5","text":"takes that categorical field type"},"633.32":{"start":"633.32","dur":"2.5","text":"and converts it into integer"},"635.82":{"start":"635.82","dur":"4","text":"of either &quot;0&quot;, &quot;1&quot;, or &quot;2&quot;"},"639.82":{"start":"639.82","dur":"5","text":"so our transform process looks good"},"646.42":{"start":"646.42","dur":"4.4","text":"once we have written and printed out"},"650.82":{"start":"650.82","dur":"4","text":"our transformation steps. It is time to execute them"},"654.82":{"start":"654.82","dur":"4.5","text":"and when we do that, the current implementation is spark only."},"659.32":{"start":"659.32","dur":"3","text":"so we will need to use spark"},"662.32":{"start":"662.32","dur":"4.2","text":"spark is rather straightforward to setup. We are going to run local spark"},"666.52":{"start":"666.52","dur":"6","text":"so you do not need to execute it in a cluster. You certainly could, but in this case, we are not"},"672.52":{"start":"672.52","dur":"4","text":"so I need to set up a SparkConf"},"693.52":{"start":"693.52","dur":"4.8","text":"Once I have setup a sparkConf, I need to set the master."},"698.32":{"start":"698.32","dur":"5","text":"I am call a setter on the master"},"707.32":{"start":"707.32","dur":"4","text":"we need to set the master to &quot;local&quot;"},"716.42":{"start":"716.42","dur":"2.4","text":"when a spark application is running"},"718.82":{"start":"718.82","dur":"1.7","text":"we can set an application name"},"720.52":{"start":"720.52","dur":"5.3","text":"so when we are look at the spark user interface, we can see that this job is running"},"725.82":{"start":"725.82","dur":"4","text":"and see what resources that it was using"},"729.82":{"start":"729.82","dur":"7","text":"and we are going to call this the storm record reader transform"},"740.82":{"start":"740.82","dur":"2.5","text":"now that we have set"},"743.32":{"start":"743.32","dur":"5.5","text":"local configuration execute on local spark master, and that we have given the application a name"},"748.82":{"start":"748.82","dur":"4","text":"we need to create a spark context"},"752.82":{"start":"752.82","dur":"5","text":"so we create a new context, and pass it our configuration"},"758.82":{"start":"758.82","dur":"3.5","text":"once we specify our spark configuration"},"762.32":{"start":"762.32","dur":"6","text":"then we need to work with spark by creating these JavaRDDs"},"768.32":{"start":"768.32","dur":"5.6","text":"it is a in memory representation of our data. So the first one we create called lines"},"773.92":{"start":"773.92","dur":"3","text":"that&#39;s gonna contain String data"},"776.92":{"start":"776.92","dur":"4","text":"and its gonna be generated by reading the text file from our input path"},"780.92":{"start":"780.92","dur":"3.3","text":"so that reads our data in"},"784.22":{"start":"784.22","dur":"2.7","text":"then we need to convert that data"},"786.92":{"start":"786.92","dur":"2","text":"to collection of writable"},"788.92":{"start":"788.92","dur":"4","text":"for our record reader to process"},"794.72":{"start":"794.72","dur":"4","text":"so this RDD, stormReports, is going to be generated"},"798.72":{"start":"798.72","dur":"5","text":"by running this map function, a &quot;StringToWritables&quot;"},"804.72":{"start":"804.72","dur":"2","text":"across the data set"},"806.72":{"start":"806.72","dur":"5","text":"so we are reading this String, convert them to writables, and that is at this step"},"812.72":{"start":"812.72","dur":"3","text":"now that we have them in datavec writables"},"815.72":{"start":"815.72","dur":"5.6","text":"we are gonna create a new RDD, that is gonna contain a list of writables"},"821.32":{"start":"821.32","dur":"3","text":"we gonna call that &quot;processed&quot;"},"824.32":{"start":"824.32","dur":"5.3","text":"and it is gonna generated by running our transform executor"},"829.62":{"start":"829.62","dur":"5","text":"against this last RDD, against  stormReports"},"834.62":{"start":"834.62","dur":"6","text":"and we gonna pass that transform executor, our transform process that we defined up above"},"842.62":{"start":"842.62","dur":"4","text":"right there"},"847.62":{"start":"847.62","dur":"3","text":"so at this point, we have converted our data"},"850.62":{"start":"850.62","dur":"3","text":"to writable, so that we could pass them"},"853.62":{"start":"853.62","dur":"5","text":"onto a deeplearning neural network, building in the array from them"},"858.62":{"start":"858.62","dur":"4","text":"but in our case, we gonna go ahead and write them"},"862.62":{"start":"862.62","dur":"2","text":"back to disc"},"864.62":{"start":"864.62","dur":"5","text":"so I gonna create new RDD, call &quot;toSave&quot;"},"869.62":{"start":"869.62","dur":"2","text":"and we are gonna map"},"871.62":{"start":"871.62","dur":"4.5","text":"the Writables back to String. So we convert the String to Writables"},"876.12":{"start":"876.12","dur":"3","text":"run our transformer across those writables"},"879.12":{"start":"879.12","dur":"7","text":"and then convert our Writables back to String delineate them with commas"},"886.12":{"start":"886.12","dur":"3","text":"then all that left to do"},"889.12":{"start":"889.12","dur":"3","text":"is to take our toSave"},"892.12":{"start":"892.12","dur":"3.5","text":"our last RDD"},"895.62":{"start":"895.62","dur":"2","text":"and save that as text file"},"897.62":{"start":"897.62","dur":"4","text":"and then specify our output path"},"909.62":{"start":"909.62","dur":"2.5","text":"all that is left to do is to run our code"},"912.12":{"start":"912.12","dur":"6","text":"and when I went to do that, I realized I had a typo here in my path"},"921.12":{"start":"921.12","dur":"4","text":"so I fixed that typo and will go ahead and run that code"},"926.72":{"start":"926.72","dur":"2","text":"so it should read our data"},"930.72":{"start":"930.72","dur":"2","text":"should output our schema"},"934.72":{"start":"934.72","dur":"4","text":"and get a bunch of info"},"938.72":{"start":"938.72","dur":"3","text":"log notices from spark"},"941.72":{"start":"941.72","dur":"3","text":"and it looks like it is completed"},"944.72":{"start":"944.72","dur":"2","text":"let&#39;s take a look"},"947.72":{"start":"947.72","dur":"4.5","text":"so when this code executed, it created a directory"},"952.22":{"start":"952.22","dur":"5.6","text":"instead of a file, spark&#39;s distributed framework can have multiple workers write into the same file"},"957.82":{"start":"957.82","dur":"3","text":"so each write their output file into a directory"},"960.82":{"start":"960.82","dur":"4.5","text":"so when you specify an output path in spark, it is always a directory"},"965.32":{"start":"965.32","dur":"2","text":"we can see that there"},"969.32":{"start":"969.32","dur":"6","text":"and we see 0 length success file, and then two part files"},"977.32":{"start":"977.32","dur":"5","text":"so let&#39;s go ahead and take a look at one of the part files"},"983.22":{"start":"983.22","dur":"4","text":"and then we see what we had hoped for, we see latitude"},"987.22":{"start":"987.22","dur":"2","text":"longitude"},"989.22":{"start":"989.22","dur":"3","text":"&quot;0&quot; for tornadoes"},"992.22":{"start":"992.22","dur":"5","text":"and so we see the first 20 records are the tornadoes"},"997.22":{"start":"997.22","dur":"4","text":"and then we see the beginning of the wind reports"},"1001.22":{"start":"1001.22","dur":"2","text":"&quot;1&quot; for wind"},"1003.22":{"start":"1003.22","dur":"4","text":"and then the second part file"},"1007.22":{"start":"1007.22","dur":"4","text":"we will see the rest of the wind reports"},"1011.22":{"start":"1011.22","dur":"2","text":"and then the hail reports"},"1016.22":{"start":"1016.22","dur":"5","text":"so congratulations, we have successfully transformed a data set"},"1021.22":{"start":"1021.22","dur":"3.5","text":"into a format that can be ingested"},"1024.72":{"start":"1024.72","dur":"2","text":"into our neural net"},"1026.72":{"start":"1026.72","dur":"2","text":"Thank you"}}