{"0.00":{"start":"0","dur":"4.673","text":"[MUSIC]"},"4.67":{"start":"4.673","dur":"2.996","text":"Stanford University."},"7.67":{"start":"7.669","dur":"1.09","text":"&gt;&gt; Okay hi everyone."},"8.76":{"start":"8.759","dur":"2.491","text":"Let&#39;s get started again."},"11.25":{"start":"11.25","dur":"8.39","text":"We&#39;re back with we&#39;re into\nweek six now and Lecture 11."},"20.74":{"start":"20.74","dur":"4.65","text":"This is basically the third\nnow last of our lectures."},"25.39":{"start":"25.39","dur":"5.52","text":"It&#39;s sort of essentially concentrating on\nwhat we can do with recurrent models and"},"30.91":{"start":"30.91","dur":"2.72","text":"sequence to sequence architectures."},"33.63":{"start":"33.63","dur":"5.86","text":"I thought what I&#39;d do in the first\npart of the lecture is have one"},"39.49":{"start":"39.49","dur":"4.95","text":"more attempt at explaining some\nof the ideas about GRUs and"},"44.44":{"start":"44.44","dur":"3.7","text":"LSTMs and where do they come from and\nhow do they work?"},"48.14":{"start":"48.14","dur":"3.54","text":"I&#39;d sort of decide to do\nthat anyway on the weekend,"},"51.68":{"start":"51.68","dur":"5.88","text":"just because I know that when I first\nstarted seeing some of these gated models,"},"57.56":{"start":"57.56","dur":"4.68","text":"that it took a long time for\nthem to make much sense to me, and"},"62.24":{"start":"62.24","dur":"2.9","text":"not just seem like a complete surprise and\nmystery."},"65.14":{"start":"65.14","dur":"1.4","text":"That&#39;s the way they work so"},"66.54":{"start":"66.54","dur":"4.41","text":"I hope I can do a bit of good at\nexplaining that one more time."},"70.95":{"start":"70.95","dur":"4.46","text":"That feeling was reconfirmed when we\nstarted seeing some of the people"},"75.41":{"start":"75.41","dur":"4.725","text":"who&#39;ve filled in the midterm survey so\nthanks to all the people who filled it in."},"80.14":{"start":"80.135","dur":"1.12","text":"For people who haven&#39;t,"},"81.26":{"start":"81.255","dur":"4.04","text":"I&#39;m still happy to have you fill it\nin over the last couple of days."},"85.30":{"start":"85.295","dur":"4.68","text":"While there were a couple of people\nwho put LSTMs in the list of"},"89.98":{"start":"89.975","dur":"3.315","text":"concepts they felt that they\nunderstood really well."},"93.29":{"start":"93.29","dur":"2.87","text":"Dozens of people put LSTMs and"},"96.16":{"start":"96.16","dur":"4.89","text":"GRUs into the list of concepts\nthey felt kind of unsure about."},"101.05":{"start":"101.05","dur":"5.35","text":"This first part is for you and if you&#39;re\none of the ones that already understand"},"106.40":{"start":"106.4","dur":"4.22","text":"it really well, I guess you&#39;ll just\nhave to skip ahead to the second part."},"110.62":{"start":"110.62","dur":"3.25","text":"Then we&#39;ll have the research\nhighlight which should be fun today."},"113.87":{"start":"113.87","dur":"3.95","text":"And then, so\nmoving on from that it&#39;s then completing,"},"117.82":{"start":"117.82","dur":"2.79","text":"saying a bit more about\nmachine translation."},"120.61":{"start":"120.61","dur":"4.78","text":"It&#39;s a bit that we sort of had skipped and\nprobably should have explained earlier"},"125.39":{"start":"125.39","dur":"3.22","text":"which is how do people evaluate\nmachine translation systems?"},"128.61":{"start":"128.61","dur":"3.943","text":"Because we&#39;ve been showing you numbers and\ngraphs and so on and never discussed that."},"132.55":{"start":"132.553","dur":"5.153","text":"And then I wanna sort of say a bit\nmore about a couple of things that"},"137.71":{"start":"137.706","dur":"5.164","text":"come up when trying to build new\nmachines translation systems."},"142.87":{"start":"142.87","dur":"4.05","text":"And in some sense, this is sort of\ndone on the weed stuff it&#39;s not"},"146.92":{"start":"146.92","dur":"4.6","text":"that this is sort of one central concept\nthat you can possibly finish your"},"151.52":{"start":"151.52","dur":"2.984","text":"neural networks class\nwithout having learned."},"154.50":{"start":"154.504","dur":"4.452","text":"But on the other hand, I think that all of\nthese sort of kind of things that come up"},"158.96":{"start":"158.956","dur":"4.385","text":"if you are actually trying to build\nsomething where you&#39;ve actually got a deep"},"163.34":{"start":"163.341","dur":"3.654","text":"learning system that you can use to\ndo useful stuff in the world and"},"167.00":{"start":"166.995","dur":"3.955","text":"that they&#39;re useful, good,\nnew concepts to know."},"170.95":{"start":"170.95","dur":"1.42","text":"Okay."},"172.37":{"start":"172.37","dur":"2.57","text":"Lastly just the reminders and\nvarious things."},"174.94":{"start":"174.94","dur":"2.8","text":"The midterm, we have got it all graded."},"177.74":{"start":"177.74","dur":"2.86","text":"And our plan is that we are going to"},"180.60":{"start":"180.6","dur":"3.39","text":"return it to the people\nwho are here after class."},"183.99":{"start":"183.99","dur":"4.75","text":"Where in particular, there&#39;s another\nevent that&#39;s on here after class,"},"188.74":{"start":"188.74","dur":"4.5","text":"so where we&#39;re going to return it\nafter class is outside the door."},"193.24":{"start":"193.24","dur":"3.825","text":"That you should be able to find\nTAs with boxes of midterms and"},"197.07":{"start":"197.065","dur":"1.541","text":"be able to return them."},"198.61":{"start":"198.606","dur":"4.6","text":"Assignment three, yeah so this has\nbeen a little bit of a stretch for"},"203.21":{"start":"203.206","dur":"2.987","text":"everybody on assignment three I realized,"},"206.19":{"start":"206.193","dur":"4.379","text":"because sort of the midterm got\nin the way and people got behind."},"210.57":{"start":"210.572","dur":"4.836","text":"And we&#39;ve also actually we&#39;re hoping\nto be sort of right ready to go with"},"215.41":{"start":"215.408","dur":"4.917","text":"giving people GPU resources on Azure and\nthat&#39;s kinda&#39;ve gone behind,"},"220.33":{"start":"220.325","dur":"4.52","text":"they&#39;re trying to work on that right\nnow so with any luck maybe by the end"},"224.85":{"start":"224.845","dur":"4.175","text":"of today we might have the GPU\nresources part in place."},"229.02":{"start":"229.02","dur":"4.75","text":"I mean, at any rate, you should absolutely\nbe getting start on the assignment and"},"233.77":{"start":"233.77","dur":"1.97","text":"writing the code."},"235.74":{"start":"235.74","dur":"4.27","text":"But we also do really hope that\nbefore you finish this assignment,"},"240.01":{"start":"240.01","dur":"4.39","text":"you take a chance to try out Azure,\nDocker and"},"244.40":{"start":"244.4","dur":"3.72","text":"getting stuff working on GPUs because\nthat&#39;s really good experience to have."},"249.44":{"start":"249.44","dur":"5.57","text":"Then final projects,\nthe thing that we all noticed about our"},"255.01":{"start":"255.01","dur":"5.35","text":"office hours last week after the midterm\nis that barely anybody came to them."},"260.36":{"start":"260.36","dur":"5.916","text":"We&#39;d really like to urge for this week,\nplease come along to office hours again."},"266.28":{"start":"266.276","dur":"4.456","text":"And especially if you&#39;re doing\na final project, we&#39;d really,"},"270.73":{"start":"270.732","dur":"4.617","text":"really like you to turn up and\ntalk to us about your final projects and"},"275.35":{"start":"275.349","dur":"4.699","text":"in particular tonight after class and\na bit of dinner which is again,"},"280.05":{"start":"280.048","dur":"3.261","text":"we&#39;re going be doing\nunlimited office hours."},"283.31":{"start":"283.309","dur":"1.708","text":"Feel free to come and see him, and"},"285.02":{"start":"285.017","dur":"3.782","text":"possibly even depending on how you feel\nabout it, you might even go off and"},"288.80":{"start":"288.799","dur":"4.151","text":"have dinner first and then come back and\nsee him to spread things out a little bit."},"294.23":{"start":"294.23","dur":"3.34","text":"Are there any questions\npeople are dying to know,"},"297.57":{"start":"297.57","dur":"2.2","text":"or do I head straight into\ncontent at that point?"},"303.87":{"start":"303.87","dur":"1.36","text":"I&#39;ll head straight into content."},"306.34":{"start":"306.34","dur":"4.56","text":"Basically I wanted to sort of spend\na bit of time going through, again,"},"310.90":{"start":"310.9","dur":"5.85","text":"the sort of ideas of where did these\nkinds of fancy recurrent units come from?"},"316.75":{"start":"316.75","dur":"3.95","text":"What are they going to try and achieve and\nhow do they go about doing it?"},"321.70":{"start":"321.7","dur":"4.636","text":"Our starting point is, what we have\nwith a recurrent neural network is that"},"326.34":{"start":"326.336","dur":"3.399","text":"we&#39;ve got something that&#39;s\nevolving through time."},"329.74":{"start":"329.735","dur":"7.663","text":"And at the end of that we&#39;re at some\npoint in that here where time t plus n."},"337.40":{"start":"337.398","dur":"4.444","text":"And then what we want to do\nis have some sense of well,"},"341.84":{"start":"341.842","dur":"6.948","text":"this stuff that we saw at time t, is that\naffecting what happens at time t plus n?"},"348.79":{"start":"348.79","dur":"6.49","text":"That&#39;s the kind of thing of is it\nthe fact that we saw at time t"},"355.28":{"start":"355.28","dur":"7.66","text":"this verb squash that is having\nsome effect on the n words later,"},"362.94":{"start":"362.94","dur":"5.53","text":"that this is being someone saying\nthe word window because this is some"},"368.47":{"start":"368.47","dur":"5.43","text":"kind of association between squashing and\nwindows or is that completely irrelevant?"},"373.90":{"start":"373.9","dur":"3.88","text":"We wanna sort of measure\nhow what you&#39;re doing here"},"377.78":{"start":"377.78","dur":"4.91","text":"affects what&#39;s happening maybe six,\neight, ten words later."},"382.69":{"start":"382.69","dur":"5.33","text":"And so the question is how can we\nachieve that and how can we achieve it?"},"388.02":{"start":"388.02","dur":"4.38","text":"And what Richard discussed and\nthere was some sort of complex math here"},"392.40":{"start":"392.4","dur":"3.88","text":"which I&#39;m not going to explain,\nagain, in great detail."},"396.28":{"start":"396.28","dur":"4.97","text":"But what we found is if we had a basic\nrecurrent neural network what we&#39;re"},"401.25":{"start":"401.25","dur":"4.91","text":"doing at each time step in the basic\nrecurrent neural network is"},"406.16":{"start":"406.16","dur":"4.41","text":"we&#39;ve got some hidden state and\nwe&#39;re multiplying it by matrix and"},"410.57":{"start":"410.57","dur":"3.79","text":"then we&#39;re adding some stuff to do with\nthe input and then we go onto next"},"414.36":{"start":"414.36","dur":"5","text":"time stamp where we&#39;re multiplying that\nhidden state by the same matrix again and"},"419.36":{"start":"419.36","dur":"4.16","text":"adding some input stuff and then we\ngo onto the time step and we model."},"423.52":{"start":"423.52","dur":"3.926","text":"Multiplying that,\nhidden stuff by the same matrix again."},"427.45":{"start":"427.446","dur":"4.894","text":"It keeping on doing these matrix\nmultiplies and when you keep on doing"},"432.34":{"start":"432.34","dur":"5.83","text":"these matrix multiplies you can\npotentially get into trouble."},"438.17":{"start":"438.17","dur":"5.45","text":"And the trouble you get into is\nif your gradient is going to zero"},"443.62":{"start":"443.62","dur":"4.82","text":"you kind of can&#39;t tell whether that\nmeans that actually what happened"},"448.44":{"start":"448.44","dur":"5.21","text":"in words ago is having no effect\non what you&#39;re seeing now."},"453.65":{"start":"453.65","dur":"5.312","text":"Or whether it is you hadn&#39;t set all\nof the things in your matrixes norm"},"458.96":{"start":"458.962","dur":"6.242","text":"exactly right and so that the gradient\nis going to zero because it&#39;s vanishing."},"470.59":{"start":"470.594","dur":"4.923","text":"This is where the stuff about eigenvalues\nand stuff like that comes in."},"475.52":{"start":"475.517","dur":"3.843","text":"But kind of the problem is with."},"479.36":{"start":"479.36","dur":"4.321","text":"Basic RNA, sort of a bit too much\nlike having to land your aircraft"},"483.68":{"start":"483.681","dur":"3.235","text":"on the aircraft carrier or\nsomething like that."},"486.92":{"start":"486.916","dur":"3.098","text":"That if you can get things\njust the right size,"},"490.01":{"start":"490.014","dur":"3.098","text":"things you can land on\nthe aircraft carrier but"},"493.11":{"start":"493.112","dur":"5.499","text":"if somehow your eigenvalues are a bit too\nsmall then you have vanishing gradients."},"498.61":{"start":"498.611","dur":"5.141","text":"And if they&#39;re a bit too large\nyou have exploding gradients and"},"503.75":{"start":"503.752","dur":"3.977","text":"you sort of,\nit&#39;s very hard to get it right and so"},"507.73":{"start":"507.729","dur":"6.708","text":"this this naive transition function seems\nto be the cause of a lot of the problems."},"514.44":{"start":"514.437","dur":"3.175","text":"With the naive transition\nfunction in particular,"},"517.61":{"start":"517.612","dur":"4.598","text":"what it means is that sorta we&#39;re doing\nthis sequence of matrix multipliers."},"522.21":{"start":"522.21","dur":"3.9","text":"So we&#39;re keeping on multiplying\nby matrix at each time step."},"526.11":{"start":"526.11","dur":"3.54","text":"And so, that means that when\nwe&#39;re then trying to learn."},"529.65":{"start":"529.65","dur":"4.065","text":"How much effect things have\non our decisions up here."},"533.72":{"start":"533.715","dur":"4.277","text":"We&#39;re doing that by backpropagating\nthrough this whole sequence of"},"537.99":{"start":"537.992","dur":"2.113","text":"intermediate nodes."},"540.11":{"start":"540.105","dur":"6.062","text":"And so, the whole idea of all of these\ngated recurrent models is to say,"},"546.17":{"start":"546.167","dur":"5.558","text":"well, somehow, we&#39;d like to be\nable to get more direct evidence"},"551.73":{"start":"551.725","dur":"4.95","text":"of the effect of early time\nsteps on much later time steps,"},"556.68":{"start":"556.675","dur":"7.408","text":"without having to do this long sequence\nmatrix multiplies, which almost certainly."},"564.08":{"start":"564.083","dur":"3.737","text":"Give us the danger of\nkilling off the evidence."},"567.82":{"start":"567.82","dur":"3.06","text":"So essentially what we wanna have is,"},"570.88":{"start":"570.88","dur":"4.19","text":"we want to kinda consider the time\nsequence that&#39;s our straight line."},"575.07":{"start":"575.07","dur":"5.106","text":"We also want to allow these shortcut\nconnections so ht can directly"},"580.18":{"start":"580.176","dur":"5.926","text":"affect ht +2 because if we could do\nthat we then when we&#39;re backpropagating"},"586.10":{"start":"586.102","dur":"6.032","text":"we&#39;ll then be able to measure in the\nbackward phase the effect of ht on ht + 2."},"592.13":{"start":"592.134","dur":"1.013","text":"And therefore,"},"593.15":{"start":"593.147","dur":"4.133","text":"we would be much more likely to\nlearn these long term dependencies."},"598.79":{"start":"598.79","dur":"2.13","text":"So that seems a good idea."},"603.59":{"start":"603.59","dur":"2.88","text":"So I&#39;m gonna do the kinda gated\nrecurrent units first, and"},"606.47":{"start":"606.47","dur":"3.7","text":"then kinda build onto LSTMs,\nwhich are even more complex."},"610.17":{"start":"610.17","dur":"4.07","text":"So essentially that&#39;s what we&#39;re\ndoing in the gated recurrent unit."},"614.24":{"start":"614.24","dur":"4.5","text":"And we&#39;re only making it a little\nbit more complex by saying, well,"},"618.74":{"start":"618.74","dur":"5.123","text":"rather than just uniformly\nputting in stuff from time -1 and"},"623.86":{"start":"623.863","dur":"6.037","text":"time -2, maybe we can have adaptive\nshortcut connections where we&#39;re"},"630.92":{"start":"630.92","dur":"4.84","text":"deciding how much attention to pay to\nthe past, as well as to the present."},"635.76":{"start":"635.76","dur":"3.82","text":"And so, that&#39;s essentially what you\nget with the gated recurrent unit."},"639.58":{"start":"639.58","dur":"5.67","text":"So the key equation of the gated\nrecurrent unit is this first one."},"645.25":{"start":"645.25","dur":"4.74","text":"So it&#39;s sort of saying, well, we&#39;re\ngoing to do the normal neural network"},"649.99":{"start":"649.99","dur":"3.07","text":"recurrent units stuff,\nthat&#39;s the stuff in green."},"653.06":{"start":"653.06","dur":"5.54","text":"So for the stuff in green, we take the\ncurrent input and multiply it by a matrix."},"658.60":{"start":"658.6","dur":"3.21","text":"We take the previous hidden statement and\nmultiply it by a matrix."},"661.81":{"start":"661.81","dur":"2.39","text":"We add all of those things with a bias and"},"664.20":{"start":"664.2","dur":"5.25","text":"put it through a tanh, that&#39;s exactly the\nstandard recurrent neural network update."},"669.45":{"start":"669.45","dur":"7.292","text":"So we&#39;re going to do that candidate\nupdate just like a regular RNN."},"676.74":{"start":"676.742","dur":"4.098","text":"But to actually work out what\nfunction we&#39;re computing,"},"680.84":{"start":"680.84","dur":"4.934","text":"we&#39;re then going to adaptively learn\nhow much and on which dimensions"},"685.77":{"start":"685.774","dur":"5.099","text":"to use that candidate update and\nhow much that we just gonna shortcut it,"},"690.87":{"start":"690.873","dur":"4.657","text":"and just stick with what we had\nfrom the previous time step."},"695.53":{"start":"695.53","dur":"4.33","text":"And while that stuff in the previous\ntime step will have been to some"},"699.86":{"start":"699.86","dur":"4.73","text":"extent computed by this regular and\nupdated the previous time step."},"704.59":{"start":"704.59","dur":"2.99","text":"But of course, that was also a mixture, so"},"707.58":{"start":"707.58","dur":"4.9","text":"to some extent, it will have been directly\ninherited from the time step before that."},"712.48":{"start":"712.48","dur":"4","text":"And so,\nwe kind of adaptively allowing things from"},"716.48":{"start":"716.48","dur":"4.42","text":"far past time steps just to\nbe passed straight through,"},"720.90":{"start":"720.9","dur":"4.62","text":"with no further multiplications\ninto the current time step."},"725.52":{"start":"725.52","dur":"3.55","text":"So a lot of the key to is it\nthat we have this plus here."},"729.07":{"start":"729.07","dur":"4.558","text":"The stuff that is on this side\nof the plus, we&#39;re just saying,"},"733.63":{"start":"733.628","dur":"4.644","text":"just move along the stuff you had\nbefore onto the next time step,"},"738.27":{"start":"738.272","dur":"4.902","text":"which has the effect that we&#39;re\ndirectly having stuff from the past"},"743.17":{"start":"743.174","dur":"3.193","text":"be present to affect further on decisions."},"746.37":{"start":"746.367","dur":"3.914","text":"So that&#39;s most of what\nwe have in a GRU and"},"750.28":{"start":"750.281","dur":"7.139","text":"a GRU is then just a little bit more\ncomplex than that because if we do this,"},"757.42":{"start":"757.42","dur":"6.47","text":"it&#39;s sort of all additive,\nyou kinda kick stuff around forever."},"763.89":{"start":"763.89","dur":"2.892","text":"You&#39;re deciding which to pay attention to,\nbut"},"766.78":{"start":"766.782","dur":"3.537","text":"once you&#39;ve paid attention to it,\nit&#39;s around forever."},"770.32":{"start":"770.319","dur":"4.632","text":"And that&#39;s because you&#39;re sort\nof just adding stuff on here."},"774.95":{"start":"774.951","dur":"5.376","text":"And so, the final step is to say well\nactually, maybe we want to sort of prune"},"780.33":{"start":"780.327","dur":"5.683","text":"away some of the past stuff adaptively so\nit doesn&#39;t hang around forever."},"786.01":{"start":"786.01","dur":"4.28","text":"And so, to do that, we&#39;re adding\nthis second gate, the reset gate."},"790.29":{"start":"790.29","dur":"6.252","text":"And so, the reset gate gives you a vector\nof, again, numbers between zero and"},"796.54":{"start":"796.542","dur":"5.425","text":"one, which is calculated like a kind\nof a standard recurrent unit."},"801.97":{"start":"801.967","dur":"5.132","text":"But it&#39;s sort of saying,\nwell to some extent, what we want to do is"},"807.10":{"start":"807.099","dur":"6.751","text":"be able to delete some of the stuff that\nwas in ht- 1 when it&#39;s no longer relevant."},"813.85":{"start":"813.85","dur":"2.38","text":"And so,\nwe doing this sort of hadamard product,"},"816.23":{"start":"816.23","dur":"4.33","text":"the element wise product of the reset\ngate and the previous hidden state."},"820.56":{"start":"820.56","dur":"2.97","text":"And so,\nwe can forget parts of the hidden state."},"823.53":{"start":"823.53","dur":"4.07","text":"And the parts that we&#39;re forgetting is\nembedded in this kind of candidate update."},"827.60":{"start":"827.6","dur":"5.044","text":"The part that&#39;s being just\npassed along from the past to"},"832.64":{"start":"832.644","dur":"5.381","text":"have direct updates is still\njust exactly as it was before."},"838.03":{"start":"838.025","dur":"4.285","text":"So to have one attempt to\nbe more visual at that."},"842.31":{"start":"842.31","dur":"4.132","text":"So if we have a basic vanilla tanh-RNN,"},"846.44":{"start":"846.442","dur":"6.561","text":"one way that you could think about\nthat is we have a hidden state,"},"853.00":{"start":"853.003","dur":"5.467","text":"and what our execution of our\nunit is doing as a program"},"858.47":{"start":"858.47","dur":"4.86","text":"is saying you read the whole\nof that register h,"},"863.33":{"start":"863.33","dur":"6.103","text":"you do your RNN update, and\nyou write the whole thing back."},"869.43":{"start":"869.433","dur":"3.807","text":"So you&#39;ve got this one memory register."},"873.24":{"start":"873.24","dur":"4.87","text":"You read it all, do a standard recurrent\nupdate, and write it all back."},"878.11":{"start":"878.11","dur":"2.81","text":"So that&#39;s sort of very inflexible."},"880.92":{"start":"880.92","dur":"4.32","text":"And you&#39;re just sort of repeating that\nover and over again at each time step."},"885.24":{"start":"885.24","dur":"4.93","text":"So in contrast to that,\nwhen you have a GRU unit, that is then,"},"890.17":{"start":"890.17","dur":"5.06","text":"allowing you to sort of learn\nthis adaptive flexibility."},"895.23":{"start":"895.23","dur":"4.76","text":"So first of all,\nwith the reset gate, you can learn"},"899.99":{"start":"899.99","dur":"5.38","text":"a subset of the hidden state that\nyou want to read and make use of."},"905.37":{"start":"905.37","dur":"2.38","text":"And the rest of it will\nthen get thrown away."},"907.75":{"start":"907.75","dur":"2.505","text":"So you have an ability to forget stuff."},"910.26":{"start":"910.255","dur":"5.07","text":"And then,\nonce you&#39;ve sort of read your subset,"},"915.33":{"start":"915.325","dur":"4.707","text":"you&#39;ll then going to do\non it your standard RNN"},"920.03":{"start":"920.032","dur":"3.878","text":"computation of how to update things."},"923.91":{"start":"923.91","dur":"3.91","text":"But then secondly,\nyou&#39;re gonna select the writable subset."},"927.82":{"start":"927.82","dur":"1.73","text":"So this is saying,"},"929.55":{"start":"929.55","dur":"3.76","text":"some of the hidden state we&#39;re\njust gonna carry on from the past."},"933.31":{"start":"933.31","dur":"3.908","text":"We&#39;re only now going to\nedit part of the register."},"937.22":{"start":"937.218","dur":"4.633","text":"And saying part of the register,\nI guess is a lying and simplifying a bit,"},"941.85":{"start":"941.851","dur":"3.814","text":"because really,\nyou&#39;ve got this vector of real numbers and"},"945.67":{"start":"945.665","dur":"4.936","text":"some said the part of the register is\n70% updating this dimension and 20%"},"950.60":{"start":"950.601","dur":"5.699","text":"updating this dimension that values could\nbe one or zero but normally they won&#39;t be."},"956.30":{"start":"956.3","dur":"2.59","text":"So I choose the writable subset And"},"958.89":{"start":"958.89","dur":"5.08","text":"then it&#39;s that part of it that I&#39;m\nthen updating with my new candidate"},"963.97":{"start":"963.97","dur":"3.66","text":"update which is then written back,\nadding on to it."},"968.82":{"start":"968.82","dur":"2.965","text":"And so\nboth of those concepts in the gating,"},"971.79":{"start":"971.785","dur":"4.425","text":"the one gate is selecting what to read for\nyour candidate update."},"976.21":{"start":"976.21","dur":"7.042","text":"And the other gate is saying, which\nparts of the hidden state to overwrite?"},"983.25":{"start":"983.252","dur":"2.982","text":"Does that sort of make\nsense how that&#39;s a useful,"},"986.23":{"start":"986.234","dur":"3.773","text":"more powerful way of thinking\nabout having a recurrent model?"},"994.29":{"start":"994.286","dur":"1.2","text":"Yes, a question?"},"1003.31":{"start":"1003.306","dur":"5.464","text":"Yeah, so how you select the readable\nsubset is based on this reset gate?"},"1008.77":{"start":"1008.77","dur":"4.643","text":"So, the reset gate decides\nwhich parts of the hidden"},"1013.41":{"start":"1013.413","dur":"3.703","text":"state to read to update the hidden state."},"1017.12":{"start":"1017.116","dur":"6.016","text":"So, the reset gate calculates which parts\nto read based on the current input and"},"1023.13":{"start":"1023.132","dur":"2.248","text":"the previous hidden state."},"1025.38":{"start":"1025.38","dur":"7.495","text":"So it&#39;s gonna say, okay, I wanna pay a lot\nof attention to dimensions 7 and 52."},"1032.88":{"start":"1032.875","dur":"3.155","text":"And so, those are the ones and\na little to others."},"1036.03":{"start":"1036.03","dur":"4.473","text":"And so those are the ones that\nwill be being read here and"},"1040.50":{"start":"1040.503","dur":"4.378","text":"used in the calculation of\nthe new candidate update,"},"1044.88":{"start":"1044.881","dur":"6.05","text":"which is then sort of mixed together\nwith carrying on what you had before."},"1050.93":{"start":"1050.931","dur":"1.353","text":"Any, yes."},"1066.05":{"start":"1066.05","dur":"4.238","text":"So, the question was explain this again."},"1070.29":{"start":"1070.288","dur":"0.78","text":"I&#39;ll try."},"1071.07":{"start":"1071.068","dur":"2.826","text":"[LAUGH] I will try."},"1073.89":{"start":"1073.894","dur":"1.496","text":"I will try and do that."},"1075.39":{"start":"1075.39","dur":"4.21","text":"Let me go back to this slide first,\ncuz this has most of that,"},"1079.60":{"start":"1079.6","dur":"2.24","text":"except the last piece, right."},"1081.84":{"start":"1081.84","dur":"8.15","text":"So here, what we want to do is we&#39;re\ncarrying along a hidden state over time."},"1089.99":{"start":"1089.99","dur":"4.853","text":"And at each point in time,\nwe&#39;re going to say, well,"},"1094.84":{"start":"1094.843","dur":"4.747","text":"based on the new input and\nthe previous hidden state,"},"1099.59":{"start":"1099.59","dur":"4.535","text":"we want to try and\ncalculate a new hidden state, but"},"1104.13":{"start":"1104.125","dur":"4.768","text":"we don&#39;t fully want to\ncalculate a new hidden state."},"1108.89":{"start":"1108.893","dur":"5.037","text":"Sometimes, it will be useful just to\ncarry over information from further back."},"1113.93":{"start":"1113.93","dur":"5.077","text":"That&#39;s how we&#39;re going to get longer term\nmemory into our current neural network."},"1119.01":{"start":"1119.007","dur":"5.02","text":"Cuz if we kind of keep on doing\nmultiplications at each time step"},"1124.03":{"start":"1124.027","dur":"4.853","text":"along a basic RNN,\nwe lose any notion of long-term memory."},"1128.88":{"start":"1128.88","dur":"5.265","text":"And essentially, we can&#39;t remember things\nfor more than seven to ten time steps."},"1134.15":{"start":"1134.145","dur":"7.022","text":"So that is sort of the top level equation\nto say, well, what we gonna calculate."},"1141.17":{"start":"1141.167","dur":"6.049","text":"We want to calculate a mixture\nof a candidate update and"},"1147.22":{"start":"1147.216","dur":"6.564","text":"keeping what we had there before and\nhow do we do that?"},"1153.78":{"start":"1153.78","dur":"5.59","text":"Well, what we&#39;re going to learn is\nthis ut vector, the update gate and"},"1159.37":{"start":"1159.37","dur":"4.24","text":"the elements of that vector\nare gonna be between zero and one."},"1163.61":{"start":"1163.61","dur":"2.777","text":"And if they&#39;re close to one,\nit&#39;s gonna say,"},"1166.39":{"start":"1166.387","dur":"4.46","text":"overwrite the current hidden state with\nwhat we calculated this time step."},"1170.85":{"start":"1170.847","dur":"2.871","text":"And if they&#39;re close to zero,\nit&#39;s gonna say,"},"1173.72":{"start":"1173.718","dur":"3.462","text":"keep this element vector\njust what it used to be."},"1177.18":{"start":"1177.18","dur":"4.859","text":"And so how we calculate the update\ngate is using our regular kind"},"1182.04":{"start":"1182.039","dur":"4.4","text":"of recurrent unit where it\nlooks at the current input and"},"1186.44":{"start":"1186.439","dur":"5.224","text":"it looks at the recent history and\nit calculates a value with the only"},"1191.66":{"start":"1191.663","dur":"4.859","text":"difference that we use here sigmoid,\nso that&#39;s between 0 and"},"1196.52":{"start":"1196.522","dur":"4.708","text":"1 rather than tanh that puts\nthat at between minus 1 and 1."},"1201.23":{"start":"1201.23","dur":"5.756","text":"And so the kind of hope\nhere intuitively is suppose"},"1206.99":{"start":"1206.986","dur":"5.207","text":"we have a unit that is\nsort of sensitive to what"},"1212.19":{"start":"1212.193","dur":"5.481","text":"verb we&#39;re on,\nthen what we wanna say is well,"},"1217.67":{"start":"1217.674","dur":"7.146","text":"we&#39;re going through this sentence and\nwe&#39;ve seen a verb."},"1224.82":{"start":"1224.82","dur":"5.395","text":"We wanted that unit, well, sorry,\nthese dimension of the vector."},"1230.22":{"start":"1230.215","dur":"3.736","text":"Let&#39;s say, their five dimensions of the\nvector that sort of record what kind of"},"1233.95":{"start":"1233.951","dur":"1.399","text":"verb it&#39;s just seen."},"1235.35":{"start":"1235.35","dur":"6.724","text":"We want those dimensions of the vector\nto just stay recording what verb was"},"1242.07":{"start":"1242.074","dur":"5.827","text":"seen until such time as in the input,\na band new verb appears."},"1247.90":{"start":"1247.901","dur":"5.579","text":"And it&#39;s at precisely that point, we wanna\nsay, okay, now is the time to update."},"1253.48":{"start":"1253.48","dur":"3.287","text":"Forget about what used to be\nstored in those five dimensions."},"1256.77":{"start":"1256.767","dur":"4.043","text":"Now, you should store\na representation of the new verb."},"1260.81":{"start":"1260.81","dur":"3.402","text":"And so, that&#39;s exactly what\nthe update gate could do here."},"1264.21":{"start":"1264.212","dur":"5.998","text":"It could be looking at the input and\nsay, okay, I found a new verb."},"1270.21":{"start":"1270.21","dur":"5.407","text":"So dimensions 47 to 52 should\nbe being given a value of 1 and"},"1275.62":{"start":"1275.617","dur":"7.243","text":"that means that they&#39;ll be storing a value\ncalculated from this candidate update,"},"1282.86":{"start":"1282.86","dur":"4.197","text":"and ignoring what they\nused to store in the past."},"1287.06":{"start":"1287.057","dur":"3.76","text":"But if the update gate finds\nit&#39;s looking at a preposition or"},"1290.82":{"start":"1290.817","dur":"3.624","text":"at a term in our It&#39;ll say,\nno, not interested in those."},"1294.44":{"start":"1294.441","dur":"4.018","text":"So it&#39;ll make the update\nvalue close to 0 and"},"1298.46":{"start":"1298.459","dur":"4.971","text":"that means that dimensions\n47 to 52 will continue to"},"1303.43":{"start":"1303.43","dur":"5.306","text":"store the verb that you last saw\neven if it was ten words ago."},"1308.74":{"start":"1308.736","dur":"1.25","text":"I haven&#39;t quite finish."},"1309.99":{"start":"1309.986","dur":"2.494","text":"So that was that part of it, so yes."},"1312.48":{"start":"1312.48","dur":"1.884","text":"So, the candidate update."},"1314.36":{"start":"1314.364","dur":"1.426","text":"So, that&#39;s the update gate."},"1315.79":{"start":"1315.79","dur":"4.957","text":"And when we do update, the candidate\nupdate is just exactly the same as"},"1320.75":{"start":"1320.747","dur":"4.957","text":"it always was in our current new\nnetwork that you&#39;re calculating this"},"1325.70":{"start":"1325.704","dur":"3.951","text":"function of the important\nthe previous hidden state and"},"1329.66":{"start":"1329.655","dur":"3.552","text":"put it through a tanh\ntogether from minus 1 to 1."},"1333.21":{"start":"1333.207","dur":"3.862","text":"Then the final idea here is that well,"},"1337.07":{"start":"1337.069","dur":"6.153","text":"if you just have this,\nif you&#39;re doing a candidate update,"},"1343.22":{"start":"1343.222","dur":"5.189","text":"you&#39;re always using\nthe previous hidden state and"},"1348.41":{"start":"1348.411","dur":"4.359","text":"the new input word in\nexactly the same way."},"1353.88":{"start":"1353.88","dur":"5.36","text":"Whereas really for my example, what I was\nsaying was if you have detected a new"},"1359.24":{"start":"1359.24","dur":"5.775","text":"verb in the input, you should be storing\nthat new verb in dimensions 47 to 52 and"},"1365.02":{"start":"1365.015","dur":"3.901","text":"you should just be ignoring\nwhat you used to have there."},"1368.92":{"start":"1368.916","dur":"3.477","text":"And so it&#39;s sort of seems like\nat least in some circumstances"},"1372.39":{"start":"1372.393","dur":"3.613","text":"what you&#39;d like to do is throw\naway your current hidden state,"},"1376.01":{"start":"1376.006","dur":"3.344","text":"so you could replace it\nwith some new hidden state."},"1379.35":{"start":"1379.35","dur":"3.767","text":"And so that&#39;s what this second gate,\nthe reset gate does."},"1383.12":{"start":"1383.117","dur":"4.712","text":"So the reset gate can also look at\nthe current import in the previous hidden"},"1387.83":{"start":"1387.829","dur":"3.691","text":"state and\nit choses a value between zero, and one."},"1391.52":{"start":"1391.52","dur":"3.768","text":"And if the reset gate choses\na value close to zero,"},"1395.29":{"start":"1395.288","dur":"5.299","text":"you&#39;re essentially just throwing\naway the previous hidden state and"},"1400.59":{"start":"1400.587","dur":"3.696","text":"calculating something\nbased on your new input."},"1404.28":{"start":"1404.283","dur":"4.173","text":"And the suggestion there for\nlanguage analogy is well,"},"1408.46":{"start":"1408.456","dur":"6.86","text":"if it&#39;s something like you&#39;re recording,\nthe last seen verb in dimensions 47 to 52."},"1415.32":{"start":"1415.316","dur":"4.859","text":"When you see a new verb, well, the right\nthing to do is to throw away what you"},"1420.18":{"start":"1420.175","dur":"4.94","text":"have in your history from 47 to 52 and\njust calculate something new based"},"1425.12":{"start":"1425.115","dur":"4.579","text":"on the input, but that&#39;s not always\ngonna be what you want to do."},"1429.69":{"start":"1429.694","dur":"4.924","text":"For example, in English, English is\nfamous for having a lot of verb particle"},"1434.62":{"start":"1434.618","dur":"4.788","text":"combinations which cause enormous\ndifficulty to non-native speakers."},"1439.41":{"start":"1439.406","dur":"6.488","text":"So that&#39;s all of these things\nlike make up, make out, take up."},"1445.89":{"start":"1445.894","dur":"2.227","text":"All of these combinations of a verb and"},"1448.12":{"start":"1448.121","dur":"3.769","text":"a preposition have a special\nmeaning that you just have to know."},"1451.89":{"start":"1451.89","dur":"5.41","text":"It isn&#39;t really, you can&#39;t tell\nfrom the words most of the time."},"1457.30":{"start":"1457.3","dur":"5.249","text":"So if you are wanting to work out\nwhat the meaning of make out is,"},"1462.55":{"start":"1462.549","dur":"5.866","text":"so you&#39;ve seen make and\nyou put in that into dimensions 47 to 52."},"1468.42":{"start":"1468.415","dur":"4.699","text":"But if dimensions 47 to 52 are really\nstoring main predicate meaning,"},"1473.11":{"start":"1473.114","dur":"5.147","text":"if you see the word out coming next, you\ndon&#39;t wanna throw away make because it&#39;s"},"1478.26":{"start":"1478.261","dur":"5.707","text":"a big difference in meaning whether\nit&#39;s make out or take out will give out."},"1483.97":{"start":"1483.968","dur":"3.912","text":"What you wanna do is you wanna combine\nboth of them together to try and"},"1487.88":{"start":"1487.88","dur":"1.89","text":"calculate the predicate&#39;s meaning."},"1489.77":{"start":"1489.77","dur":"5.34","text":"So in that case, you want your reset\ngate to have a value near one so you&#39;re"},"1495.11":{"start":"1495.11","dur":"4.38","text":"still keeping it and you&#39;re keeping the\nnew import and calculating another value."},"1502.17":{"start":"1502.17","dur":"3.194","text":"Okay, that was my attempt to explain GRUs,\nand now the question."},"1518.95":{"start":"1518.954","dur":"4.928","text":"So the question is okay, but\nwhy this gated recurrent"},"1523.88":{"start":"1523.882","dur":"5.048","text":"unit not suffer from\nthe vanishing gradient problem?"},"1529.99":{"start":"1529.99","dur":"5.843","text":"And really the secret is\nright here in this plus sign."},"1539.04":{"start":"1539.044","dur":"5.019","text":"If you allowed me to simplify slightly,"},"1544.06":{"start":"1544.063","dur":"7.687","text":"and this is actually a version\nof a network that has been used."},"1551.75":{"start":"1551.75","dur":"5.46","text":"It&#39;s essentially, not more details,\nbut this aspect of it actually"},"1557.21":{"start":"1557.21","dur":"5.289","text":"corresponds to the very original\nform of an LSTM that was proposed."},"1562.50":{"start":"1562.499","dur":"8.001","text":"Suppose I just delete this this- ut here,\nso this just was 1."},"1570.50":{"start":"1570.5","dur":"5.74","text":"So what we have here is ht- 1,\nso kind of like the reset gate,"},"1576.24":{"start":"1576.24","dur":"4.79","text":"the update gate is only\nbeing used on this side."},"1581.03":{"start":"1581.03","dur":"4.862","text":"It&#39;s saying should you pay any\nattention to the new candidate,"},"1585.89":{"start":"1585.892","dur":"3.433","text":"but you&#39;re always plussing it with ht-1."},"1589.33":{"start":"1589.325","dur":"4.63","text":"If you&#39;ll imagine that\nslightly simplified form,"},"1593.96":{"start":"1593.955","dur":"3.786","text":"well, if you think about your gradients,"},"1597.74":{"start":"1597.741","dur":"5.364","text":"then what we&#39;ve got here is when\nwe&#39;re kind of working at h,"},"1603.11":{"start":"1603.105","dur":"3.172","text":"this has been used to calculate ht."},"1606.28":{"start":"1606.277","dur":"4.776","text":"Ht-1 is being used to calculate ht, so"},"1611.05":{"start":"1611.053","dur":"3.231","text":"ht equals a plus ht-1, so"},"1614.28":{"start":"1614.284","dur":"5.198","text":"there&#39;s a completely linear relationship"},"1619.48":{"start":"1619.482","dur":"5.765","text":"with a coefficient of one between ht and\nht-1."},"1625.25":{"start":"1625.247","dur":"3.861","text":"Okay, and so\ntherefore when you do your calculus and"},"1629.11":{"start":"1629.108","dur":"4.673","text":"you back prop that, right,\nyou have something with slope 1."},"1633.78":{"start":"1633.781","dur":"5.634","text":"That ht is just directly reflecting ht-1."},"1639.42":{"start":"1639.415","dur":"4.685","text":"And that&#39;s the perfect case for\ngradients to flow beautifully."},"1644.10":{"start":"1644.1","dur":"4.81","text":"Nothing is lost, it&#39;s just going\nstraight back down the line."},"1648.91":{"start":"1648.91","dur":"5.33","text":"And so that&#39;s why it can carry\ninformation for a very long time."},"1654.24":{"start":"1654.24","dur":"5.775","text":"So once we put in this update gate,\nwhat we&#39;re having is the providing"},"1660.02":{"start":"1660.015","dur":"5.755","text":"ut is close to zero,\nthis is gonna be approximately one,"},"1665.77":{"start":"1665.77","dur":"4.42","text":"and so the gradients are just gonna flow\nstraight back to the line in an arbitrary"},"1670.19":{"start":"1670.19","dur":"4.46","text":"distance and\nyou can have long distance dependencies."},"1674.65":{"start":"1674.65","dur":"3.84","text":"Crucially, it&#39;s not like you&#39;re\nmultiplying by a matrix every time,"},"1678.49":{"start":"1678.49","dur":"3.24","text":"which causes all with vanishing gradients."},"1681.73":{"start":"1681.73","dur":"5.724","text":"It&#39;s just almost one there,\nstraight linear sequence."},"1687.45":{"start":"1687.454","dur":"6.854","text":"Now of course, if at some point ut is\nclose to 1, so this is close to zero,"},"1694.31":{"start":"1694.308","dur":"4.927","text":"well then almost nothing\nis flowing in from ht-1."},"1699.24":{"start":"1699.235","dur":"3.085","text":"But that&#39;s then saying there\nis no long term dependency."},"1702.32":{"start":"1702.32","dur":"2.4","text":"That&#39;s what the model learn."},"1704.72":{"start":"1704.72","dur":"4.787","text":"So nothing flows a long way back."},"1709.51":{"start":"1709.507","dur":"0.925","text":"Is that a question?"},"1710.43":{"start":"1710.432","dur":"0.838","text":"Yeah."},"1719.24":{"start":"1719.236","dur":"7.474","text":"So the question is,\nisn&#39;t ht tilted ut both dependent on ht-1."},"1726.71":{"start":"1726.71","dur":"1.266","text":"And yeah, they are."},"1727.98":{"start":"1727.976","dur":"6.906","text":"Just like the ut you&#39;re calculating\nit here in terms of ht-1."},"1734.88":{"start":"1734.882","dur":"6.049","text":"So in some sense the answer is yeah,\nyou are right but"},"1740.93":{"start":"1740.931","dur":"5.109","text":"it&#39;s sort of turns out not matter, right?"},"1746.04":{"start":"1746.04","dur":"4.35","text":"So the thing I think is If I put words\nin to your mouth, the thing that you&#39;re"},"1750.39":{"start":"1750.39","dur":"5.18","text":"thinking about is well, this ut\nlook right down at the bottom here,"},"1755.57":{"start":"1755.57","dur":"4.484","text":"you&#39;ll calculate it by matrix\nvector multiply from ht-1."},"1760.05":{"start":"1760.054","dur":"4.776","text":"And well then, where the ht-1 come from,"},"1764.83":{"start":"1764.83","dur":"5.13","text":"it came from ht-2 and there was some\nmore matrix vector multiplies here,"},"1769.96":{"start":"1769.96","dur":"4.26","text":"so there is a pathway going\nthrough the gates where"},"1774.22":{"start":"1774.22","dur":"4.39","text":"you&#39;re keep on doing matrix vector\nmultiplies, and that is true."},"1778.61":{"start":"1778.61","dur":"3.579","text":"But, it turns out that sort\nof doesn&#39;t really matter,"},"1782.19":{"start":"1782.189","dur":"4.954","text":"because of the fact that there is this\ndirect pathway, where you&#39;re getting"},"1787.14":{"start":"1787.143","dur":"4.743","text":"this straight linear flow of gradient\ninformation, going back in time."},"1794.34":{"start":"1794.335","dur":"1.038","text":"Any other question?"},"1795.37":{"start":"1795.373","dur":"4.768","text":"Yes, I don&#39;t think I&#39;ll get any further\nin this class if I&#39;m not careful."},"1810.66":{"start":"1810.658","dur":"1.542","text":"I&#39;m sorry if that&#39;s true."},"1813.55":{"start":"1813.55","dur":"5.35","text":"So the question was, why when you\nIs before ut and one, one is ut."},"1818.90":{"start":"1818.9","dur":"0.863","text":"We swapped."},"1819.76":{"start":"1819.763","dur":"3.417","text":"&gt;&gt; [INAUDIBLE]\n&gt;&gt; Yeah, if that&#39;s true, sorry about that."},"1823.18":{"start":"1823.18","dur":"2.235","text":"That was bad, boo boo mistake,"},"1825.42":{"start":"1825.415","dur":"2.51","text":"cuz obviously we should be\ntrying to be consistent."},"1827.93":{"start":"1827.925","dur":"3.964","text":"But, it totally doesn&#39;t matter."},"1831.89":{"start":"1831.889","dur":"4.544","text":"This is sort of, in some sense, whether\nyou&#39;re thinking of it as the forget"},"1836.43":{"start":"1836.433","dur":"4.357","text":"gate or a remember gate, and\nyou can kind of have it either way round."},"1840.79":{"start":"1840.79","dur":"4.059","text":"And that doesn&#39;t effect how the math and\nthe learning works."},"1848.10":{"start":"1848.104","dur":"1.622","text":"Any other questions?"},"1851.83":{"start":"1851.829","dur":"4.629","text":"I&#39;m happy to talk about this because I do\nactually think it&#39;s useful to understand"},"1856.46":{"start":"1856.458","dur":"4.296","text":"this stuff cuz in some sense these kind\nof gated units have been the biggest and"},"1860.75":{"start":"1860.754","dur":"4.186","text":"most useful idea for making practical\nsystems in the last couple of years."},"1864.94":{"start":"1864.94","dur":"0.684","text":"Yes."},"1871.28":{"start":"1871.282","dur":"5.24","text":"I actually have a picture for\nan LSTM later on."},"1876.52":{"start":"1876.522","dur":"3.774","text":"It depends on a lot of particularities,\nbut"},"1880.30":{"start":"1880.296","dur":"4.24","text":"it sort of seems like\nsomewhere around 100."},"1884.54":{"start":"1884.536","dur":"5.135","text":"Sorry the question was how long does a GRU\nactually end up remembering for and I"},"1889.67":{"start":"1889.671","dur":"5.829","text":"kind of think order of magnitude the kind\nnumber you want in your head is 100 steps."},"1895.50":{"start":"1895.5","dur":"5.326","text":"So they don&#39;t remember forever I think\nthat&#39;s something people also get wrong."},"1900.83":{"start":"1900.826","dur":"5.859","text":"If we go back to the other one,\nthat I hope to get to eventually,"},"1906.69":{"start":"1906.685","dur":"2.885","text":"the name is kind of a mouthful."},"1909.57":{"start":"1909.57","dur":"4.831","text":"I think it was actually very\ndeliberately named, where it was called,"},"1914.40":{"start":"1914.401","dur":"1.929","text":"long short term memory."},"1916.33":{"start":"1916.33","dur":"4.81","text":"Right there was no idea in people&#39;s\nheads that this was meant to be"},"1921.14":{"start":"1921.14","dur":"4.5","text":"the model of long term\nmemory in the human brain."},"1925.64":{"start":"1925.64","dur":"3.1","text":"Long term memory is\nfundamentally different and"},"1928.74":{"start":"1928.74","dur":"3.03","text":"needs to be modeled in other ways and\nmaybe later in the class,"},"1931.77":{"start":"1931.77","dur":"4.57","text":"we&#39;ll say a little a bit about the kind\nof ideas people thinking about this."},"1936.34":{"start":"1936.34","dur":"3.216","text":"What this was about was saying okay,"},"1939.56":{"start":"1939.556","dur":"5.304","text":"well people have a short term memory and\nit lasts for a while."},"1944.86":{"start":"1944.86","dur":"4.869","text":"Whereas the problem was our current\nneural networks are losing all of there"},"1949.73":{"start":"1949.729","dur":"1.706","text":"memory in ten time steps."},"1951.44":{"start":"1951.435","dur":"4.901","text":"So if we could get that pushed out\nanother order of magnitude during"},"1956.34":{"start":"1956.336","dur":"4.026","text":"100 time steps that would\nbe really useful to give us"},"1960.36":{"start":"1960.362","dur":"3.345","text":"a more human like sense\nof short term memory."},"1963.71":{"start":"1963.707","dur":"0.5","text":"Sorry, yeah?"},"1969.93":{"start":"1969.93","dur":"5.673","text":"So the question is,\ndo GRUs train faster than LSTMs?"},"1975.60":{"start":"1975.603","dur":"4.022","text":"I don&#39;t think that&#39;s true,\ndoes Richard have an opinion?"},"1979.63":{"start":"1979.625","dur":"5.667","text":"&gt;&gt; [INAUDIBLE]\n&gt;&gt; Yes,"},"1985.29":{"start":"1985.292","dur":"5.134","text":"so Richard says less computation\nthe computational cost is faster,"},"1990.43":{"start":"1990.426","dur":"4.967","text":"but I sort of feel that sometimes\nLSTMs have a slight edge on speed."},"1995.39":{"start":"1995.393","dur":"1.982","text":"No huge difference,\nlet&#39;s say that&#39;s the answer."},"1997.38":{"start":"1997.375","dur":"5.895","text":"Any other, was there another\nquestion that people want to ask?"},"2004.77":{"start":"2004.77","dur":"1.49","text":"Okay, I&#39;ll go on."},"2006.26":{"start":"2006.26","dur":"5.483","text":"You can ask them again in a minute and\nI go on."},"2011.74":{"start":"2011.743","dur":"4.808","text":"Okay, so then finally I wanted to sort"},"2016.55":{"start":"2016.551","dur":"4.068","text":"of say a little bit about LSTMs."},"2020.62":{"start":"2020.619","dur":"5.176","text":"So LSTMs are more complex because there\nare more equations down the right side."},"2025.80":{"start":"2025.795","dur":"6.835","text":"And there&#39;s more gates but they&#39;re barely\ndifferent when it comes down to it."},"2032.63":{"start":"2032.63","dur":"7.54","text":"And to some extent, they look more\ndifferent than they are because of"},"2040.17":{"start":"2040.17","dur":"5.64","text":"certain arbitrary choices of notation\nthat was made when LSTMs were introduced."},"2045.81":{"start":"2045.81","dur":"5.043","text":"So when LSTMs were introduced,\nHochreiter &amp; Schmidhuber"},"2050.85":{"start":"2050.853","dur":"4.627","text":"sort of decided to say, well,\nwe have this privileged notion of"},"2055.48":{"start":"2055.48","dur":"4.77","text":"memory in the LSTM,\nwhich we&#39;re going to call the cell."},"2060.25":{"start":"2060.25","dur":"4.267","text":"And so people use C for\nthe cell of the LSTM."},"2064.52":{"start":"2064.517","dur":"5.582","text":"But the crucial thing to notice\nIs that the cell of the LSTM"},"2070.10":{"start":"2070.099","dur":"5.581","text":"is behaving like the hidden\nstate of the GRU, so really,"},"2075.68":{"start":"2075.68","dur":"5.65","text":"the h of the GRU is equivalent\nto the c of the LSTM."},"2081.33":{"start":"2081.33","dur":"4.05","text":"Whereas the h of the LSTM is"},"2085.38":{"start":"2085.38","dur":"3.94","text":"something different that&#39;s related\nto sort what&#39;s exposed to the world."},"2089.32":{"start":"2089.32","dur":"6.66","text":"So the center of the LSTM,\nthis equation for updating the cell."},"2095.98":{"start":"2095.98","dur":"5.83","text":"Is do a first approximation exactly\nthe same as this most crucial equation for"},"2101.81":{"start":"2101.81","dur":"2.88","text":"updating the hidden state of the GRU."},"2104.69":{"start":"2104.69","dur":"4.429","text":"Now, if you stare a bit,\nthey&#39;re not quite the same,"},"2109.12":{"start":"2109.119","dur":"3.761","text":"the way they are different is very small."},"2112.88":{"start":"2112.88","dur":"5.276","text":"So in the LSTM you have two gates\na forget gate and then an input gate so"},"2118.16":{"start":"2118.156","dur":"5.662","text":"both of those for each of the dimension\nhave a value between zero and one."},"2123.82":{"start":"2123.818","dur":"4.458","text":"So you can simultaneously keep\neverything from the past and"},"2128.28":{"start":"2128.276","dur":"3.835","text":"keep everything from your\nnew calculated value and"},"2132.11":{"start":"2132.111","dur":"4.179","text":"sum them together which is\na little bit different."},"2136.29":{"start":"2136.29","dur":"5.43","text":"To the GRU where you&#39;re sort of doing\nthis tradeoff as to how much to take"},"2141.72":{"start":"2141.72","dur":"5.46","text":"directly, copy across the path versus\nhow much to use your candidate update."},"2147.18":{"start":"2147.18","dur":"4.137","text":"So it split those into two functions,\nso you get the sum of them both."},"2151.32":{"start":"2151.317","dur":"3.304","text":"But other than that,\nit&#39;s exactly the same, right?"},"2154.62":{"start":"2154.621","dur":"2.832","text":"Where&#39;s my mouse?"},"2157.45":{"start":"2157.453","dur":"4.613","text":"The candidate update is\nexactly the same as what&#39;s"},"2162.07":{"start":"2162.066","dur":"4.074","text":"being listed in terms of c tilde and\nh tilde but"},"2166.14":{"start":"2166.14","dur":"4.504","text":"the candidate update is exactly,\nwell, sorry,"},"2170.64":{"start":"2170.644","dur":"5.148","text":"it&#39;s not quite I guess it&#39;s\nthe reset gate the candidate"},"2175.79":{"start":"2175.792","dur":"5.888","text":"update is virtually the same as\nthe standard LSTM style unit."},"2181.68":{"start":"2181.68","dur":"4.264","text":"And then for the gates,\nthe gates are sort of the same,"},"2185.94":{"start":"2185.944","dur":"2.967","text":"that they&#39;re using these sort of R and"},"2188.91":{"start":"2188.911","dur":"5.953","text":"N style calculations to get a value\nbetween zero for one for each dimension."},"2194.86":{"start":"2194.864","dur":"4.958","text":"So the differences\nare that we added one more"},"2199.82":{"start":"2199.822","dur":"4.422","text":"gate because we kinda having forget and"},"2204.24":{"start":"2204.244","dur":"4.958","text":"input gates here and\nthe other difference is"},"2209.20":{"start":"2209.202","dur":"5.226","text":"to have the ability to\nsort of that the GRUs sort"},"2214.43":{"start":"2214.428","dur":"4.824","text":"of has this reset gate where it&#39;s saying,"},"2219.25":{"start":"2219.252","dur":"8.418","text":"I might ignore part of the past when\ncalculating My candidate update."},"2227.67":{"start":"2227.67","dur":"3.64","text":"The LSTM is doing it\na little bit differently."},"2231.31":{"start":"2231.31","dur":"6.08","text":"So the LSTM in the candidate update,\nit&#39;s always using the current input."},"2237.39":{"start":"2237.39","dur":"5.007","text":"But for this other half here, it&#39;s not"},"2242.40":{"start":"2242.397","dur":"5.632","text":"using ct minus 1, it&#39;s using ht minus 1."},"2248.03":{"start":"2248.029","dur":"5.531","text":"So the LSTM has this extra\nht which is derived from ct."},"2254.78":{"start":"2254.78","dur":"4.987","text":"And the way that it&#39;s derived from ct\nis that there&#39;s an extra tanh here but"},"2259.77":{"start":"2259.767","dur":"3.019","text":"then you&#39;re scaling with this output gate."},"2262.79":{"start":"2262.786","dur":"6.714","text":"So the output gate is sort of equivalent\nof the reset gate of the GRU."},"2269.50":{"start":"2269.5","dur":"4.191","text":"But effectively,\nit&#39;s one one time step earlier,"},"2273.69":{"start":"2273.691","dur":"4","text":"cuz on the LSTM side,\non the preceding time step,"},"2277.69":{"start":"2277.691","dur":"5.524","text":"you also calculate an ht by ignoring\nsome stuff with the output gate,"},"2283.22":{"start":"2283.215","dur":"3.81","text":"whereas in the GRU, for\nthe current time step,"},"2287.03":{"start":"2287.025","dur":"6.124","text":"you&#39;re multiplying with the reset gate\ntimes your previous hidden state."},"2293.15":{"start":"2293.149","dur":"1.045","text":"That sorta makes sense?"},"2294.19":{"start":"2294.194","dur":"1.126","text":"A question."},"2309.96":{"start":"2309.96","dur":"1.536","text":"Right, yes, the don&#39;t forget gate."},"2311.50":{"start":"2311.496","dur":"4.894","text":"[LAUGH] You&#39;re right, so\nit&#39;s the question about was the ft."},"2316.39":{"start":"2316.39","dur":"1.44","text":"Is it really a forget gate?"},"2317.83":{"start":"2317.83","dur":"3.49","text":"No, as presented here,\nit&#39;s a don&#39;t forget gate."},"2321.32":{"start":"2321.32","dur":"4.43","text":"Again, you could do the 1 minus trick if\nyou wanted to and call this 1 minus f1,"},"2325.75":{"start":"2325.75","dur":"4.31","text":"but yeah, as presented here,\nif the value is close to 1,"},"2330.06":{"start":"2330.06","dur":"2.64","text":"it means don&#39;t forget, yeah, absolutely."},"2343.28":{"start":"2343.28","dur":"5.791","text":"So this one here is genuinely\nan update gate because if If the value"},"2349.07":{"start":"2349.071","dur":"6.259","text":"of it is close to 1,\nyou&#39;re updating with the candidate update."},"2355.33":{"start":"2355.33","dur":"2.16","text":"And if the value is close to zero,"},"2357.49":{"start":"2357.49","dur":"2.655","text":"you&#39;re keeping the previous\ncontents of the hidden state."},"2360.15":{"start":"2360.145","dur":"5.832","text":"&gt;&gt; [INAUDIBLE]\nreset."},"2365.98":{"start":"2365.977","dur":"3.989","text":"&gt;&gt; Right, so the reset gate is\nsort of a don&#39;t reset gate."},"2369.97":{"start":"2369.966","dur":"0.767","text":"[LAUGH] Yeah, okay."},"2370.73":{"start":"2370.733","dur":"5.979","text":"[LAUGH] I&#39;m having a hard time\nwith the terminology here [LAUGH]."},"2376.71":{"start":"2376.712","dur":"2.622","text":"You are right."},"2379.33":{"start":"2379.334","dur":"3.206","text":"Another question?"},"2403.35":{"start":"2403.349","dur":"6.075","text":"So okay, so the question was\nsometimes you&#39;re using ct-1,"},"2409.42":{"start":"2409.424","dur":"3.674","text":"and sometimes you&#39;re using ht-1."},"2413.10":{"start":"2413.098","dur":"1.534","text":"What&#39;s going on there?"},"2414.63":{"start":"2414.632","dur":"7.748","text":"And the question is in what sense\nis ct less exposed in the LSTM?"},"2422.38":{"start":"2422.38","dur":"4.82","text":"Right, so there was something I glossed\nover in my LSTM presentation, and"},"2427.20":{"start":"2427.2","dur":"1.42","text":"I&#39;m being called on it."},"2428.62":{"start":"2428.62","dur":"5.12","text":"Is look, actually for the LSTM, it&#39;s ht-1"},"2433.74":{"start":"2433.74","dur":"5.791","text":"that&#39;s being used everywhere for\nall three gates."},"2439.53":{"start":"2439.531","dur":"4.727","text":"So really, when I sort of said\nthat what we&#39;re doing here,"},"2444.26":{"start":"2444.258","dur":"5.592","text":"calculating ht, that&#39;s sort of\nsimilar to the reset gate in the GRU."},"2451.18":{"start":"2451.18","dur":"2.42","text":"I kind of glossed over that a little."},"2453.60":{"start":"2453.6","dur":"5.17","text":"It&#39;s sort of true in terms of thinking of\nthe calculation of the candidate update"},"2458.77":{"start":"2458.77","dur":"5.175","text":"cuz this ht- 1 will then go\ninto the candidate update."},"2463.95":{"start":"2463.945","dur":"5.425","text":"But&#39;s a bit more than that, cuz actually,\nstuff that you throw away with your"},"2469.37":{"start":"2469.37","dur":"5.54","text":"output gate at one time step is\nthen also gonna be thrown away"},"2474.91":{"start":"2474.91","dur":"5.51","text":"in the calculation of every\ngate at the next time step."},"2480.42":{"start":"2480.42","dur":"8.402","text":"Yeah, and so then the second question is\nin what sense is the cell less exposed?"},"2488.82":{"start":"2488.822","dur":"1.707","text":"And that&#39;s sort of the answer to that."},"2490.53":{"start":"2490.529","dur":"4.995","text":"The sense in which the cell\nis less exposed is"},"2495.52":{"start":"2495.524","dur":"5.265","text":"the only place that\nthe cell is directly used,"},"2500.79":{"start":"2500.789","dur":"5.805","text":"is to sort of linearly add\non the cell at the previous"},"2506.59":{"start":"2506.594","dur":"4.696","text":"time step plus its candidate update."},"2511.29":{"start":"2511.29","dur":"2.41","text":"For all the other computations,"},"2513.70":{"start":"2513.7","dur":"4.73","text":"you&#39;re sort of partially hiding\nthe cell using this output gate."},"2521.22":{"start":"2521.22","dur":"1.578","text":"Another question, sure."},"2548.96":{"start":"2548.964","dur":"3.375","text":"Hm, okay, so the question is, gee,"},"2552.34":{"start":"2552.339","dur":"5.804","text":"why do you need this tanh here,\ncouldn&#39;t you just drop that one?"},"2562.07":{"start":"2562.068","dur":"0.65","text":"Whoops."},"2571.19":{"start":"2571.19","dur":"1.179","text":"Hm."},"2574.70":{"start":"2574.697","dur":"3.809","text":"I&#39;m not sure I have such a good\nanswer to that question."},"2578.51":{"start":"2578.506","dur":"10","text":"&gt;&gt; [INAUDIBLE]"},"2597.11":{"start":"2597.112","dur":"2.69","text":"&gt;&gt; Okay, so Richard&#39;s suggestion is,"},"2599.80":{"start":"2599.802","dur":"3.276","text":"well this ct is kind of\nlike a linear layer, and"},"2603.08":{"start":"2603.078","dur":"5.892","text":"therefore it&#39;s kind of insured if you\nshould add a non linearity after it."},"2608.97":{"start":"2608.97","dur":"2.61","text":"And that gives you a bit more power."},"2612.75":{"start":"2612.75","dur":"3.467","text":"Maybe that&#39;s right."},"2616.22":{"start":"2616.217","dur":"4.323","text":"Well, we could try it both ways and\nsee if it makes a difference, or"},"2620.54":{"start":"2620.54","dur":"3.13","text":"maybe Shane already has,\nI&#39;m not sure [LAUGH]."},"2623.67":{"start":"2623.67","dur":"2.22","text":"Any other questions?"},"2625.89":{"start":"2625.89","dur":"3.386","text":"Make them a softball\none that I can answer."},"2629.28":{"start":"2629.276","dur":"7.122","text":"&gt;&gt; [LAUGH]\n&gt;&gt; Okay,"},"2636.40":{"start":"2636.398","dur":"5.852","text":"so I had a few more\npictures that went through"},"2642.25":{"start":"2642.25","dur":"6.2","text":"the parts of the LSTM\nwith one more picture."},"2648.45":{"start":"2648.45","dur":"4.07","text":"I&#39;m starting to think I should maybe\nnot dwell on this in much detail."},"2652.52":{"start":"2652.52","dur":"4.48","text":"Cuz we&#39;ve sort of talked about\nthe fact that there are the gates for"},"2657.00":{"start":"2657","dur":"1.68","text":"all the things."},"2658.68":{"start":"2658.68","dur":"7.016","text":"We&#39;re working out the candidate update,\njust like an RNN."},"2665.70":{"start":"2665.696","dur":"4.538","text":"The only bit that I just wanna\nsay one more time is I think"},"2670.23":{"start":"2670.234","dur":"4.636","text":"it&#39;s fair to say that the whole\nsecret of these things,"},"2674.87":{"start":"2674.87","dur":"6.32","text":"is that you&#39;re doing this addition\nwhere you&#39;re adding together."},"2681.19":{"start":"2681.19","dur":"3.4","text":"When in the addition,\nit&#39;s sort of a weighted addition."},"2684.59":{"start":"2684.59","dur":"1.58","text":"But in the addition,"},"2686.17":{"start":"2686.17","dur":"5.57","text":"one choice is you&#39;re just copying\nstuff from the previous time step."},"2691.74":{"start":"2691.74","dur":"5.054","text":"And to the extent that you&#39;re copying\nstuff from the previous time step,"},"2696.79":{"start":"2696.794","dur":"3.846","text":"you have a gradient of 1,\nwhich you&#39;re just pushing."},"2700.64":{"start":"2700.64","dur":"3.509","text":"So you can push error directly\nback across that, and"},"2704.15":{"start":"2704.149","dur":"3.971","text":"you can keep on doing that for\nany number of time steps."},"2708.12":{"start":"2708.12","dur":"4.954","text":"So it&#39;s that plus, having that plus\nwith the previous time step rather"},"2713.07":{"start":"2713.074","dur":"3.186","text":"than having it all multiplied by matrix."},"2716.26":{"start":"2716.26","dur":"6.579","text":"That is the central idea that makes LSTMs\nbe able to have long short-term memory."},"2722.84":{"start":"2722.839","dur":"4.876","text":"And I mean, that has proven to\nbe an incredibly powerful idea,"},"2727.72":{"start":"2727.715","dur":"4.416","text":"and so in general,\nit doesn&#39;t sound that profound, but"},"2732.13":{"start":"2732.131","dur":"5.336","text":"that idea has been sort of driving\na lot of the developments of what&#39;s"},"2737.47":{"start":"2737.467","dur":"4.696","text":"been happening in deep learning\nin the last couple of years."},"2742.16":{"start":"2742.163","dur":"7.292","text":"So we don&#39;t really talk about,\nin this class, about vision systems."},"2749.46":{"start":"2749.455","dur":"2.976","text":"You can do that next quarter in 231N."},"2752.43":{"start":"2752.431","dur":"5.548","text":"But one of the leading ideas and has\nbeen used recently in better systems for"},"2757.98":{"start":"2757.979","dur":"4.931","text":"doing kind of vision systems with\ndeep learning has been the idea of"},"2762.91":{"start":"2762.91","dur":"4.79","text":"residual networks,\ncommonly shortened as ResNets."},"2767.70":{"start":"2767.7","dur":"6.36","text":"And to a first approximation, so"},"2774.06":{"start":"2774.06","dur":"4.74","text":"ResNets is saying gee,\nwe want to be able to build 100 layer"},"2778.80":{"start":"2778.8","dur":"4.6","text":"deep neural networks and\nbe able to train those successfully."},"2783.40":{"start":"2783.4","dur":"1.91","text":"And to a first approximation,"},"2785.31":{"start":"2785.31","dur":"6.18","text":"the way ResNets are doing that is exactly\nthe same idea here with the plus sign."},"2791.49":{"start":"2791.49","dur":"2.725","text":"It&#39;s saying, as you go up each layer,"},"2794.22":{"start":"2794.215","dur":"6.125","text":"we&#39;re going to calculate some non-linear\nfunction using a regular neural net layer."},"2800.34":{"start":"2800.34","dur":"2.131","text":"But will offer the alternative,"},"2802.47":{"start":"2802.471","dur":"4.033","text":"which is that you can just shunt\nstuff up from the layer before,"},"2806.50":{"start":"2806.504","dur":"4.926","text":"add those two together, and\nrepeat over again and go up 100 layers."},"2811.43":{"start":"2811.43","dur":"4.906","text":"And so this plus sign,\nyou may have learned in third grade, but"},"2816.34":{"start":"2816.336","dur":"5.938","text":"turns out plus signs have been a really\nuseful part of modern deep learning."},"2822.27":{"start":"2822.274","dur":"7.452","text":"Okay, Yeah, here is my little picture,\nwhich I&#39;ll just show."},"2829.73":{"start":"2829.726","dur":"4.579","text":"I think you&#39;ll have to sort\nof then slow it down to"},"2834.31":{"start":"2834.305","dur":"4.917","text":"understand that this is sort\nof going backwards from"},"2839.22":{"start":"2839.222","dur":"5.25","text":"Time 128 as to how long\ninformation lasts in an LSTM,"},"2844.47":{"start":"2844.472","dur":"4.548","text":"and it sort of looks\nlike this if I play it."},"2849.02":{"start":"2849.02","dur":"4.505","text":"And so if we then try and drag it back,\nI think, then I can play it more slowly."},"2853.53":{"start":"2853.525","dur":"4.942","text":"All right, so that almost instantaneously,\nthe RNN has less"},"2858.47":{"start":"2858.467","dur":"5.841","text":"information because of\nthe Matrix multiply."},"2864.31":{"start":"2864.308","dur":"3.472","text":"But as you go back,\nthat by the time you&#39;ve gone back so"},"2867.78":{"start":"2867.78","dur":"5.12","text":"at ten times steps, the RNN is\nessentially lost the information."},"2872.90":{"start":"2872.9","dur":"4.13","text":"Whereas the LSTM even be going back,"},"2877.03":{"start":"2877.03","dur":"4.19","text":"it starts loose information, but you know\nyou sort of gain back this sort of more"},"2881.22":{"start":"2881.22","dur":"4.93","text":"like, time step 30 or\nsomething before it&#39;s kind of"},"2886.15":{"start":"2886.15","dur":"3.82","text":"lost all of its information which is sort\nof the intuition I suggested before."},"2889.97":{"start":"2889.97","dur":"6.376","text":"But something like 100 time\nsteps you can get out of a LSTM."},"2896.35":{"start":"2896.346","dur":"5.029","text":"Almost up for a halftime break,\nand the research highlight,"},"2901.38":{"start":"2901.375","dur":"4.257","text":"but before that couple other\nthings I wanted to say,"},"2905.63":{"start":"2905.632","dur":"3.988","text":"here&#39;s just a little bit\nof practical advice."},"2909.62":{"start":"2909.62","dur":"7.22","text":"So both for assignment for or\nfor many people&#39;s final projects."},"2916.84":{"start":"2916.84","dur":"3.989","text":"They&#39;re gonna be wanting\nto train recurrent neural"},"2920.83":{"start":"2920.829","dur":"3.321","text":"networks with LSTMs on a largest scale."},"2924.15":{"start":"2924.15","dur":"2.74","text":"So here is some of the tips\nthat you should know, yes."},"2926.89":{"start":"2926.89","dur":"3.517","text":"So if you wanna build a big\nrecurrent new network,"},"2930.41":{"start":"2930.407","dur":"2.692","text":"definitely use either GRU or an LSTM."},"2933.10":{"start":"2933.099","dur":"3.86","text":"So for any of these recurrent networks,"},"2936.96":{"start":"2936.959","dur":"4.663","text":"initialization is really,\nreally important."},"2941.62":{"start":"2941.622","dur":"5.495","text":"That if your net, recurrent your network\nshould work, if your network isn&#39;t"},"2947.12":{"start":"2947.117","dur":"5.663","text":"working, often times it&#39;s because\nthe initial initialization is bad."},"2952.78":{"start":"2952.78","dur":"5.18","text":"So what are the kind of initialization\nideas that often tend to be important?"},"2957.96":{"start":"2957.96","dur":"5.27","text":"It&#39;s turned to be really useful for\nthe recurrent matrices, that&#39;s the one"},"2963.23":{"start":"2963.23","dur":"4.09","text":"where you&#39;re multiplying by the previous\nhidden state of previous cell state."},"2967.32":{"start":"2967.32","dur":"2.26","text":"It&#39;s really useful to\nmake that one orthogonal."},"2969.58":{"start":"2969.58","dur":"4.02","text":"So there&#39;s chance to use your good\nold-fashioned linear algebra."},"2973.60":{"start":"2973.6","dur":"3.71","text":"There aren&#39;t actually that many\nparameters in a recurrent neural net."},"2977.31":{"start":"2977.31","dur":"4.325","text":"And giving an orthogonal\ninitialization has proved to"},"2981.64":{"start":"2981.635","dur":"4.802","text":"be a better way to kinda get\nthem learning something useful."},"2986.44":{"start":"2986.437","dur":"3.139","text":"Even with sort of these\nideas with GRUs and LSTMs,"},"2989.58":{"start":"2989.576","dur":"5.174","text":"you&#39;re gonna kinda keep multiplying\nthings in a recurrent neural network."},"2994.75":{"start":"2994.75","dur":"4.06","text":"So normally, you wanna have\nyour initialization is small."},"2998.81":{"start":"2998.81","dur":"3.44","text":"If you start off with two large\nvalues that can destroy things,"},"3002.25":{"start":"3002.25","dur":"3.58","text":"try making the numbers smaller."},"3005.83":{"start":"3005.83","dur":"2.31","text":"Here&#39;s a little trick, so"},"3008.14":{"start":"3008.14","dur":"5.7","text":"a lot of the times we initialize\nthings near zero, randomly."},"3013.84":{"start":"3013.84","dur":"5.573","text":"An exception to that is when you&#39;re\nsetting the bias of a forget gate,"},"3019.41":{"start":"3019.413","dur":"4.722","text":"it normally works out much better\nif you set the bias gate for"},"3024.14":{"start":"3024.135","dur":"4.629","text":"the forget gate to a decent size\npositive number like one or"},"3028.76":{"start":"3028.764","dur":"3.315","text":"two or\na random number close to one or two."},"3032.08":{"start":"3032.079","dur":"4.913","text":"That&#39;s sort of effectively saying\nyou should start off paying"},"3036.99":{"start":"3036.992","dur":"3.068","text":"a lot of attention to the distant past."},"3040.06":{"start":"3040.06","dur":"3.32","text":"That&#39;s sort of biasing it\nto keep long term memory."},"3043.38":{"start":"3043.38","dur":"2.65","text":"And that sort of encourages\nyou to get a good model."},"3046.03":{"start":"3046.03","dur":"2.59","text":"Which effectively uses long term memory."},"3048.62":{"start":"3048.62","dur":"5.006","text":"And if the long term past stuff isn&#39;t\nuseful, it can shrink that down."},"3053.63":{"start":"3053.626","dur":"3.657","text":"But if the forget gate starts\noff mainly forgetting stuff,"},"3057.28":{"start":"3057.283","dur":"4.037","text":"it&#39;ll just forget stuff and\nnever change to any other behavior."},"3062.73":{"start":"3062.73","dur":"3.46","text":"In general, these algorithms work much"},"3066.19":{"start":"3066.19","dur":"2.48","text":"better with modern adaptive\nlearning rate algorithms."},"3068.67":{"start":"3068.67","dur":"2.21","text":"We&#39;ve already been using\nAdam in the assignments."},"3070.88":{"start":"3070.88","dur":"5.907","text":"The ones like Adam, AdaDelta,\nRMSprop work a lot better than basic SGD."},"3076.79":{"start":"3076.787","dur":"2.703","text":"You do wanna clip\nthe norms of the gradients."},"3079.49":{"start":"3079.49","dur":"3.509","text":"You can use a number like five,\nthat&#39;ll work fine."},"3083.00":{"start":"3082.999","dur":"2.712","text":"And so,\nwe&#39;ve used dropout in the assignments, but"},"3085.71":{"start":"3085.711","dur":"3.086","text":"we haven&#39;t actually ever talked\nabout it much in lectures."},"3088.80":{"start":"3088.797","dur":"6.013","text":"For RNNs of any sort,\nit&#39;s trivial to do dropout vertically."},"3094.81":{"start":"3094.81","dur":"3.47","text":"And that usually improves performance."},"3098.28":{"start":"3098.28","dur":"1.19","text":"It doesn&#39;t work and"},"3099.47":{"start":"3099.47","dur":"5.19","text":"I either do drop out horizontally\nalong the recurrent connections."},"3104.66":{"start":"3104.66","dur":"3.699","text":"Because if you have reasonable\npercentage of drop out and"},"3108.36":{"start":"3108.359","dur":"4.47","text":"you run it horizontally then within\nthe few time steps, almost every"},"3112.83":{"start":"3112.829","dur":"5.107","text":"dimension will be dropped in one of them,\nand so you have no information flow."},"3117.94":{"start":"3117.936","dur":"5.108","text":"There have been more recent work\nthat&#39;s talked about ways that you"},"3123.04":{"start":"3123.044","dur":"5.199","text":"can successfully do horizontal\ndropout in recurrent networks in,"},"3128.24":{"start":"3128.243","dur":"4.924","text":"including orthongal&#39;s PhD student\nin England who did work on so"},"3133.17":{"start":"3133.167","dur":"3.766","text":"called base in drop out\nthat works well for that."},"3136.93":{"start":"3136.933","dur":"4.002","text":"But quite commonly, it&#39;s still the case\nthat people just drop out vertically and"},"3140.94":{"start":"3140.935","dur":"2.285","text":"don&#39;t drop out at all horizontally."},"3143.22":{"start":"3143.22","dur":"4.12","text":"The final bit of advice is be\npatient if you&#39;re running,"},"3147.34":{"start":"3147.34","dur":"4.24","text":"if you&#39;re learning recurrent\nnets over large data sets,"},"3151.58":{"start":"3151.58","dur":"2.79","text":"it often takes quite a while and\nyou don&#39;t wanna give up."},"3154.37":{"start":"3154.37","dur":"3.35","text":"Sometimes if you just train them\nlong enough start to learn stuff."},"3157.72":{"start":"3157.72","dur":"4.921","text":"This is one of the reasons why we\nreally want to get you guys started"},"3162.64":{"start":"3162.641","dur":"3.221","text":"using GPUs because the fact of the matter,"},"3165.86":{"start":"3165.862","dur":"4.923","text":"if you&#39;re actually trying to do\nthings on decent size data sets,"},"3170.79":{"start":"3170.785","dur":"5.568","text":"you just don&#39;t wanna be trying to train\nin LSTM or GRU without Using a GPU."},"3176.35":{"start":"3176.353","dur":"5.977","text":"One other last tip that we should\nmention some time is ensembling."},"3182.33":{"start":"3182.33","dur":"5.86","text":"If you&#39;d like your numbers to be 2%\nhigher, very effective strategy,"},"3188.19":{"start":"3188.19","dur":"4.2","text":"which again, makes it good to have a GPU,\nis don&#39;t train just one model,"},"3192.39":{"start":"3192.39","dur":"3.86","text":"train ten models and\nyou average their predictions and"},"3196.25":{"start":"3196.25","dur":"3.39","text":"that that normally gives you\nquite significant gains."},"3199.64":{"start":"3199.64","dur":"4.738","text":"So here are some results\nfrom MT Systems trained."},"3204.38":{"start":"3204.378","dur":"2.058","text":"Montreal again."},"3206.44":{"start":"3206.436","dur":"3.882","text":"So it&#39;s different language pairs."},"3210.32":{"start":"3210.318","dur":"2.343","text":"The red ones is a single model."},"3212.66":{"start":"3212.661","dur":"4.949","text":"The purple ones are training 8 models,\nand in this case,"},"3217.61":{"start":"3217.61","dur":"4.555","text":"it&#39;s actually just majority\nvoting them together."},"3222.17":{"start":"3222.165","dur":"3.478","text":"But you can also sort of\naverage their predictions and"},"3225.64":{"start":"3225.643","dur":"5.255","text":"you can see it&#39;s just giving very nice\ngains in performance using the measure for"},"3230.90":{"start":"3230.898","dur":"3.494","text":"mt performance which I&#39;ll\nexplain after the break."},"3234.39":{"start":"3234.392","dur":"7.868","text":"But we&#39;re now gonna have Michael up\nto talk about the research highlight."},"3242.26":{"start":"3242.26","dur":"1.813","text":"And I&#39;ll quickly explain\nit until the video is in"},"3244.07":{"start":"3244.073","dur":"0.561","text":"there-\n&gt;&gt; Okay."},"3244.63":{"start":"3244.634","dur":"1.375","text":"&gt;&gt; After the picture."},"3246.01":{"start":"3246.009","dur":"1.491","text":"&gt;&gt; Okay."},"3248.52":{"start":"3248.52","dur":"0.672","text":"Hi, everyone."},"3249.19":{"start":"3249.192","dur":"4.188","text":"I&#39;m gonna be presenting the paper\nLip Reading Sentences in the Wild."},"3254.63":{"start":"3254.63","dur":"4.44","text":"So our task is basically taking a video,\nwhich we preprocessed into"},"3259.07":{"start":"3259.07","dur":"4.83","text":"a sequence of lip-centered images,\nwith or without audio."},"3263.90":{"start":"3263.9","dur":"4.539","text":"And we&#39;re trying to predict like the words\nthat are being said in the video."},"3268.44":{"start":"3268.439","dur":"2.836","text":"&gt;&gt; Just slide after that one."},"3274.28":{"start":"3274.277","dur":"2.238","text":"Maybe it doesn&#39;t"},"3303.06":{"start":"3303.061","dur":"2.999","text":"&gt;&gt; The government will pay for both sides."},"3306.06":{"start":"3306.06","dur":"3.2","text":"&gt;&gt; We have to look at whether it\n&gt;&gt; Not."},"3309.26":{"start":"3309.26","dur":"3.092","text":"Said security had been\nstepped up in Britain."},"3329.49":{"start":"3329.494","dur":"4.658","text":"&gt;&gt; Cool, so anyway,\nit&#39;s hard to do lip reading."},"3334.15":{"start":"3334.152","dur":"4.188","text":"So anyway, and for the rest of this I&#39;ll\ntalk about what architecture they use,"},"3338.34":{"start":"3338.34","dur":"3.983","text":"which is, they deem the watch,\nlisten, attend, and spell model."},"3342.32":{"start":"3342.323","dur":"3.423","text":"&gt;&gt; Gonna talk about some of these training\nstrategies that might also be helpful for"},"3345.75":{"start":"3345.746","dur":"0.965","text":"your final projects."},"3346.71":{"start":"3346.711","dur":"2.204","text":"There&#39;s also the dataset and"},"3348.92":{"start":"3348.915","dur":"5.209","text":"the results was actually surpassing\nlike a professional lip reader."},"3354.12":{"start":"3354.124","dur":"4.086","text":"So, the architecture basically\nbreaks down into three components."},"3358.21":{"start":"3358.21","dur":"5.088","text":"We have a watch component which takes\nin the visual and the listening"},"3363.30":{"start":"3363.298","dur":"6.069","text":"component which takes in the audio and\nthese feed information to the attend, and"},"3369.37":{"start":"3369.367","dur":"5.103","text":"spell module which outputs\nthe prediction one character at a time."},"3375.70":{"start":"3375.696","dur":"4.714","text":"And they also use this with like, just the\nwatch module or just the listen module."},"3382.51":{"start":"3382.51","dur":"4.1","text":"To go into slightly more detail,\nfor the watch module,"},"3386.61":{"start":"3386.61","dur":"5.52","text":"we take a sliding window over\nlike the face centered images and"},"3392.13":{"start":"3392.13","dur":"3.32","text":"feed that into a CNN,\nwhich then the output of"},"3395.45":{"start":"3395.45","dur":"5.79","text":"the CNN gets fed into an LSTM\nmuch size over the time steps."},"3401.24":{"start":"3401.24","dur":"5.52","text":"We output a single state vector S of v,\nas well as the set of"},"3406.76":{"start":"3406.76","dur":"5.757","text":"output vectors L of v and\nthe listen module is very similar."},"3412.52":{"start":"3412.517","dur":"2.871","text":"We take the pre-processed speech and"},"3415.39":{"start":"3415.388","dur":"5.203","text":"we again site over using the LSTM,\nand we have another state vector,"},"3420.59":{"start":"3420.591","dur":"5.039","text":"and another set of output vectors,\nand then in the decoding step."},"3426.94":{"start":"3426.94","dur":"4.68","text":"So we have an LSTM as a really\nsteps of during the decoding and"},"3431.62":{"start":"3431.62","dur":"5.04","text":"the initial hidden state is initialized\nas the concatenation of the two hidden"},"3436.66":{"start":"3436.66","dur":"4.86","text":"states from the two previous\nmodules as well as we have"},"3441.52":{"start":"3441.52","dur":"4.82","text":"like a dual attention mechanism\nwhich takes in the output"},"3446.34":{"start":"3446.34","dur":"4.69","text":"vectors from each of their respective\nmodules, and we take those together, and"},"3451.03":{"start":"3451.03","dur":"4.18","text":"we make our prediction using a softmax\nover a multi-layer procepteron."},"3457.32":{"start":"3457.32","dur":"3.04","text":"And so, one strategy that uses\ncalled curriculum learning."},"3460.36":{"start":"3460.36","dur":"5.611","text":"So ordinarily, when you&#39;re training\nthis sequence to sequence models,"},"3465.97":{"start":"3465.971","dur":"4.194","text":"you might be tend to just use\none full sentence at a time."},"3470.17":{"start":"3470.165","dur":"5.615","text":"Tip by what they do on curriculum learning\nis you start with the word length like"},"3475.78":{"start":"3475.78","dur":"5.872","text":"segment and then you can slowly increase\nthe length of your training sequences and"},"3481.65":{"start":"3481.652","dur":"5.361","text":"what happens is you&#39;re actually like\nthe idea is you&#39;re trying to learn,"},"3487.01":{"start":"3487.013","dur":"3.574","text":"like slowly build up the learning for\nthe model and"},"3490.59":{"start":"3490.587","dur":"5.827","text":"what happens is it ends up converging\nfaster as well as decreasing overfitting."},"3496.41":{"start":"3496.414","dur":"3.839","text":"Another thing that they use\nis called scheduled sampling."},"3500.25":{"start":"3500.253","dur":"4.396","text":"So ordinarily during training,\nyou&#39;ll be using"},"3504.65":{"start":"3504.649","dur":"4.71","text":"the ground truth input like\ncharacter sequence, but"},"3509.36":{"start":"3509.359","dur":"5.234","text":"during the test time you\nwouldn&#39;t be using that you&#39;d just"},"3514.59":{"start":"3514.593","dur":"5.597","text":"be using your previous prediction\nafter every time step."},"3520.19":{"start":"3520.19","dur":"3.79","text":"So what you do in scheduled sampling is\nkind of like bridge the difference in"},"3523.98":{"start":"3523.98","dur":"3.46","text":"scenarios between training and\ntesting is that you actually just for"},"3527.44":{"start":"3527.44","dur":"4.03","text":"a random small probability,\nlike sample from the previous input"},"3531.47":{"start":"3531.47","dur":"3.67","text":"instead of the ground truth input for\nthat time step during training."},"3538.18":{"start":"3538.18","dur":"6.756","text":"So the dataset was taken from the authors\ncollected it from the BBC News and"},"3544.94":{"start":"3544.936","dur":"4.684","text":"they have like dataset that&#39;s much\nlarger than the previous ones"},"3549.62":{"start":"3549.62","dur":"4.17","text":"out there with over 17,000\nvocabulary words and"},"3553.79":{"start":"3553.79","dur":"4.37","text":"the other the quite a bit like processing\nto like some other things on the lips, and"},"3558.16":{"start":"3558.16","dur":"2.96","text":"do like the alignment of the audio,\nand the visuals."},"3563.81":{"start":"3563.81","dur":"5.257","text":"So, just to talk about the results, I\nguess the most eye popping result is that"},"3569.07":{"start":"3569.067","dur":"5.102","text":"they gave the test set to actually like\na company that does like professional"},"3574.17":{"start":"3574.169","dur":"5.259","text":"lip reading and they&#39;re only able to get\nabout like one in four words correct or"},"3579.43":{"start":"3579.428","dur":"5.121","text":"as this model was able to get one in two,\nroughly, based on word error rate."},"3584.55":{"start":"3584.549","dur":"4.435","text":"And they also did some other\nexperience as well with looking at,"},"3588.98":{"start":"3588.984","dur":"3.057","text":"if you combine the lips\nversion with the audio,"},"3592.04":{"start":"3592.041","dur":"5.428","text":"you get like a slightly better model which\nshows that using both modalities improves"},"3597.47":{"start":"3597.469","dur":"4.68","text":"the model as well as looking at what\nhappens if you add noise to the model."},"3602.15":{"start":"3602.149","dur":"1.301","text":"Great.\nThanks."},"3603.45":{"start":"3603.45","dur":"5.948","text":"&gt;&gt; [APPLAUSE]\n&gt;&gt; Thanks, Michael."},"3609.40":{"start":"3609.398","dur":"1.981","text":"Yeah, so obviously,\na lot of details there."},"3611.38":{"start":"3611.379","dur":"4.832","text":"But again, that&#39;s kind of an example of\nwhat&#39;s been happening with deep learning"},"3616.21":{"start":"3616.211","dur":"4.9","text":"where you&#39;re taking this basic model\narchitecture, things like LSTM and saying,"},"3621.11":{"start":"3621.111","dur":"3.381","text":"here&#39;s another problem,\nlet&#39;s try it on that as well and"},"3624.49":{"start":"3624.492","dur":"2.441","text":"it turns out to work fantastically well."},"3626.93":{"start":"3626.933","dur":"1.432","text":"Let&#39;s say, 20 minutes left."},"3628.37":{"start":"3628.365","dur":"3.504","text":"I&#39;ll see how high I can get in teaching\neverything else about it on machine"},"3631.87":{"start":"3631.869","dur":"0.744","text":"translation."},"3632.61":{"start":"3632.613","dur":"4.215","text":"So it&#39;s something I did just want\nto explain is so, back here and"},"3636.83":{"start":"3636.828","dur":"4.622","text":"in general, when we&#39;ve been showing\nmachine translation results."},"3641.45":{"start":"3641.45","dur":"2.762","text":"We&#39;ve been divvying these\ngraphs that up is good and"},"3644.21":{"start":"3644.212","dur":"4.135","text":"what it&#39;s been measuring with these\nnumbers are things called blue scores."},"3648.35":{"start":"3648.347","dur":"5.704","text":"So, I wanted to give you some idea of how\nand why we evaluate machine translation."},"3654.05":{"start":"3654.051","dur":"6.215","text":"So the central thing to know about machine\ntranslation is if you take a paragraph or"},"3660.27":{"start":"3660.266","dur":"4.144","text":"text and give it to ten\ndifferent humans translators,"},"3664.41":{"start":"3664.41","dur":"3.93","text":"you&#39;ll get back ten\ndifferent translations."},"3668.34":{"start":"3668.34","dur":"4.728","text":"There&#39;s no correct answer\nas to how to translate"},"3673.07":{"start":"3673.068","dur":"3.469","text":"a sentence into another language."},"3676.54":{"start":"3676.537","dur":"4.47","text":"And in practice, most of the time\nall translations are imperfect and"},"3681.01":{"start":"3681.007","dur":"5.084","text":"it&#39;s kind of deciding what you wanna pay\nmost attention to is that do you want to"},"3686.09":{"start":"3686.091","dur":"5.088","text":"maximally preserve the metaphor that\nthe person used in the source language or"},"3691.18":{"start":"3691.179","dur":"3.777","text":"do you wanna more directly\nconvey the meaning it conveys,"},"3694.96":{"start":"3694.956","dur":"5.71","text":"because that metaphor won&#39;t really be\nfamiliar to people in the target language."},"3700.67":{"start":"3700.666","dur":"2.809","text":"Do you want to choose sort\nof short direct words,"},"3703.48":{"start":"3703.475","dur":"2.887","text":"because it&#39;s written in a short,\ndirect style?"},"3706.36":{"start":"3706.362","dur":"2.203","text":"Or do you more want to sort of,"},"3708.57":{"start":"3708.565","dur":"4.419","text":"you choose a longer word that&#39;s\na more exact translation?"},"3712.98":{"start":"3712.984","dur":"3.888","text":"There&#39;s all of these decisions and\nthings and in some sense a translator is"},"3716.87":{"start":"3716.872","dur":"2.838","text":"optimizing over if we do it\nin machine learning terms,"},"3719.71":{"start":"3719.71","dur":"2.484","text":"but the reality is it&#39;s\nsort of not very clear."},"3722.19":{"start":"3722.194","dur":"1.786","text":"There are a lot of choices."},"3723.98":{"start":"3723.98","dur":"3.804","text":"You have lots of syntactic choices\nas whether you make it a passive or"},"3727.78":{"start":"3727.784","dur":"2.08","text":"an active and word order, and so on."},"3729.86":{"start":"3729.864","dur":"0.932","text":"No right answer."},"3730.80":{"start":"3730.796","dur":"3.842","text":"So we just can&#39;t have it like a lot\nthings of saying, here&#39;s the accuracy,"},"3734.64":{"start":"3734.638","dur":"2.042","text":"that was what you were meant to use."},"3736.68":{"start":"3736.68","dur":"1.593","text":"So, how do you do it?"},"3738.27":{"start":"3738.273","dur":"4.177","text":"So, one way to do MT evaluation\nis to do it manually."},"3742.45":{"start":"3742.45","dur":"4.204","text":"You get human beings to look\nat translations and to say,"},"3746.65":{"start":"3746.654","dur":"1.435","text":"how good they are."},"3748.09":{"start":"3748.089","dur":"1.812","text":"And to this day, basically,"},"3749.90":{"start":"3749.901","dur":"4.639","text":"that&#39;s regarded as the gold standard\nof machine translation evaluation,"},"3754.54":{"start":"3754.54","dur":"4.2","text":"because we don&#39;t have a better\nway to fully automate things."},"3758.74":{"start":"3758.74","dur":"4.472","text":"So one way of doing that is things\nlike Likert scales where you&#39;re"},"3763.21":{"start":"3763.212","dur":"3.579","text":"getting humans to judge\ntranslations to adequacy,"},"3766.79":{"start":"3766.791","dur":"5.286","text":"which is how well they convey the meaning\nof the source and fluency which is for"},"3772.08":{"start":"3772.077","dur":"4.415","text":"how natural the output sentence\nsounds in the target language."},"3776.49":{"start":"3776.492","dur":"5.783","text":"Commonly, a way that&#39;s more easily\nmeasurable that people prefer is actually"},"3782.28":{"start":"3782.275","dur":"5.783","text":"if you&#39;re comparing systems for goodness\nis that you directly ask human beings"},"3788.06":{"start":"3788.058","dur":"5.545","text":"to do pairwise judgments of which is\nbetter translation A or translation B."},"3793.60":{"start":"3793.603","dur":"3.984","text":"I mean, it turns out that even\nthat is incredibly hard for"},"3797.59":{"start":"3797.587","dur":"5.484","text":"humans to do as someone who has sat around\ndoing this task of human evaluation."},"3803.07":{"start":"3803.071","dur":"2.957","text":"I mean, all the time, it&#39;s kind of okay,"},"3806.03":{"start":"3806.028","dur":"4.765","text":"this one made a bad word choice here and\nthis one got the wrong verb form"},"3810.79":{"start":"3810.793","dur":"3.385","text":"there which of these do I\nregard as a worse error."},"3814.18":{"start":"3814.178","dur":"4.552","text":"So it&#39;s a difficult thing, but\nwe use the data we can from human beings."},"3818.73":{"start":"3818.73","dur":"3.09","text":"Okay, that&#39;s still the best\nthing that we can do."},"3821.82":{"start":"3821.82","dur":"1.71","text":"It has problems."},"3823.53":{"start":"3823.53","dur":"7.24","text":"Basically, it&#39;s slow and expensive to get\nhuman beings to judge translation quality."},"3830.77":{"start":"3830.77","dur":"1.77","text":"So what else could we do?"},"3832.54":{"start":"3832.54","dur":"4.914","text":"Well, another obvious idea is to say,\nwell, If we can embed machine"},"3837.45":{"start":"3837.454","dur":"5.538","text":"translation into some task, we can just\nsee which is more easily a valuable."},"3842.99":{"start":"3842.992","dur":"6.368","text":"We could just see which MT system\nlets us do the final task better."},"3849.36":{"start":"3849.36","dur":"4.24","text":"So, we&#39;d like to do question answering\nover foreign language documents."},"3853.60":{"start":"3853.6","dur":"3.717","text":"We&#39;ll just to get our question\nanswers correct score, and"},"3857.32":{"start":"3857.317","dur":"2.316","text":"they&#39;ll be much easier to measure."},"3859.63":{"start":"3859.633","dur":"2.772","text":"And that&#39;s something that you can do, but"},"3862.41":{"start":"3862.405","dur":"3.472","text":"it turns out that that often\nisn&#39;t very successful."},"3865.88":{"start":"3865.877","dur":"3.849","text":"Cuz commonly your accuracy\non the downstream task is"},"3869.73":{"start":"3869.726","dur":"4.727","text":"very little affected by many of\nthe fine points of translation."},"3874.45":{"start":"3874.453","dur":"4.487","text":"An extreme example of that is sort of\nlike cross-lingual information retrieval."},"3878.94":{"start":"3878.94","dur":"2.75","text":"When you&#39;re just wanting\nto retrieve relevant"},"3881.69":{"start":"3881.69","dur":"2.31","text":"documents to a query in another language."},"3884.00":{"start":"3884","dur":"4.113","text":"That providing you can kind of produce\nsome of the main content words in"},"3888.11":{"start":"3888.113","dur":"5.018","text":"the translation, it really doesn&#39;t matter\nhow you screw up the details of syntax and"},"3893.13":{"start":"3893.131","dur":"1.134","text":"verb inflection."},"3894.27":{"start":"3894.265","dur":"1.645","text":"It&#39;s not really gonna affect your score."},"3897.65":{"start":"3897.65","dur":"3","text":"Okay, so what people have\nwanted to have is a direct"},"3902.07":{"start":"3902.07","dur":"4.27","text":"metric that is fast and cheap to apply."},"3906.34":{"start":"3906.34","dur":"4.93","text":"And for a long time, I think no one\nthought there was such a thing."},"3911.27":{"start":"3911.27","dur":"4.69","text":"And so\nthen starting in the very early 2000s,"},"3915.96":{"start":"3915.96","dur":"4.96","text":"people at IBM suggested\nthis first idea of, hey,"},"3920.92":{"start":"3920.92","dur":"5.82","text":"here&#39;s a cheap way in which we can\nmeasure word translation quality."},"3926.74":{"start":"3926.74","dur":"2.82","text":"And so they called it the BLEU metric."},"3929.56":{"start":"3929.56","dur":"4.764","text":"And so\nhere was the idea of how they do that."},"3934.32":{"start":"3934.324","dur":"5.216","text":"What they said is let us\nproduce reference translations."},"3939.54":{"start":"3939.54","dur":"4.9","text":"We know that there are many, many possible\nways that something can be translated."},"3944.44":{"start":"3944.44","dur":"8.71","text":"But let&#39;s get a human being to\nproduce a reference translation."},"3953.15":{"start":"3953.15","dur":"4.824","text":"So what we are going to do is then we&#39;re\ngoing to have a reference translation by"},"3957.97":{"start":"3957.974","dur":"3.966","text":"a human, and\nwe&#39;re going to have a machine translation."},"3961.94":{"start":"3961.94","dur":"4.682","text":"And to a first approximation we&#39;re\ngoing to say that the machine"},"3966.62":{"start":"3966.622","dur":"4.868","text":"translation is good to the extent\nthat you can find word n-grams."},"3971.49":{"start":"3971.49","dur":"4.201","text":"So sequences of words like three\nwords in a row, two words in a row,"},"3975.69":{"start":"3975.691","dur":"4.619","text":"which also appear in the reference\ntranslation anywhere."},"3980.31":{"start":"3980.31","dur":"1.636","text":"So what are the elements of this?"},"3981.95":{"start":"3981.946","dur":"6.834","text":"So by having multi-word sequences,\nthat&#39;s meant to be trying to"},"3988.78":{"start":"3988.78","dur":"4.88","text":"judge whether you have some understanding\nof the sort of right syntax and arguments."},"3993.66":{"start":"3993.66","dur":"3.61","text":"Because you&#39;re much more likely\nto match a four word sequence"},"3997.27":{"start":"3997.27","dur":"3.26","text":"if it&#39;s not just you&#39;ve\ngot a bag of keywords."},"4000.53":{"start":"4000.53","dur":"3.56","text":"You actually understand something\nof the syntax of the sentence."},"4004.09":{"start":"4004.09","dur":"4.62","text":"The fact that you can match it anywhere\nis meant to be dealing with the fact that"},"4008.71":{"start":"4008.71","dur":"3.15","text":"human languages normally have\nquite flexible word order."},"4011.86":{"start":"4011.86","dur":"5.679","text":"So it&#39;s not adequate to insist that\nthe phrases appear in the same word order."},"4017.54":{"start":"4017.539","dur":"4.964","text":"Of course, in general in English, a lot of\nthe time you can say, last night I went"},"4022.50":{"start":"4022.503","dur":"4.537","text":"to my friend&#39;s place, or,\nI went to my friend&#39;s place last night."},"4027.04":{"start":"4027.04","dur":"2.85","text":"And it seems like you should\nget credit for last night"},"4029.89":{"start":"4029.89","dur":"2.55","text":"regardless of whether you put it at\nthe beginning or the end of the sentence."},"4033.91":{"start":"4033.91","dur":"3.923","text":"So, that was the general idea\nin slightly more detail."},"4037.83":{"start":"4037.833","dur":"2.737","text":"The BLEU measure is a precision score."},"4040.57":{"start":"4040.57","dur":"4.455","text":"So it&#39;s looking at whether\nn-grams that are in the machine"},"4045.03":{"start":"4045.025","dur":"4.287","text":"translation also appear in\nthe reference translation."},"4049.31":{"start":"4049.312","dur":"2.398","text":"There are a couple of fine points then."},"4051.71":{"start":"4051.71","dur":"5.03","text":"You are only allowed to count for\na certain n and n-gram once."},"4056.74":{"start":"4056.74","dur":"5.06","text":"So if in your translation,\nthe airport appears three times,"},"4061.80":{"start":"4061.8","dur":"2.86","text":"but there&#39;s only one\nthe airport in the reference,"},"4064.66":{"start":"4064.66","dur":"3.98","text":"you&#39;re only allowed to count one of\nthem as correct, not all three of them."},"4068.64":{"start":"4068.64","dur":"5.26","text":"And then there&#39;s this other trick that we\nhave, this thing called a brevity penalty."},"4073.90":{"start":"4073.9","dur":"3.676","text":"Because if it&#39;s purely\na precision-oriented measure,"},"4077.58":{"start":"4077.576","dur":"4.356","text":"saying is what appears in the machine\ntranslation in the reference."},"4081.93":{"start":"4081.932","dur":"1.84","text":"There are games you could play,"},"4083.77":{"start":"4083.772","dur":"3.626","text":"like you could just translate\nevery passage with the word the."},"4087.40":{"start":"4087.398","dur":"3.889","text":"Because if it&#39;s English the word the is\npretty sure to appear somewhere in"},"4091.29":{"start":"4091.287","dur":"2.835","text":"the reference translation,\nand get precision one."},"4094.12":{"start":"4094.122","dur":"2.008","text":"And that seems like it&#39;s cheating."},"4096.13":{"start":"4096.13","dur":"5.345","text":"So if you&#39;re making what your translation\nis shorter than the human translations,"},"4101.48":{"start":"4101.475","dur":"3.565","text":"you&#39;ll lose."},"4105.04":{"start":"4105.04","dur":"6.21","text":"Okay, so more formally, so you&#39;re doing\nthis with n-grams up to a certain size."},"4111.25":{"start":"4111.25","dur":"4.08","text":"Commonly it&#39;s four so you use single\nwords, pairs of words, triples,"},"4115.33":{"start":"4115.33","dur":"1.34","text":"and four words."},"4116.67":{"start":"4116.67","dur":"2.226","text":"You work out this kind\nof precision of each."},"4118.90":{"start":"4118.896","dur":"4.179","text":"And then you&#39;re working out a kind\nof a weighted geometric mean"},"4123.08":{"start":"4123.075","dur":"1.78","text":"of those precisions."},"4124.86":{"start":"4124.855","dur":"2.898","text":"And you multiplying that\nby brevity penalty."},"4127.75":{"start":"4127.753","dur":"4.949","text":"And the brevity penalty penalizes\nyou if your translation"},"4132.70":{"start":"4132.702","dur":"3.844","text":"is shorter than the reference translation."},"4136.55":{"start":"4136.546","dur":"4.044","text":"There are some details here, but\nmaybe I&#39;ll just skip them and go ahead."},"4140.59":{"start":"4140.59","dur":"6.02","text":"So there&#39;s one other idea then which is,\nwell, what about this big problem that,"},"4146.61":{"start":"4146.61","dur":"4.481","text":"well, there are a lot of different\nways to translate things."},"4151.09":{"start":"4151.091","dur":"4.248","text":"And there&#39;s no guarantee that your\ntranslation could be great, and"},"4155.34":{"start":"4155.339","dur":"3.531","text":"it might just not match\nthe human&#39;s translation."},"4158.87":{"start":"4158.87","dur":"5.356","text":"And so the answer to that that\nthe original IBM paper suggested"},"4164.23":{"start":"4164.226","dur":"5.871","text":"was what we should do is collect\na bunch of reference translations."},"4170.10":{"start":"4170.097","dur":"3.993","text":"And the suggested number that&#39;s\nbeen widely used was four."},"4174.09":{"start":"4174.09","dur":"5.59","text":"And so then, most likely,\nif you&#39;re giving a good translation,"},"4179.68":{"start":"4179.68","dur":"3.39","text":"it&#39;ll appear in one of\nthe reference translations."},"4183.07":{"start":"4183.07","dur":"2.822","text":"And then, you&#39;ll get a matching n-gram."},"4185.89":{"start":"4185.892","dur":"3.218","text":"Now, of course,\nthat&#39;s the sort of a statistical argument."},"4189.11":{"start":"4189.11","dur":"2.671","text":"Cuz you might have a really\ngood translation and"},"4191.78":{"start":"4191.781","dur":"2.218","text":"none of the four translators chose it."},"4194.00":{"start":"4193.999","dur":"3.786","text":"And the truth is then in\nthat case you just lose."},"4197.79":{"start":"4197.785","dur":"4.96","text":"And indeed what&#39;s happened in more\nrecent work is quite a lot of the time,"},"4202.75":{"start":"4202.745","dur":"4.973","text":"actually, the BLEU measure is only\nrun with one reference translation."},"4207.72":{"start":"4207.718","dur":"2.415","text":"And that&#39;s seems a little bit cheap."},"4210.13":{"start":"4210.133","dur":"4.789","text":"And it&#39;s certainly the case that if you&#39;re\nrunning with one reference translation,"},"4214.92":{"start":"4214.922","dur":"4.526","text":"you&#39;re either just lucky or unlucky as to\nwhether you guessed to translate the way"},"4219.45":{"start":"4219.448","dur":"1.822","text":"the translator translates."},"4221.27":{"start":"4221.27","dur":"4.373","text":"But you can make a sort of a statistical\nargument which by and large is valid."},"4225.64":{"start":"4225.643","dur":"2.906","text":"That if you&#39;re coming up\nwith good translations,"},"4228.55":{"start":"4228.549","dur":"4.81","text":"providing there&#39;s no correlation somehow\nbetween one system and the translator."},"4233.36":{"start":"4233.359","dur":"4.656","text":"That you&#39;d still expect on balance that\nyou&#39;ll get a higher score if you&#39;re"},"4238.02":{"start":"4238.015","dur":"2.935","text":"consistently giving better translations."},"4240.95":{"start":"4240.95","dur":"2.31","text":"And broadly speaking, that&#39;s right."},"4243.26":{"start":"4243.26","dur":"6.185","text":"Though this problem of correlation does\nactually start to rear its head, right?"},"4249.45":{"start":"4249.445","dur":"4.679","text":"That if the reference translator always\ntranslated the things as US, and"},"4254.12":{"start":"4254.124","dur":"5.212","text":"one system translates with US, and the\nother one translates with United States."},"4259.34":{"start":"4259.336","dur":"1.875","text":"Kind of one person will get lucky, and"},"4261.21":{"start":"4261.211","dur":"2.936","text":"the other one will get unlucky\nin a kind of a correlated way."},"4264.15":{"start":"4264.147","dur":"2.857","text":"And that can create problems."},"4267.00":{"start":"4267.004","dur":"5.651","text":"So even though it was very simple\nwhen BLEU was initially introduced,"},"4272.66":{"start":"4272.655","dur":"4.79","text":"it seemed to be miraculously\ngood that it just corresponded"},"4277.45":{"start":"4277.445","dur":"5.26","text":"really well with human judgments\nof translation quality."},"4282.71":{"start":"4282.705","dur":"4.18","text":"Rarely do you see an empirical\ndata set that&#39;s as linear as that."},"4286.89":{"start":"4286.885","dur":"2.71","text":"And so this seemed really awesome."},"4289.60":{"start":"4289.595","dur":"4.16","text":"Like many things that\nare surrogate metrics,"},"4293.76":{"start":"4293.755","dur":"5.015","text":"there are a lot of surrogate metrics that\nwork really well If no one is trying"},"4298.77":{"start":"4298.77","dur":"5.09","text":"to optimize them but don&#39;t work so well\nonce people are trying to optimize them."},"4303.86":{"start":"4303.86","dur":"1.76","text":"So what happen then was,"},"4305.62":{"start":"4305.62","dur":"4.07","text":"everyone evaluated their systems\non BLEU scores and so therefore,"},"4309.69":{"start":"4309.69","dur":"4.28","text":"all researchers worked on how to make\ntheir systems have better BLEU scores."},"4313.97":{"start":"4313.97","dur":"4.88","text":"And then what happened is this\ncorrelation graph went way down."},"4318.85":{"start":"4318.85","dur":"4.97","text":"And so the truth is now that current,\nand this relates to the sort of"},"4323.82":{"start":"4323.82","dur":"3.08","text":"when I was saying the Google\nresults were exaggerated."},"4326.90":{"start":"4326.9","dur":"7.48","text":"The truth is that current MT systems\nproduce BLEU scores that are very similar"},"4334.38":{"start":"4334.38","dur":"4.06","text":"to human translations for many language\npairs which reflects the fact that"},"4338.44":{"start":"4338.44","dur":"5.1","text":"different human beings are quite creative\nand vary in how they translate sensors."},"4343.54":{"start":"4343.54","dur":"4.23","text":"But in truth, the quality of machine\ntranslation is still well below"},"4347.77":{"start":"4347.77","dur":"3.881","text":"the quality of human translation."},"4351.65":{"start":"4351.651","dur":"3.544","text":"Okay, few minutes left to\nsay a bit more about MT."},"4355.20":{"start":"4355.195","dur":"2.4","text":"I think I can&#39;t get through\nall this material, but"},"4357.60":{"start":"4357.595","dur":"3.59","text":"let me just give you a little\nbit of a sense of some of it."},"4362.20":{"start":"4362.2","dur":"4.71","text":"Okay, so one of the big problems you\nhave if you&#39;ve tried to build something,"},"4366.91":{"start":"4366.91","dur":"2.26","text":"any kind of generation system,"},"4369.17":{"start":"4369.17","dur":"5.9","text":"where you&#39;re generating words is you have\na problem that there are a lot of words."},"4375.07":{"start":"4375.07","dur":"3.1","text":"Languages have very large vocabularies."},"4378.17":{"start":"4378.17","dur":"4.31","text":"So from the hidden state,\nwhat we&#39;re doing is multiplying by this"},"4382.48":{"start":"4382.48","dur":"5.26","text":"matrix of Softmax parameters,\nwhich is the size of the vocabulary"},"4387.74":{"start":"4387.74","dur":"4.73","text":"times the size of the hidden\nstate doing this Softmax."},"4392.47":{"start":"4392.47","dur":"2.21","text":"And that&#39;s giving us\nthe probability of different words."},"4396.02":{"start":"4396.02","dur":"3.06","text":"And so the problem is if you wanna\nhave a very large vocabulary,"},"4399.08":{"start":"4399.08","dur":"5.55","text":"you spend a huge amount of time just doing\nthese Softmaxes over, and over again."},"4404.63":{"start":"4404.63","dur":"5.01","text":"And so, for instance, you saw that in\nthe kind of pictures of the Google system,"},"4409.64":{"start":"4409.64","dur":"4.55","text":"that over half of their\ncomputational power was just going"},"4414.19":{"start":"4414.19","dur":"4.43","text":"into calculating these Softmax so\nthat&#39;s being a real problem."},"4418.62":{"start":"4418.62","dur":"4.11","text":"So something people have worked\non quite a lot is how can we"},"4422.73":{"start":"4422.73","dur":"2.4","text":"string the cost of that computation."},"4426.24":{"start":"4426.24","dur":"3.41","text":"Well one thing we can do is say,\nha, let&#39;s use a smaller vocabulary."},"4429.65":{"start":"4429.65","dur":"4.7","text":"Let&#39;s only use a 50,000 word\nvocabulary for our MT system, and"},"4434.35":{"start":"4434.35","dur":"2.64","text":"some of the early MT\nwork did precisely that."},"4436.99":{"start":"4436.99","dur":"4.98","text":"But the problem is, that if you do that,\nyou start with lively sentences."},"4441.97":{"start":"4441.97","dur":"4.68","text":"And instead what you get is unk,\nunk, unk because all of"},"4446.65":{"start":"4446.65","dur":"5.62","text":"the interesting words in the sentence fall\noutside of your 50,000 word vocabulary."},"4452.27":{"start":"4452.27","dur":"5.92","text":"And those kind of sentences are not very\ngood ones to show that human beings,"},"4458.19":{"start":"4458.19","dur":"2.41","text":"because they don&#39;t like them very much."},"4460.60":{"start":"4460.6","dur":"5.14","text":"So, it seems like we need to\nsomehow do better than that."},"4465.74":{"start":"4465.74","dur":"4.8","text":"So, there&#39;s been work on, well, how can\nwe more effectively do the softmaxes"},"4470.54":{"start":"4470.54","dur":"2.93","text":"without having to do as much computation."},"4473.47":{"start":"4473.47","dur":"2.93","text":"And so,\nthere have been some ideas on that."},"4476.40":{"start":"4476.4","dur":"5.02","text":"One idea is to sort of have a hierarchical\nSoftmax where we do the standard"},"4481.42":{"start":"4481.42","dur":"3.09","text":"computer scientist trick\nof putting a tree structure"},"4484.51":{"start":"4484.51","dur":"2.32","text":"to improve our amount of computation."},"4486.83":{"start":"4486.83","dur":"3.94","text":"So if you can sort of divide the\nvocabulary into sort of tree pieces and"},"4490.77":{"start":"4490.77","dur":"4.08","text":"divide down branches of the tree,\nwe can do less computation."},"4496.07":{"start":"4496.07","dur":"4.097","text":"Remember, we did noise\ncontrast to destination for"},"4500.17":{"start":"4500.167","dur":"3.831","text":"words of that was a way\nof avoiding computation."},"4504.00":{"start":"4503.998","dur":"2.862","text":"Those are possible ways to do things."},"4506.86":{"start":"4506.86","dur":"2.5","text":"They are not very\nGPU-friendly unfortunately."},"4509.36":{"start":"4509.36","dur":"4.56","text":"Once you start taking branches down\nthe tree, you then can&#39;t do the kind"},"4513.92":{"start":"4513.92","dur":"3.9","text":"of nice just bang bang bang type\nof computations down to GPU."},"4517.82":{"start":"4517.82","dur":"4.11","text":"So there&#39;s been on work on coming\nup with alternatives to that, and"},"4521.93":{"start":"4521.93","dur":"3","text":"I wanted to mention one example of this."},"4524.93":{"start":"4524.93","dur":"4.932","text":"And an idea of this is well,\nmaybe we can actually"},"4529.86":{"start":"4529.862","dur":"4.953","text":"sort of just work with small\nvocabularies at any one time."},"4534.82":{"start":"4534.815","dur":"4.81","text":"So when we&#39;re training our models,\nwe could train using subsets of"},"4539.63":{"start":"4539.625","dur":"5.435","text":"the vocabulary because there&#39;s a lot\nof rare words but they&#39;re rare."},"4545.06":{"start":"4545.06","dur":"6.16","text":"So if you pick any slice of the training\ndata most rare words won&#39;t be in it."},"4551.22":{"start":"4551.22","dur":"3.79","text":"Commonly if you look at your\nwhole vocabulary about 40% of"},"4555.01":{"start":"4555.01","dur":"2.76","text":"your word types occur only once."},"4557.77":{"start":"4557.77","dur":"4.26","text":"That means if you cut your\ndata set into 20 pieces,"},"4562.03":{"start":"4562.03","dur":"2.66","text":"19 of those 20 will not contain that word."},"4564.69":{"start":"4564.69","dur":"4.45","text":"And then,\nwe also wanna be smart on testing."},"4569.14":{"start":"4569.14","dur":"6.33","text":"So we wanna be able to, at test time\nas well, generate sort of a smaller"},"4575.47":{"start":"4575.47","dur":"5.52","text":"set of words for our soft max, and so we\ncan be fast at both train and test time."},"4580.99":{"start":"4580.99","dur":"1.3","text":"Well, how can you do that?"},"4582.29":{"start":"4582.29","dur":"6.34","text":"Well, so at training time,\nwe want to have a small vocabulary."},"4588.63":{"start":"4588.63","dur":"5.1","text":"And so we can do that by partitioning the\nvocab, for partitioning the training data,"},"4593.73":{"start":"4593.73","dur":"4.719","text":"each slice of the training data,\nwe&#39;ll have a much lower vocabulary."},"4599.58":{"start":"4599.58","dur":"5.316","text":"And then we could partition randomly or\nwe could even smarter and"},"4604.90":{"start":"4604.896","dur":"4.636","text":"we can cut it into pieces\nthat have similar vocabulary."},"4609.53":{"start":"4609.532","dur":"3.102","text":"If we put all the basketball\narticles in one file and"},"4612.63":{"start":"4612.634","dur":"5.141","text":"all the foot walled articles in another\npile, will shrink the vocabulary further."},"4617.78":{"start":"4617.775","dur":"5.46","text":"And so they look at ways of doing that,\nso in practice that they can get down and"},"4623.24":{"start":"4623.235","dur":"4.62","text":"order a magnitude or more in the size\nof the vocab that they need for"},"4627.86":{"start":"4627.855","dur":"2.865","text":"each slice of the data, that&#39;s great."},"4633.04":{"start":"4633.04","dur":"2.94","text":"Okay, so what do we do at test time?"},"4635.98":{"start":"4635.98","dur":"2.82","text":"Well, what we wanna do\nit at test time as well,"},"4638.80":{"start":"4638.8","dur":"5","text":"when we&#39;re actually translating,\nwe want to use as much smaller vocabulary."},"4643.80":{"start":"4643.8","dur":"2.45","text":"Well, here&#39;s an idea of\nhow you could do that."},"4646.25":{"start":"4646.25","dur":"2.64","text":"Firstly, we say, they&#39;re are just common"},"4648.89":{"start":"4648.89","dur":"3.41","text":"function words that we always\ngonna want to have available."},"4652.30":{"start":"4652.3","dur":"2.73","text":"So we pick the K most frequent words and"},"4655.03":{"start":"4655.03","dur":"2.39","text":"say we&#39;re always gonna\nhave them in our Softmax."},"4657.42":{"start":"4657.42","dur":"2.51","text":"But then for the rest of it,"},"4659.93":{"start":"4659.93","dur":"4.8","text":"what we&#39;re actually gonna do is\nsort of have a lexicon on the side"},"4664.73":{"start":"4664.73","dur":"4.77","text":"where we&#39;re gonna know about likely\ntranslations for each source word."},"4669.50":{"start":"4669.5","dur":"4.8","text":"So that we&#39;ll have stored ways that would\nbe reasonable to translate she loves"},"4674.30":{"start":"4674.3","dur":"2.14","text":"cats into French."},"4676.44":{"start":"4676.44","dur":"3.46","text":"And so when we&#39;re translating a sentence,\nwe&#39;ll look out for"},"4679.90":{"start":"4679.9","dur":"4.62","text":"each word in the source sentence what\nare likely translations of it and"},"4684.52":{"start":"4684.52","dur":"3.25","text":"throw those into our candidates for\nthe Softmax."},"4689.54":{"start":"4689.54","dur":"5.21","text":"And so then we&#39;ve got a sort\nof a candidate list of words."},"4694.75":{"start":"4694.75","dur":"2.91","text":"And when translating\na particular soft sentence,"},"4697.66":{"start":"4697.66","dur":"3.45","text":"we&#39;ll only run our\nSoftmax over those words."},"4701.11":{"start":"4701.11","dur":"7.18","text":"And then again, we can save well over\nan order of magnitude computations."},"4708.29":{"start":"4708.29","dur":"5.94","text":"So, K prime is about 10 or 20 and\nK is sort of a reasonable size vocab."},"4714.23":{"start":"4714.23","dur":"4.771","text":"We can again, sort of cut at least in\nthe order of magnitude the size of"},"4719.00":{"start":"4719.001","dur":"3.72","text":"our soft mixers and\nact as if we had large vocabulary."},"4724.56":{"start":"4724.558","dur":"5.502","text":"There are other ways to do that too,\nwhich are on this slide."},"4730.06":{"start":"4730.06","dur":"4.89","text":"And what I was then going to go on,\nand we&#39;ll decide whether it does or"},"4734.95":{"start":"4734.95","dur":"2.17","text":"doesn&#39;t happen based on the syllabus."},"4737.12":{"start":"4737.12","dur":"5.81","text":"I mean, you could sort of say,\nwell, that&#39;s still insufficient"},"4742.93":{"start":"4742.93","dur":"4.83","text":"because I sort of said that you have\nto deal with a large vocabulary."},"4747.76":{"start":"4747.76","dur":"6.2","text":"And you&#39;ve sort of told us how to deal\nwith a large vocabulary more efficiently."},"4753.96":{"start":"4753.96","dur":"4.73","text":"But you&#39;ve still got problems, because\nin any new piece of text you give it,"},"4758.69":{"start":"4758.69","dur":"4.77","text":"you&#39;re going to have things like new\nnames turn up, new numbers turn up, and"},"4763.46":{"start":"4763.46","dur":"3.07","text":"you&#39;re going to want to\ndeal with those as well."},"4766.53":{"start":"4766.53","dur":"4.09","text":"And so\nit seems like somehow we want to be able"},"4770.62":{"start":"4770.62","dur":"5.1","text":"to just deal with new stuff at test time,\nat translation time."},"4775.72":{"start":"4775.72","dur":"3.55","text":"Which effectively means that\nkind of theoretically we have"},"4779.27":{"start":"4779.27","dur":"1.32","text":"an infinite vocabulary."},"4780.59":{"start":"4780.59","dur":"3.67","text":"And so, there&#39;s also been a bunch of\nwork on newer machine translation and"},"4784.26":{"start":"4784.26","dur":"1.21","text":"dealing with that."},"4785.47":{"start":"4785.47","dur":"3.62","text":"But unfortunately, this class time is not"},"4789.09":{"start":"4789.09","dur":"4.65","text":"long enough to tell you about it right\nnow, so I&#39;ll stop here for today."},"4793.74":{"start":"4793.74","dur":"4.96","text":"And don&#39;t forget, outside you can\ncollect your midterm on the way out."}}