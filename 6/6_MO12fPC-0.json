{"0":{"dur":4,"text":"[MUSIC]"},"4":{"dur":2,"text":"Stanford University."},"7":{"dur":1,"text":">> Okay hi everyone."},"8":{"dur":2,"text":"Let's get started again."},"11":{"dur":8,"text":"We're back with we're into\nweek six now and Lecture 11."},"20":{"dur":4,"text":"This is basically the third\nnow last of our lectures."},"25":{"dur":5,"text":"It's sort of essentially concentrating on\nwhat we can do with recurrent models and"},"30":{"dur":2,"text":"sequence to sequence architectures."},"33":{"dur":5,"text":"I thought what I'd do in the first\npart of the lecture is have one"},"39":{"dur":4,"text":"more attempt at explaining some\nof the ideas about GRUs and"},"44":{"dur":3,"text":"LSTMs and where do they come from and\nhow do they work?"},"48":{"dur":3,"text":"I'd sort of decide to do\nthat anyway on the weekend,"},"51":{"dur":5,"text":"just because I know that when I first\nstarted seeing some of these gated models,"},"57":{"dur":4,"text":"that it took a long time for\nthem to make much sense to me, and"},"62":{"dur":2,"text":"not just seem like a complete surprise and\nmystery."},"65":{"dur":1,"text":"That's the way they work so"},"66":{"dur":4,"text":"I hope I can do a bit of good at\nexplaining that one more time."},"70":{"dur":4,"text":"That feeling was reconfirmed when we\nstarted seeing some of the people"},"75":{"dur":4,"text":"who've filled in the midterm survey so\nthanks to all the people who filled it in."},"80":{"dur":1,"text":"For people who haven't,"},"81":{"dur":4,"text":"I'm still happy to have you fill it\nin over the last couple of days."},"85":{"dur":4,"text":"While there were a couple of people\nwho put LSTMs in the list of"},"89":{"dur":3,"text":"concepts they felt that they\nunderstood really well."},"93":{"dur":2,"text":"Dozens of people put LSTMs and"},"96":{"dur":4,"text":"GRUs into the list of concepts\nthey felt kind of unsure about."},"101":{"dur":5,"text":"This first part is for you and if you're\none of the ones that already understand"},"106":{"dur":4,"text":"it really well, I guess you'll just\nhave to skip ahead to the second part."},"110":{"dur":3,"text":"Then we'll have the research\nhighlight which should be fun today."},"113":{"dur":3,"text":"And then, so\nmoving on from that it's then completing,"},"117":{"dur":2,"text":"saying a bit more about\nmachine translation."},"120":{"dur":4,"text":"It's a bit that we sort of had skipped and\nprobably should have explained earlier"},"125":{"dur":3,"text":"which is how do people evaluate\nmachine translation systems?"},"128":{"dur":3,"text":"Because we've been showing you numbers and\ngraphs and so on and never discussed that."},"132":{"dur":5,"text":"And then I wanna sort of say a bit\nmore about a couple of things that"},"137":{"dur":5,"text":"come up when trying to build new\nmachines translation systems."},"142":{"dur":4,"text":"And in some sense, this is sort of\ndone on the weed stuff it's not"},"146":{"dur":4,"text":"that this is sort of one central concept\nthat you can possibly finish your"},"151":{"dur":2,"text":"neural networks class\nwithout having learned."},"154":{"dur":4,"text":"But on the other hand, I think that all of\nthese sort of kind of things that come up"},"158":{"dur":4,"text":"if you are actually trying to build\nsomething where you've actually got a deep"},"163":{"dur":3,"text":"learning system that you can use to\ndo useful stuff in the world and"},"166":{"dur":3,"text":"that they're useful, good,\nnew concepts to know."},"170":{"dur":1,"text":"Okay."},"172":{"dur":2,"text":"Lastly just the reminders and\nvarious things."},"174":{"dur":2,"text":"The midterm, we have got it all graded."},"177":{"dur":2,"text":"And our plan is that we are going to"},"180":{"dur":3,"text":"return it to the people\nwho are here after class."},"183":{"dur":4,"text":"Where in particular, there's another\nevent that's on here after class,"},"188":{"dur":4,"text":"so where we're going to return it\nafter class is outside the door."},"193":{"dur":3,"text":"That you should be able to find\nTAs with boxes of midterms and"},"197":{"dur":1,"text":"be able to return them."},"198":{"dur":4,"text":"Assignment three, yeah so this has\nbeen a little bit of a stretch for"},"203":{"dur":2,"text":"everybody on assignment three I realized,"},"206":{"dur":4,"text":"because sort of the midterm got\nin the way and people got behind."},"210":{"dur":4,"text":"And we've also actually we're hoping\nto be sort of right ready to go with"},"215":{"dur":4,"text":"giving people GPU resources on Azure and\nthat's kinda've gone behind,"},"220":{"dur":4,"text":"they're trying to work on that right\nnow so with any luck maybe by the end"},"224":{"dur":4,"text":"of today we might have the GPU\nresources part in place."},"229":{"dur":4,"text":"I mean, at any rate, you should absolutely\nbe getting start on the assignment and"},"233":{"dur":1,"text":"writing the code."},"235":{"dur":4,"text":"But we also do really hope that\nbefore you finish this assignment,"},"240":{"dur":4,"text":"you take a chance to try out Azure,\nDocker and"},"244":{"dur":3,"text":"getting stuff working on GPUs because\nthat's really good experience to have."},"249":{"dur":5,"text":"Then final projects,\nthe thing that we all noticed about our"},"255":{"dur":5,"text":"office hours last week after the midterm\nis that barely anybody came to them."},"260":{"dur":5,"text":"We'd really like to urge for this week,\nplease come along to office hours again."},"266":{"dur":4,"text":"And especially if you're doing\na final project, we'd really,"},"270":{"dur":4,"text":"really like you to turn up and\ntalk to us about your final projects and"},"275":{"dur":4,"text":"in particular tonight after class and\na bit of dinner which is again,"},"280":{"dur":3,"text":"we're going be doing\nunlimited office hours."},"283":{"dur":1,"text":"Feel free to come and see him, and"},"285":{"dur":3,"text":"possibly even depending on how you feel\nabout it, you might even go off and"},"288":{"dur":4,"text":"have dinner first and then come back and\nsee him to spread things out a little bit."},"294":{"dur":3,"text":"Are there any questions\npeople are dying to know,"},"297":{"dur":2,"text":"or do I head straight into\ncontent at that point?"},"303":{"dur":1,"text":"I'll head straight into content."},"306":{"dur":4,"text":"Basically I wanted to sort of spend\na bit of time going through, again,"},"310":{"dur":5,"text":"the sort of ideas of where did these\nkinds of fancy recurrent units come from?"},"316":{"dur":3,"text":"What are they going to try and achieve and\nhow do they go about doing it?"},"321":{"dur":4,"text":"Our starting point is, what we have\nwith a recurrent neural network is that"},"326":{"dur":3,"text":"we've got something that's\nevolving through time."},"329":{"dur":7,"text":"And at the end of that we're at some\npoint in that here where time t plus n."},"337":{"dur":4,"text":"And then what we want to do\nis have some sense of well,"},"341":{"dur":6,"text":"this stuff that we saw at time t, is that\naffecting what happens at time t plus n?"},"348":{"dur":6,"text":"That's the kind of thing of is it\nthe fact that we saw at time t"},"355":{"dur":7,"text":"this verb squash that is having\nsome effect on the n words later,"},"362":{"dur":5,"text":"that this is being someone saying\nthe word window because this is some"},"368":{"dur":5,"text":"kind of association between squashing and\nwindows or is that completely irrelevant?"},"373":{"dur":3,"text":"We wanna sort of measure\nhow what you're doing here"},"377":{"dur":4,"text":"affects what's happening maybe six,\neight, ten words later."},"382":{"dur":5,"text":"And so the question is how can we\nachieve that and how can we achieve it?"},"388":{"dur":4,"text":"And what Richard discussed and\nthere was some sort of complex math here"},"392":{"dur":3,"text":"which I'm not going to explain,\nagain, in great detail."},"396":{"dur":4,"text":"But what we found is if we had a basic\nrecurrent neural network what we're"},"401":{"dur":4,"text":"doing at each time step in the basic\nrecurrent neural network is"},"406":{"dur":4,"text":"we've got some hidden state and\nwe're multiplying it by matrix and"},"410":{"dur":3,"text":"then we're adding some stuff to do with\nthe input and then we go onto next"},"414":{"dur":5,"text":"time stamp where we're multiplying that\nhidden state by the same matrix again and"},"419":{"dur":4,"text":"adding some input stuff and then we\ngo onto the time step and we model."},"423":{"dur":3,"text":"Multiplying that,\nhidden stuff by the same matrix again."},"427":{"dur":4,"text":"It keeping on doing these matrix\nmultiplies and when you keep on doing"},"432":{"dur":5,"text":"these matrix multiplies you can\npotentially get into trouble."},"438":{"dur":5,"text":"And the trouble you get into is\nif your gradient is going to zero"},"443":{"dur":4,"text":"you kind of can't tell whether that\nmeans that actually what happened"},"448":{"dur":5,"text":"in words ago is having no effect\non what you're seeing now."},"453":{"dur":5,"text":"Or whether it is you hadn't set all\nof the things in your matrixes norm"},"458":{"dur":6,"text":"exactly right and so that the gradient\nis going to zero because it's vanishing."},"470":{"dur":4,"text":"This is where the stuff about eigenvalues\nand stuff like that comes in."},"475":{"dur":3,"text":"But kind of the problem is with."},"479":{"dur":4,"text":"Basic RNA, sort of a bit too much\nlike having to land your aircraft"},"483":{"dur":3,"text":"on the aircraft carrier or\nsomething like that."},"486":{"dur":3,"text":"That if you can get things\njust the right size,"},"490":{"dur":3,"text":"things you can land on\nthe aircraft carrier but"},"493":{"dur":5,"text":"if somehow your eigenvalues are a bit too\nsmall then you have vanishing gradients."},"498":{"dur":5,"text":"And if they're a bit too large\nyou have exploding gradients and"},"503":{"dur":3,"text":"you sort of,\nit's very hard to get it right and so"},"507":{"dur":6,"text":"this this naive transition function seems\nto be the cause of a lot of the problems."},"514":{"dur":3,"text":"With the naive transition\nfunction in particular,"},"517":{"dur":4,"text":"what it means is that sorta we're doing\nthis sequence of matrix multipliers."},"522":{"dur":3,"text":"So we're keeping on multiplying\nby matrix at each time step."},"526":{"dur":3,"text":"And so, that means that when\nwe're then trying to learn."},"529":{"dur":4,"text":"How much effect things have\non our decisions up here."},"533":{"dur":4,"text":"We're doing that by backpropagating\nthrough this whole sequence of"},"537":{"dur":2,"text":"intermediate nodes."},"540":{"dur":6,"text":"And so, the whole idea of all of these\ngated recurrent models is to say,"},"546":{"dur":5,"text":"well, somehow, we'd like to be\nable to get more direct evidence"},"551":{"dur":4,"text":"of the effect of early time\nsteps on much later time steps,"},"556":{"dur":7,"text":"without having to do this long sequence\nmatrix multiplies, which almost certainly."},"564":{"dur":3,"text":"Give us the danger of\nkilling off the evidence."},"567":{"dur":3,"text":"So essentially what we wanna have is,"},"570":{"dur":4,"text":"we want to kinda consider the time\nsequence that's our straight line."},"575":{"dur":5,"text":"We also want to allow these shortcut\nconnections so ht can directly"},"580":{"dur":5,"text":"affect ht +2 because if we could do\nthat we then when we're backpropagating"},"586":{"dur":6,"text":"we'll then be able to measure in the\nbackward phase the effect of ht on ht + 2."},"592":{"dur":1,"text":"And therefore,"},"593":{"dur":4,"text":"we would be much more likely to\nlearn these long term dependencies."},"598":{"dur":2,"text":"So that seems a good idea."},"603":{"dur":2,"text":"So I'm gonna do the kinda gated\nrecurrent units first, and"},"606":{"dur":3,"text":"then kinda build onto LSTMs,\nwhich are even more complex."},"610":{"dur":4,"text":"So essentially that's what we're\ndoing in the gated recurrent unit."},"614":{"dur":4,"text":"And we're only making it a little\nbit more complex by saying, well,"},"618":{"dur":5,"text":"rather than just uniformly\nputting in stuff from time -1 and"},"623":{"dur":6,"text":"time -2, maybe we can have adaptive\nshortcut connections where we're"},"630":{"dur":4,"text":"deciding how much attention to pay to\nthe past, as well as to the present."},"635":{"dur":3,"text":"And so, that's essentially what you\nget with the gated recurrent unit."},"639":{"dur":5,"text":"So the key equation of the gated\nrecurrent unit is this first one."},"645":{"dur":4,"text":"So it's sort of saying, well, we're\ngoing to do the normal neural network"},"649":{"dur":3,"text":"recurrent units stuff,\nthat's the stuff in green."},"653":{"dur":5,"text":"So for the stuff in green, we take the\ncurrent input and multiply it by a matrix."},"658":{"dur":3,"text":"We take the previous hidden statement and\nmultiply it by a matrix."},"661":{"dur":2,"text":"We add all of those things with a bias and"},"664":{"dur":5,"text":"put it through a tanh, that's exactly the\nstandard recurrent neural network update."},"669":{"dur":7,"text":"So we're going to do that candidate\nupdate just like a regular RNN."},"676":{"dur":4,"text":"But to actually work out what\nfunction we're computing,"},"680":{"dur":4,"text":"we're then going to adaptively learn\nhow much and on which dimensions"},"685":{"dur":5,"text":"to use that candidate update and\nhow much that we just gonna shortcut it,"},"690":{"dur":4,"text":"and just stick with what we had\nfrom the previous time step."},"695":{"dur":4,"text":"And while that stuff in the previous\ntime step will have been to some"},"699":{"dur":4,"text":"extent computed by this regular and\nupdated the previous time step."},"704":{"dur":2,"text":"But of course, that was also a mixture, so"},"707":{"dur":4,"text":"to some extent, it will have been directly\ninherited from the time step before that."},"712":{"dur":4,"text":"And so,\nwe kind of adaptively allowing things from"},"716":{"dur":4,"text":"far past time steps just to\nbe passed straight through,"},"720":{"dur":4,"text":"with no further multiplications\ninto the current time step."},"725":{"dur":3,"text":"So a lot of the key to is it\nthat we have this plus here."},"729":{"dur":4,"text":"The stuff that is on this side\nof the plus, we're just saying,"},"733":{"dur":4,"text":"just move along the stuff you had\nbefore onto the next time step,"},"738":{"dur":4,"text":"which has the effect that we're\ndirectly having stuff from the past"},"743":{"dur":3,"text":"be present to affect further on decisions."},"746":{"dur":3,"text":"So that's most of what\nwe have in a GRU and"},"750":{"dur":7,"text":"a GRU is then just a little bit more\ncomplex than that because if we do this,"},"757":{"dur":6,"text":"it's sort of all additive,\nyou kinda kick stuff around forever."},"763":{"dur":2,"text":"You're deciding which to pay attention to,\nbut"},"766":{"dur":3,"text":"once you've paid attention to it,\nit's around forever."},"770":{"dur":4,"text":"And that's because you're sort\nof just adding stuff on here."},"774":{"dur":5,"text":"And so, the final step is to say well\nactually, maybe we want to sort of prune"},"780":{"dur":5,"text":"away some of the past stuff adaptively so\nit doesn't hang around forever."},"786":{"dur":4,"text":"And so, to do that, we're adding\nthis second gate, the reset gate."},"790":{"dur":6,"text":"And so, the reset gate gives you a vector\nof, again, numbers between zero and"},"796":{"dur":5,"text":"one, which is calculated like a kind\nof a standard recurrent unit."},"801":{"dur":5,"text":"But it's sort of saying,\nwell to some extent, what we want to do is"},"807":{"dur":6,"text":"be able to delete some of the stuff that\nwas in ht- 1 when it's no longer relevant."},"813":{"dur":2,"text":"And so,\nwe doing this sort of hadamard product,"},"816":{"dur":4,"text":"the element wise product of the reset\ngate and the previous hidden state."},"820":{"dur":2,"text":"And so,\nwe can forget parts of the hidden state."},"823":{"dur":4,"text":"And the parts that we're forgetting is\nembedded in this kind of candidate update."},"827":{"dur":5,"text":"The part that's being just\npassed along from the past to"},"832":{"dur":5,"text":"have direct updates is still\njust exactly as it was before."},"838":{"dur":4,"text":"So to have one attempt to\nbe more visual at that."},"842":{"dur":4,"text":"So if we have a basic vanilla tanh-RNN,"},"846":{"dur":6,"text":"one way that you could think about\nthat is we have a hidden state,"},"853":{"dur":5,"text":"and what our execution of our\nunit is doing as a program"},"858":{"dur":4,"text":"is saying you read the whole\nof that register h,"},"863":{"dur":6,"text":"you do your RNN update, and\nyou write the whole thing back."},"869":{"dur":3,"text":"So you've got this one memory register."},"873":{"dur":4,"text":"You read it all, do a standard recurrent\nupdate, and write it all back."},"878":{"dur":2,"text":"So that's sort of very inflexible."},"880":{"dur":4,"text":"And you're just sort of repeating that\nover and over again at each time step."},"885":{"dur":4,"text":"So in contrast to that,\nwhen you have a GRU unit, that is then,"},"890":{"dur":5,"text":"allowing you to sort of learn\nthis adaptive flexibility."},"895":{"dur":4,"text":"So first of all,\nwith the reset gate, you can learn"},"899":{"dur":5,"text":"a subset of the hidden state that\nyou want to read and make use of."},"905":{"dur":2,"text":"And the rest of it will\nthen get thrown away."},"907":{"dur":2,"text":"So you have an ability to forget stuff."},"910":{"dur":5,"text":"And then,\nonce you've sort of read your subset,"},"915":{"dur":4,"text":"you'll then going to do\non it your standard RNN"},"920":{"dur":3,"text":"computation of how to update things."},"923":{"dur":3,"text":"But then secondly,\nyou're gonna select the writable subset."},"927":{"dur":1,"text":"So this is saying,"},"929":{"dur":3,"text":"some of the hidden state we're\njust gonna carry on from the past."},"933":{"dur":3,"text":"We're only now going to\nedit part of the register."},"937":{"dur":4,"text":"And saying part of the register,\nI guess is a lying and simplifying a bit,"},"941":{"dur":3,"text":"because really,\nyou've got this vector of real numbers and"},"945":{"dur":4,"text":"some said the part of the register is\n70% updating this dimension and 20%"},"950":{"dur":5,"text":"updating this dimension that values could\nbe one or zero but normally they won't be."},"956":{"dur":2,"text":"So I choose the writable subset And"},"958":{"dur":5,"text":"then it's that part of it that I'm\nthen updating with my new candidate"},"963":{"dur":3,"text":"update which is then written back,\nadding on to it."},"968":{"dur":2,"text":"And so\nboth of those concepts in the gating,"},"971":{"dur":4,"text":"the one gate is selecting what to read for\nyour candidate update."},"976":{"dur":7,"text":"And the other gate is saying, which\nparts of the hidden state to overwrite?"},"983":{"dur":2,"text":"Does that sort of make\nsense how that's a useful,"},"986":{"dur":3,"text":"more powerful way of thinking\nabout having a recurrent model?"},"994":{"dur":1,"text":"Yes, a question?"},"1003":{"dur":5,"text":"Yeah, so how you select the readable\nsubset is based on this reset gate?"},"1008":{"dur":4,"text":"So, the reset gate decides\nwhich parts of the hidden"},"1013":{"dur":3,"text":"state to read to update the hidden state."},"1017":{"dur":6,"text":"So, the reset gate calculates which parts\nto read based on the current input and"},"1023":{"dur":2,"text":"the previous hidden state."},"1025":{"dur":7,"text":"So it's gonna say, okay, I wanna pay a lot\nof attention to dimensions 7 and 52."},"1032":{"dur":3,"text":"And so, those are the ones and\na little to others."},"1036":{"dur":4,"text":"And so those are the ones that\nwill be being read here and"},"1040":{"dur":4,"text":"used in the calculation of\nthe new candidate update,"},"1044":{"dur":6,"text":"which is then sort of mixed together\nwith carrying on what you had before."},"1050":{"dur":1,"text":"Any, yes."},"1066":{"dur":4,"text":"So, the question was explain this again."},"1070":{"dur":0,"text":"I'll try."},"1071":{"dur":2,"text":"[LAUGH] I will try."},"1073":{"dur":1,"text":"I will try and do that."},"1075":{"dur":4,"text":"Let me go back to this slide first,\ncuz this has most of that,"},"1079":{"dur":2,"text":"except the last piece, right."},"1081":{"dur":8,"text":"So here, what we want to do is we're\ncarrying along a hidden state over time."},"1089":{"dur":4,"text":"And at each point in time,\nwe're going to say, well,"},"1094":{"dur":4,"text":"based on the new input and\nthe previous hidden state,"},"1099":{"dur":4,"text":"we want to try and\ncalculate a new hidden state, but"},"1104":{"dur":4,"text":"we don't fully want to\ncalculate a new hidden state."},"1108":{"dur":5,"text":"Sometimes, it will be useful just to\ncarry over information from further back."},"1113":{"dur":5,"text":"That's how we're going to get longer term\nmemory into our current neural network."},"1119":{"dur":5,"text":"Cuz if we kind of keep on doing\nmultiplications at each time step"},"1124":{"dur":4,"text":"along a basic RNN,\nwe lose any notion of long-term memory."},"1128":{"dur":5,"text":"And essentially, we can't remember things\nfor more than seven to ten time steps."},"1134":{"dur":7,"text":"So that is sort of the top level equation\nto say, well, what we gonna calculate."},"1141":{"dur":6,"text":"We want to calculate a mixture\nof a candidate update and"},"1147":{"dur":6,"text":"keeping what we had there before and\nhow do we do that?"},"1153":{"dur":5,"text":"Well, what we're going to learn is\nthis ut vector, the update gate and"},"1159":{"dur":4,"text":"the elements of that vector\nare gonna be between zero and one."},"1163":{"dur":2,"text":"And if they're close to one,\nit's gonna say,"},"1166":{"dur":4,"text":"overwrite the current hidden state with\nwhat we calculated this time step."},"1170":{"dur":2,"text":"And if they're close to zero,\nit's gonna say,"},"1173":{"dur":3,"text":"keep this element vector\njust what it used to be."},"1177":{"dur":4,"text":"And so how we calculate the update\ngate is using our regular kind"},"1182":{"dur":4,"text":"of recurrent unit where it\nlooks at the current input and"},"1186":{"dur":5,"text":"it looks at the recent history and\nit calculates a value with the only"},"1191":{"dur":4,"text":"difference that we use here sigmoid,\nso that's between 0 and"},"1196":{"dur":4,"text":"1 rather than tanh that puts\nthat at between minus 1 and 1."},"1201":{"dur":5,"text":"And so the kind of hope\nhere intuitively is suppose"},"1206":{"dur":5,"text":"we have a unit that is\nsort of sensitive to what"},"1212":{"dur":5,"text":"verb we're on,\nthen what we wanna say is well,"},"1217":{"dur":7,"text":"we're going through this sentence and\nwe've seen a verb."},"1224":{"dur":5,"text":"We wanted that unit, well, sorry,\nthese dimension of the vector."},"1230":{"dur":3,"text":"Let's say, their five dimensions of the\nvector that sort of record what kind of"},"1233":{"dur":1,"text":"verb it's just seen."},"1235":{"dur":6,"text":"We want those dimensions of the vector\nto just stay recording what verb was"},"1242":{"dur":5,"text":"seen until such time as in the input,\na band new verb appears."},"1247":{"dur":5,"text":"And it's at precisely that point, we wanna\nsay, okay, now is the time to update."},"1253":{"dur":3,"text":"Forget about what used to be\nstored in those five dimensions."},"1256":{"dur":4,"text":"Now, you should store\na representation of the new verb."},"1260":{"dur":3,"text":"And so, that's exactly what\nthe update gate could do here."},"1264":{"dur":5,"text":"It could be looking at the input and\nsay, okay, I found a new verb."},"1270":{"dur":5,"text":"So dimensions 47 to 52 should\nbe being given a value of 1 and"},"1275":{"dur":7,"text":"that means that they'll be storing a value\ncalculated from this candidate update,"},"1282":{"dur":4,"text":"and ignoring what they\nused to store in the past."},"1287":{"dur":3,"text":"But if the update gate finds\nit's looking at a preposition or"},"1290":{"dur":3,"text":"at a term in our It'll say,\nno, not interested in those."},"1294":{"dur":4,"text":"So it'll make the update\nvalue close to 0 and"},"1298":{"dur":4,"text":"that means that dimensions\n47 to 52 will continue to"},"1303":{"dur":5,"text":"store the verb that you last saw\neven if it was ten words ago."},"1308":{"dur":1,"text":"I haven't quite finish."},"1309":{"dur":2,"text":"So that was that part of it, so yes."},"1312":{"dur":1,"text":"So, the candidate update."},"1314":{"dur":1,"text":"So, that's the update gate."},"1315":{"dur":4,"text":"And when we do update, the candidate\nupdate is just exactly the same as"},"1320":{"dur":4,"text":"it always was in our current new\nnetwork that you're calculating this"},"1325":{"dur":3,"text":"function of the important\nthe previous hidden state and"},"1329":{"dur":3,"text":"put it through a tanh\ntogether from minus 1 to 1."},"1333":{"dur":3,"text":"Then the final idea here is that well,"},"1337":{"dur":6,"text":"if you just have this,\nif you're doing a candidate update,"},"1343":{"dur":5,"text":"you're always using\nthe previous hidden state and"},"1348":{"dur":4,"text":"the new input word in\nexactly the same way."},"1353":{"dur":5,"text":"Whereas really for my example, what I was\nsaying was if you have detected a new"},"1359":{"dur":5,"text":"verb in the input, you should be storing\nthat new verb in dimensions 47 to 52 and"},"1365":{"dur":3,"text":"you should just be ignoring\nwhat you used to have there."},"1368":{"dur":3,"text":"And so it's sort of seems like\nat least in some circumstances"},"1372":{"dur":3,"text":"what you'd like to do is throw\naway your current hidden state,"},"1376":{"dur":3,"text":"so you could replace it\nwith some new hidden state."},"1379":{"dur":3,"text":"And so that's what this second gate,\nthe reset gate does."},"1383":{"dur":4,"text":"So the reset gate can also look at\nthe current import in the previous hidden"},"1387":{"dur":3,"text":"state and\nit choses a value between zero, and one."},"1391":{"dur":3,"text":"And if the reset gate choses\na value close to zero,"},"1395":{"dur":5,"text":"you're essentially just throwing\naway the previous hidden state and"},"1400":{"dur":3,"text":"calculating something\nbased on your new input."},"1404":{"dur":4,"text":"And the suggestion there for\nlanguage analogy is well,"},"1408":{"dur":6,"text":"if it's something like you're recording,\nthe last seen verb in dimensions 47 to 52."},"1415":{"dur":4,"text":"When you see a new verb, well, the right\nthing to do is to throw away what you"},"1420":{"dur":4,"text":"have in your history from 47 to 52 and\njust calculate something new based"},"1425":{"dur":4,"text":"on the input, but that's not always\ngonna be what you want to do."},"1429":{"dur":4,"text":"For example, in English, English is\nfamous for having a lot of verb particle"},"1434":{"dur":4,"text":"combinations which cause enormous\ndifficulty to non-native speakers."},"1439":{"dur":6,"text":"So that's all of these things\nlike make up, make out, take up."},"1445":{"dur":2,"text":"All of these combinations of a verb and"},"1448":{"dur":3,"text":"a preposition have a special\nmeaning that you just have to know."},"1451":{"dur":5,"text":"It isn't really, you can't tell\nfrom the words most of the time."},"1457":{"dur":5,"text":"So if you are wanting to work out\nwhat the meaning of make out is,"},"1462":{"dur":5,"text":"so you've seen make and\nyou put in that into dimensions 47 to 52."},"1468":{"dur":4,"text":"But if dimensions 47 to 52 are really\nstoring main predicate meaning,"},"1473":{"dur":5,"text":"if you see the word out coming next, you\ndon't wanna throw away make because it's"},"1478":{"dur":5,"text":"a big difference in meaning whether\nit's make out or take out will give out."},"1483":{"dur":3,"text":"What you wanna do is you wanna combine\nboth of them together to try and"},"1487":{"dur":1,"text":"calculate the predicate's meaning."},"1489":{"dur":5,"text":"So in that case, you want your reset\ngate to have a value near one so you're"},"1495":{"dur":4,"text":"still keeping it and you're keeping the\nnew import and calculating another value."},"1502":{"dur":3,"text":"Okay, that was my attempt to explain GRUs,\nand now the question."},"1518":{"dur":4,"text":"So the question is okay, but\nwhy this gated recurrent"},"1523":{"dur":5,"text":"unit not suffer from\nthe vanishing gradient problem?"},"1529":{"dur":5,"text":"And really the secret is\nright here in this plus sign."},"1539":{"dur":5,"text":"If you allowed me to simplify slightly,"},"1544":{"dur":7,"text":"and this is actually a version\nof a network that has been used."},"1551":{"dur":5,"text":"It's essentially, not more details,\nbut this aspect of it actually"},"1557":{"dur":5,"text":"corresponds to the very original\nform of an LSTM that was proposed."},"1562":{"dur":8,"text":"Suppose I just delete this this- ut here,\nso this just was 1."},"1570":{"dur":5,"text":"So what we have here is ht- 1,\nso kind of like the reset gate,"},"1576":{"dur":4,"text":"the update gate is only\nbeing used on this side."},"1581":{"dur":4,"text":"It's saying should you pay any\nattention to the new candidate,"},"1585":{"dur":3,"text":"but you're always plussing it with ht-1."},"1589":{"dur":4,"text":"If you'll imagine that\nslightly simplified form,"},"1593":{"dur":3,"text":"well, if you think about your gradients,"},"1597":{"dur":5,"text":"then what we've got here is when\nwe're kind of working at h,"},"1603":{"dur":3,"text":"this has been used to calculate ht."},"1606":{"dur":4,"text":"Ht-1 is being used to calculate ht, so"},"1611":{"dur":3,"text":"ht equals a plus ht-1, so"},"1614":{"dur":5,"text":"there's a completely linear relationship"},"1619":{"dur":5,"text":"with a coefficient of one between ht and\nht-1."},"1625":{"dur":3,"text":"Okay, and so\ntherefore when you do your calculus and"},"1629":{"dur":4,"text":"you back prop that, right,\nyou have something with slope 1."},"1633":{"dur":5,"text":"That ht is just directly reflecting ht-1."},"1639":{"dur":4,"text":"And that's the perfect case for\ngradients to flow beautifully."},"1644":{"dur":4,"text":"Nothing is lost, it's just going\nstraight back down the line."},"1648":{"dur":5,"text":"And so that's why it can carry\ninformation for a very long time."},"1654":{"dur":5,"text":"So once we put in this update gate,\nwhat we're having is the providing"},"1660":{"dur":5,"text":"ut is close to zero,\nthis is gonna be approximately one,"},"1665":{"dur":4,"text":"and so the gradients are just gonna flow\nstraight back to the line in an arbitrary"},"1670":{"dur":4,"text":"distance and\nyou can have long distance dependencies."},"1674":{"dur":3,"text":"Crucially, it's not like you're\nmultiplying by a matrix every time,"},"1678":{"dur":3,"text":"which causes all with vanishing gradients."},"1681":{"dur":5,"text":"It's just almost one there,\nstraight linear sequence."},"1687":{"dur":6,"text":"Now of course, if at some point ut is\nclose to 1, so this is close to zero,"},"1694":{"dur":4,"text":"well then almost nothing\nis flowing in from ht-1."},"1699":{"dur":3,"text":"But that's then saying there\nis no long term dependency."},"1702":{"dur":2,"text":"That's what the model learn."},"1704":{"dur":4,"text":"So nothing flows a long way back."},"1709":{"dur":0,"text":"Is that a question?"},"1710":{"dur":0,"text":"Yeah."},"1719":{"dur":7,"text":"So the question is,\nisn't ht tilted ut both dependent on ht-1."},"1726":{"dur":1,"text":"And yeah, they are."},"1727":{"dur":6,"text":"Just like the ut you're calculating\nit here in terms of ht-1."},"1734":{"dur":6,"text":"So in some sense the answer is yeah,\nyou are right but"},"1740":{"dur":5,"text":"it's sort of turns out not matter, right?"},"1746":{"dur":4,"text":"So the thing I think is If I put words\nin to your mouth, the thing that you're"},"1750":{"dur":5,"text":"thinking about is well, this ut\nlook right down at the bottom here,"},"1755":{"dur":4,"text":"you'll calculate it by matrix\nvector multiply from ht-1."},"1760":{"dur":4,"text":"And well then, where the ht-1 come from,"},"1764":{"dur":5,"text":"it came from ht-2 and there was some\nmore matrix vector multiplies here,"},"1769":{"dur":4,"text":"so there is a pathway going\nthrough the gates where"},"1774":{"dur":4,"text":"you're keep on doing matrix vector\nmultiplies, and that is true."},"1778":{"dur":3,"text":"But, it turns out that sort\nof doesn't really matter,"},"1782":{"dur":4,"text":"because of the fact that there is this\ndirect pathway, where you're getting"},"1787":{"dur":4,"text":"this straight linear flow of gradient\ninformation, going back in time."},"1794":{"dur":1,"text":"Any other question?"},"1795":{"dur":4,"text":"Yes, I don't think I'll get any further\nin this class if I'm not careful."},"1810":{"dur":1,"text":"I'm sorry if that's true."},"1813":{"dur":5,"text":"So the question was, why when you\nIs before ut and one, one is ut."},"1818":{"dur":0,"text":"We swapped."},"1819":{"dur":3,"text":">> [INAUDIBLE]\n>> Yeah, if that's true, sorry about that."},"1823":{"dur":2,"text":"That was bad, boo boo mistake,"},"1825":{"dur":2,"text":"cuz obviously we should be\ntrying to be consistent."},"1827":{"dur":3,"text":"But, it totally doesn't matter."},"1831":{"dur":4,"text":"This is sort of, in some sense, whether\nyou're thinking of it as the forget"},"1836":{"dur":4,"text":"gate or a remember gate, and\nyou can kind of have it either way round."},"1840":{"dur":4,"text":"And that doesn't effect how the math and\nthe learning works."},"1848":{"dur":1,"text":"Any other questions?"},"1851":{"dur":4,"text":"I'm happy to talk about this because I do\nactually think it's useful to understand"},"1856":{"dur":4,"text":"this stuff cuz in some sense these kind\nof gated units have been the biggest and"},"1860":{"dur":4,"text":"most useful idea for making practical\nsystems in the last couple of years."},"1864":{"dur":0,"text":"Yes."},"1871":{"dur":5,"text":"I actually have a picture for\nan LSTM later on."},"1876":{"dur":3,"text":"It depends on a lot of particularities,\nbut"},"1880":{"dur":4,"text":"it sort of seems like\nsomewhere around 100."},"1884":{"dur":5,"text":"Sorry the question was how long does a GRU\nactually end up remembering for and I"},"1889":{"dur":5,"text":"kind of think order of magnitude the kind\nnumber you want in your head is 100 steps."},"1895":{"dur":5,"text":"So they don't remember forever I think\nthat's something people also get wrong."},"1900":{"dur":5,"text":"If we go back to the other one,\nthat I hope to get to eventually,"},"1906":{"dur":2,"text":"the name is kind of a mouthful."},"1909":{"dur":4,"text":"I think it was actually very\ndeliberately named, where it was called,"},"1914":{"dur":1,"text":"long short term memory."},"1916":{"dur":4,"text":"Right there was no idea in people's\nheads that this was meant to be"},"1921":{"dur":4,"text":"the model of long term\nmemory in the human brain."},"1925":{"dur":3,"text":"Long term memory is\nfundamentally different and"},"1928":{"dur":3,"text":"needs to be modeled in other ways and\nmaybe later in the class,"},"1931":{"dur":4,"text":"we'll say a little a bit about the kind\nof ideas people thinking about this."},"1936":{"dur":3,"text":"What this was about was saying okay,"},"1939":{"dur":5,"text":"well people have a short term memory and\nit lasts for a while."},"1944":{"dur":4,"text":"Whereas the problem was our current\nneural networks are losing all of there"},"1949":{"dur":1,"text":"memory in ten time steps."},"1951":{"dur":4,"text":"So if we could get that pushed out\nanother order of magnitude during"},"1956":{"dur":4,"text":"100 time steps that would\nbe really useful to give us"},"1960":{"dur":3,"text":"a more human like sense\nof short term memory."},"1963":{"dur":0,"text":"Sorry, yeah?"},"1969":{"dur":5,"text":"So the question is,\ndo GRUs train faster than LSTMs?"},"1975":{"dur":4,"text":"I don't think that's true,\ndoes Richard have an opinion?"},"1979":{"dur":5,"text":">> [INAUDIBLE]\n>> Yes,"},"1985":{"dur":5,"text":"so Richard says less computation\nthe computational cost is faster,"},"1990":{"dur":4,"text":"but I sort of feel that sometimes\nLSTMs have a slight edge on speed."},"1995":{"dur":1,"text":"No huge difference,\nlet's say that's the answer."},"1997":{"dur":5,"text":"Any other, was there another\nquestion that people want to ask?"},"2004":{"dur":1,"text":"Okay, I'll go on."},"2006":{"dur":5,"text":"You can ask them again in a minute and\nI go on."},"2011":{"dur":4,"text":"Okay, so then finally I wanted to sort"},"2016":{"dur":4,"text":"of say a little bit about LSTMs."},"2020":{"dur":5,"text":"So LSTMs are more complex because there\nare more equations down the right side."},"2025":{"dur":6,"text":"And there's more gates but they're barely\ndifferent when it comes down to it."},"2032":{"dur":7,"text":"And to some extent, they look more\ndifferent than they are because of"},"2040":{"dur":5,"text":"certain arbitrary choices of notation\nthat was made when LSTMs were introduced."},"2045":{"dur":5,"text":"So when LSTMs were introduced,\nHochreiter & Schmidhuber"},"2050":{"dur":4,"text":"sort of decided to say, well,\nwe have this privileged notion of"},"2055":{"dur":4,"text":"memory in the LSTM,\nwhich we're going to call the cell."},"2060":{"dur":4,"text":"And so people use C for\nthe cell of the LSTM."},"2064":{"dur":5,"text":"But the crucial thing to notice\nIs that the cell of the LSTM"},"2070":{"dur":5,"text":"is behaving like the hidden\nstate of the GRU, so really,"},"2075":{"dur":5,"text":"the h of the GRU is equivalent\nto the c of the LSTM."},"2081":{"dur":4,"text":"Whereas the h of the LSTM is"},"2085":{"dur":3,"text":"something different that's related\nto sort what's exposed to the world."},"2089":{"dur":6,"text":"So the center of the LSTM,\nthis equation for updating the cell."},"2095":{"dur":5,"text":"Is do a first approximation exactly\nthe same as this most crucial equation for"},"2101":{"dur":2,"text":"updating the hidden state of the GRU."},"2104":{"dur":4,"text":"Now, if you stare a bit,\nthey're not quite the same,"},"2109":{"dur":3,"text":"the way they are different is very small."},"2112":{"dur":5,"text":"So in the LSTM you have two gates\na forget gate and then an input gate so"},"2118":{"dur":5,"text":"both of those for each of the dimension\nhave a value between zero and one."},"2123":{"dur":4,"text":"So you can simultaneously keep\neverything from the past and"},"2128":{"dur":3,"text":"keep everything from your\nnew calculated value and"},"2132":{"dur":4,"text":"sum them together which is\na little bit different."},"2136":{"dur":5,"text":"To the GRU where you're sort of doing\nthis tradeoff as to how much to take"},"2141":{"dur":5,"text":"directly, copy across the path versus\nhow much to use your candidate update."},"2147":{"dur":4,"text":"So it split those into two functions,\nso you get the sum of them both."},"2151":{"dur":3,"text":"But other than that,\nit's exactly the same, right?"},"2154":{"dur":2,"text":"Where's my mouse?"},"2157":{"dur":4,"text":"The candidate update is\nexactly the same as what's"},"2162":{"dur":4,"text":"being listed in terms of c tilde and\nh tilde but"},"2166":{"dur":4,"text":"the candidate update is exactly,\nwell, sorry,"},"2170":{"dur":5,"text":"it's not quite I guess it's\nthe reset gate the candidate"},"2175":{"dur":5,"text":"update is virtually the same as\nthe standard LSTM style unit."},"2181":{"dur":4,"text":"And then for the gates,\nthe gates are sort of the same,"},"2185":{"dur":2,"text":"that they're using these sort of R and"},"2188":{"dur":5,"text":"N style calculations to get a value\nbetween zero for one for each dimension."},"2194":{"dur":4,"text":"So the differences\nare that we added one more"},"2199":{"dur":4,"text":"gate because we kinda having forget and"},"2204":{"dur":4,"text":"input gates here and\nthe other difference is"},"2209":{"dur":5,"text":"to have the ability to\nsort of that the GRUs sort"},"2214":{"dur":4,"text":"of has this reset gate where it's saying,"},"2219":{"dur":8,"text":"I might ignore part of the past when\ncalculating My candidate update."},"2227":{"dur":3,"text":"The LSTM is doing it\na little bit differently."},"2231":{"dur":6,"text":"So the LSTM in the candidate update,\nit's always using the current input."},"2237":{"dur":5,"text":"But for this other half here, it's not"},"2242":{"dur":5,"text":"using ct minus 1, it's using ht minus 1."},"2248":{"dur":5,"text":"So the LSTM has this extra\nht which is derived from ct."},"2254":{"dur":4,"text":"And the way that it's derived from ct\nis that there's an extra tanh here but"},"2259":{"dur":3,"text":"then you're scaling with this output gate."},"2262":{"dur":6,"text":"So the output gate is sort of equivalent\nof the reset gate of the GRU."},"2269":{"dur":4,"text":"But effectively,\nit's one one time step earlier,"},"2273":{"dur":4,"text":"cuz on the LSTM side,\non the preceding time step,"},"2277":{"dur":5,"text":"you also calculate an ht by ignoring\nsome stuff with the output gate,"},"2283":{"dur":3,"text":"whereas in the GRU, for\nthe current time step,"},"2287":{"dur":6,"text":"you're multiplying with the reset gate\ntimes your previous hidden state."},"2293":{"dur":1,"text":"That sorta makes sense?"},"2294":{"dur":1,"text":"A question."},"2309":{"dur":1,"text":"Right, yes, the don't forget gate."},"2311":{"dur":4,"text":"[LAUGH] You're right, so\nit's the question about was the ft."},"2316":{"dur":1,"text":"Is it really a forget gate?"},"2317":{"dur":3,"text":"No, as presented here,\nit's a don't forget gate."},"2321":{"dur":4,"text":"Again, you could do the 1 minus trick if\nyou wanted to and call this 1 minus f1,"},"2325":{"dur":4,"text":"but yeah, as presented here,\nif the value is close to 1,"},"2330":{"dur":2,"text":"it means don't forget, yeah, absolutely."},"2343":{"dur":5,"text":"So this one here is genuinely\nan update gate because if If the value"},"2349":{"dur":6,"text":"of it is close to 1,\nyou're updating with the candidate update."},"2355":{"dur":2,"text":"And if the value is close to zero,"},"2357":{"dur":2,"text":"you're keeping the previous\ncontents of the hidden state."},"2360":{"dur":5,"text":">> [INAUDIBLE]\nreset."},"2365":{"dur":3,"text":">> Right, so the reset gate is\nsort of a don't reset gate."},"2369":{"dur":0,"text":"[LAUGH] Yeah, okay."},"2370":{"dur":5,"text":"[LAUGH] I'm having a hard time\nwith the terminology here [LAUGH]."},"2376":{"dur":2,"text":"You are right."},"2379":{"dur":3,"text":"Another question?"},"2403":{"dur":6,"text":"So okay, so the question was\nsometimes you're using ct-1,"},"2409":{"dur":3,"text":"and sometimes you're using ht-1."},"2413":{"dur":1,"text":"What's going on there?"},"2414":{"dur":7,"text":"And the question is in what sense\nis ct less exposed in the LSTM?"},"2422":{"dur":4,"text":"Right, so there was something I glossed\nover in my LSTM presentation, and"},"2427":{"dur":1,"text":"I'm being called on it."},"2428":{"dur":5,"text":"Is look, actually for the LSTM, it's ht-1"},"2433":{"dur":5,"text":"that's being used everywhere for\nall three gates."},"2439":{"dur":4,"text":"So really, when I sort of said\nthat what we're doing here,"},"2444":{"dur":5,"text":"calculating ht, that's sort of\nsimilar to the reset gate in the GRU."},"2451":{"dur":2,"text":"I kind of glossed over that a little."},"2453":{"dur":5,"text":"It's sort of true in terms of thinking of\nthe calculation of the candidate update"},"2458":{"dur":5,"text":"cuz this ht- 1 will then go\ninto the candidate update."},"2463":{"dur":5,"text":"But's a bit more than that, cuz actually,\nstuff that you throw away with your"},"2469":{"dur":5,"text":"output gate at one time step is\nthen also gonna be thrown away"},"2474":{"dur":5,"text":"in the calculation of every\ngate at the next time step."},"2480":{"dur":8,"text":"Yeah, and so then the second question is\nin what sense is the cell less exposed?"},"2488":{"dur":1,"text":"And that's sort of the answer to that."},"2490":{"dur":4,"text":"The sense in which the cell\nis less exposed is"},"2495":{"dur":5,"text":"the only place that\nthe cell is directly used,"},"2500":{"dur":5,"text":"is to sort of linearly add\non the cell at the previous"},"2506":{"dur":4,"text":"time step plus its candidate update."},"2511":{"dur":2,"text":"For all the other computations,"},"2513":{"dur":4,"text":"you're sort of partially hiding\nthe cell using this output gate."},"2521":{"dur":1,"text":"Another question, sure."},"2548":{"dur":3,"text":"Hm, okay, so the question is, gee,"},"2552":{"dur":5,"text":"why do you need this tanh here,\ncouldn't you just drop that one?"},"2562":{"dur":0,"text":"Whoops."},"2571":{"dur":1,"text":"Hm."},"2574":{"dur":3,"text":"I'm not sure I have such a good\nanswer to that question."},"2578":{"dur":10,"text":">> [INAUDIBLE]"},"2597":{"dur":2,"text":">> Okay, so Richard's suggestion is,"},"2599":{"dur":3,"text":"well this ct is kind of\nlike a linear layer, and"},"2603":{"dur":5,"text":"therefore it's kind of insured if you\nshould add a non linearity after it."},"2608":{"dur":2,"text":"And that gives you a bit more power."},"2612":{"dur":3,"text":"Maybe that's right."},"2616":{"dur":4,"text":"Well, we could try it both ways and\nsee if it makes a difference, or"},"2620":{"dur":3,"text":"maybe Shane already has,\nI'm not sure [LAUGH]."},"2623":{"dur":2,"text":"Any other questions?"},"2625":{"dur":3,"text":"Make them a softball\none that I can answer."},"2629":{"dur":7,"text":">> [LAUGH]\n>> Okay,"},"2636":{"dur":5,"text":"so I had a few more\npictures that went through"},"2642":{"dur":6,"text":"the parts of the LSTM\nwith one more picture."},"2648":{"dur":4,"text":"I'm starting to think I should maybe\nnot dwell on this in much detail."},"2652":{"dur":4,"text":"Cuz we've sort of talked about\nthe fact that there are the gates for"},"2657":{"dur":1,"text":"all the things."},"2658":{"dur":7,"text":"We're working out the candidate update,\njust like an RNN."},"2665":{"dur":4,"text":"The only bit that I just wanna\nsay one more time is I think"},"2670":{"dur":4,"text":"it's fair to say that the whole\nsecret of these things,"},"2674":{"dur":6,"text":"is that you're doing this addition\nwhere you're adding together."},"2681":{"dur":3,"text":"When in the addition,\nit's sort of a weighted addition."},"2684":{"dur":1,"text":"But in the addition,"},"2686":{"dur":5,"text":"one choice is you're just copying\nstuff from the previous time step."},"2691":{"dur":5,"text":"And to the extent that you're copying\nstuff from the previous time step,"},"2696":{"dur":3,"text":"you have a gradient of 1,\nwhich you're just pushing."},"2700":{"dur":3,"text":"So you can push error directly\nback across that, and"},"2704":{"dur":3,"text":"you can keep on doing that for\nany number of time steps."},"2708":{"dur":4,"text":"So it's that plus, having that plus\nwith the previous time step rather"},"2713":{"dur":3,"text":"than having it all multiplied by matrix."},"2716":{"dur":6,"text":"That is the central idea that makes LSTMs\nbe able to have long short-term memory."},"2722":{"dur":4,"text":"And I mean, that has proven to\nbe an incredibly powerful idea,"},"2727":{"dur":4,"text":"and so in general,\nit doesn't sound that profound, but"},"2732":{"dur":5,"text":"that idea has been sort of driving\na lot of the developments of what's"},"2737":{"dur":4,"text":"been happening in deep learning\nin the last couple of years."},"2742":{"dur":7,"text":"So we don't really talk about,\nin this class, about vision systems."},"2749":{"dur":2,"text":"You can do that next quarter in 231N."},"2752":{"dur":5,"text":"But one of the leading ideas and has\nbeen used recently in better systems for"},"2757":{"dur":4,"text":"doing kind of vision systems with\ndeep learning has been the idea of"},"2762":{"dur":4,"text":"residual networks,\ncommonly shortened as ResNets."},"2767":{"dur":6,"text":"And to a first approximation, so"},"2774":{"dur":4,"text":"ResNets is saying gee,\nwe want to be able to build 100 layer"},"2778":{"dur":4,"text":"deep neural networks and\nbe able to train those successfully."},"2783":{"dur":1,"text":"And to a first approximation,"},"2785":{"dur":6,"text":"the way ResNets are doing that is exactly\nthe same idea here with the plus sign."},"2791":{"dur":2,"text":"It's saying, as you go up each layer,"},"2794":{"dur":6,"text":"we're going to calculate some non-linear\nfunction using a regular neural net layer."},"2800":{"dur":2,"text":"But will offer the alternative,"},"2802":{"dur":4,"text":"which is that you can just shunt\nstuff up from the layer before,"},"2806":{"dur":4,"text":"add those two together, and\nrepeat over again and go up 100 layers."},"2811":{"dur":4,"text":"And so this plus sign,\nyou may have learned in third grade, but"},"2816":{"dur":5,"text":"turns out plus signs have been a really\nuseful part of modern deep learning."},"2822":{"dur":7,"text":"Okay, Yeah, here is my little picture,\nwhich I'll just show."},"2829":{"dur":4,"text":"I think you'll have to sort\nof then slow it down to"},"2834":{"dur":4,"text":"understand that this is sort\nof going backwards from"},"2839":{"dur":5,"text":"Time 128 as to how long\ninformation lasts in an LSTM,"},"2844":{"dur":4,"text":"and it sort of looks\nlike this if I play it."},"2849":{"dur":4,"text":"And so if we then try and drag it back,\nI think, then I can play it more slowly."},"2853":{"dur":4,"text":"All right, so that almost instantaneously,\nthe RNN has less"},"2858":{"dur":5,"text":"information because of\nthe Matrix multiply."},"2864":{"dur":3,"text":"But as you go back,\nthat by the time you've gone back so"},"2867":{"dur":5,"text":"at ten times steps, the RNN is\nessentially lost the information."},"2872":{"dur":4,"text":"Whereas the LSTM even be going back,"},"2877":{"dur":4,"text":"it starts loose information, but you know\nyou sort of gain back this sort of more"},"2881":{"dur":4,"text":"like, time step 30 or\nsomething before it's kind of"},"2886":{"dur":3,"text":"lost all of its information which is sort\nof the intuition I suggested before."},"2889":{"dur":6,"text":"But something like 100 time\nsteps you can get out of a LSTM."},"2896":{"dur":5,"text":"Almost up for a halftime break,\nand the research highlight,"},"2901":{"dur":4,"text":"but before that couple other\nthings I wanted to say,"},"2905":{"dur":3,"text":"here's just a little bit\nof practical advice."},"2909":{"dur":7,"text":"So both for assignment for or\nfor many people's final projects."},"2916":{"dur":3,"text":"They're gonna be wanting\nto train recurrent neural"},"2920":{"dur":3,"text":"networks with LSTMs on a largest scale."},"2924":{"dur":2,"text":"So here is some of the tips\nthat you should know, yes."},"2926":{"dur":3,"text":"So if you wanna build a big\nrecurrent new network,"},"2930":{"dur":2,"text":"definitely use either GRU or an LSTM."},"2933":{"dur":3,"text":"So for any of these recurrent networks,"},"2936":{"dur":4,"text":"initialization is really,\nreally important."},"2941":{"dur":5,"text":"That if your net, recurrent your network\nshould work, if your network isn't"},"2947":{"dur":5,"text":"working, often times it's because\nthe initial initialization is bad."},"2952":{"dur":5,"text":"So what are the kind of initialization\nideas that often tend to be important?"},"2957":{"dur":5,"text":"It's turned to be really useful for\nthe recurrent matrices, that's the one"},"2963":{"dur":4,"text":"where you're multiplying by the previous\nhidden state of previous cell state."},"2967":{"dur":2,"text":"It's really useful to\nmake that one orthogonal."},"2969":{"dur":4,"text":"So there's chance to use your good\nold-fashioned linear algebra."},"2973":{"dur":3,"text":"There aren't actually that many\nparameters in a recurrent neural net."},"2977":{"dur":4,"text":"And giving an orthogonal\ninitialization has proved to"},"2981":{"dur":4,"text":"be a better way to kinda get\nthem learning something useful."},"2986":{"dur":3,"text":"Even with sort of these\nideas with GRUs and LSTMs,"},"2989":{"dur":5,"text":"you're gonna kinda keep multiplying\nthings in a recurrent neural network."},"2994":{"dur":4,"text":"So normally, you wanna have\nyour initialization is small."},"2998":{"dur":3,"text":"If you start off with two large\nvalues that can destroy things,"},"3002":{"dur":3,"text":"try making the numbers smaller."},"3005":{"dur":2,"text":"Here's a little trick, so"},"3008":{"dur":5,"text":"a lot of the times we initialize\nthings near zero, randomly."},"3013":{"dur":5,"text":"An exception to that is when you're\nsetting the bias of a forget gate,"},"3019":{"dur":4,"text":"it normally works out much better\nif you set the bias gate for"},"3024":{"dur":4,"text":"the forget gate to a decent size\npositive number like one or"},"3028":{"dur":3,"text":"two or\na random number close to one or two."},"3032":{"dur":4,"text":"That's sort of effectively saying\nyou should start off paying"},"3036":{"dur":3,"text":"a lot of attention to the distant past."},"3040":{"dur":3,"text":"That's sort of biasing it\nto keep long term memory."},"3043":{"dur":2,"text":"And that sort of encourages\nyou to get a good model."},"3046":{"dur":2,"text":"Which effectively uses long term memory."},"3048":{"dur":5,"text":"And if the long term past stuff isn't\nuseful, it can shrink that down."},"3053":{"dur":3,"text":"But if the forget gate starts\noff mainly forgetting stuff,"},"3057":{"dur":4,"text":"it'll just forget stuff and\nnever change to any other behavior."},"3062":{"dur":3,"text":"In general, these algorithms work much"},"3066":{"dur":2,"text":"better with modern adaptive\nlearning rate algorithms."},"3068":{"dur":2,"text":"We've already been using\nAdam in the assignments."},"3070":{"dur":5,"text":"The ones like Adam, AdaDelta,\nRMSprop work a lot better than basic SGD."},"3076":{"dur":2,"text":"You do wanna clip\nthe norms of the gradients."},"3079":{"dur":3,"text":"You can use a number like five,\nthat'll work fine."},"3082":{"dur":2,"text":"And so,\nwe've used dropout in the assignments, but"},"3085":{"dur":3,"text":"we haven't actually ever talked\nabout it much in lectures."},"3088":{"dur":6,"text":"For RNNs of any sort,\nit's trivial to do dropout vertically."},"3094":{"dur":3,"text":"And that usually improves performance."},"3098":{"dur":1,"text":"It doesn't work and"},"3099":{"dur":5,"text":"I either do drop out horizontally\nalong the recurrent connections."},"3104":{"dur":3,"text":"Because if you have reasonable\npercentage of drop out and"},"3108":{"dur":4,"text":"you run it horizontally then within\nthe few time steps, almost every"},"3112":{"dur":5,"text":"dimension will be dropped in one of them,\nand so you have no information flow."},"3117":{"dur":5,"text":"There have been more recent work\nthat's talked about ways that you"},"3123":{"dur":5,"text":"can successfully do horizontal\ndropout in recurrent networks in,"},"3128":{"dur":4,"text":"including orthongal's PhD student\nin England who did work on so"},"3133":{"dur":3,"text":"called base in drop out\nthat works well for that."},"3136":{"dur":4,"text":"But quite commonly, it's still the case\nthat people just drop out vertically and"},"3140":{"dur":2,"text":"don't drop out at all horizontally."},"3143":{"dur":4,"text":"The final bit of advice is be\npatient if you're running,"},"3147":{"dur":4,"text":"if you're learning recurrent\nnets over large data sets,"},"3151":{"dur":2,"text":"it often takes quite a while and\nyou don't wanna give up."},"3154":{"dur":3,"text":"Sometimes if you just train them\nlong enough start to learn stuff."},"3157":{"dur":4,"text":"This is one of the reasons why we\nreally want to get you guys started"},"3162":{"dur":3,"text":"using GPUs because the fact of the matter,"},"3165":{"dur":4,"text":"if you're actually trying to do\nthings on decent size data sets,"},"3170":{"dur":5,"text":"you just don't wanna be trying to train\nin LSTM or GRU without Using a GPU."},"3176":{"dur":5,"text":"One other last tip that we should\nmention some time is ensembling."},"3182":{"dur":5,"text":"If you'd like your numbers to be 2%\nhigher, very effective strategy,"},"3188":{"dur":4,"text":"which again, makes it good to have a GPU,\nis don't train just one model,"},"3192":{"dur":3,"text":"train ten models and\nyou average their predictions and"},"3196":{"dur":3,"text":"that that normally gives you\nquite significant gains."},"3199":{"dur":4,"text":"So here are some results\nfrom MT Systems trained."},"3204":{"dur":2,"text":"Montreal again."},"3206":{"dur":3,"text":"So it's different language pairs."},"3210":{"dur":2,"text":"The red ones is a single model."},"3212":{"dur":4,"text":"The purple ones are training 8 models,\nand in this case,"},"3217":{"dur":4,"text":"it's actually just majority\nvoting them together."},"3222":{"dur":3,"text":"But you can also sort of\naverage their predictions and"},"3225":{"dur":5,"text":"you can see it's just giving very nice\ngains in performance using the measure for"},"3230":{"dur":3,"text":"mt performance which I'll\nexplain after the break."},"3234":{"dur":7,"text":"But we're now gonna have Michael up\nto talk about the research highlight."},"3242":{"dur":1,"text":"And I'll quickly explain\nit until the video is in"},"3244":{"dur":1,"text":">> After the picture."},"3246":{"dur":1,"text":">> Okay."},"3248":{"dur":0,"text":"Hi, everyone."},"3249":{"dur":4,"text":"I'm gonna be presenting the paper\nLip Reading Sentences in the Wild."},"3254":{"dur":4,"text":"So our task is basically taking a video,\nwhich we preprocessed into"},"3259":{"dur":4,"text":"a sequence of lip-centered images,\nwith or without audio."},"3263":{"dur":4,"text":"And we're trying to predict like the words\nthat are being said in the video."},"3268":{"dur":2,"text":">> Just slide after that one."},"3274":{"dur":2,"text":"Maybe it doesn't"},"3303":{"dur":2,"text":">> The government will pay for both sides."},"3306":{"dur":3,"text":">> We have to look at whether it\n>> Not."},"3309":{"dur":3,"text":"Said security had been\nstepped up in Britain."},"3329":{"dur":4,"text":">> Cool, so anyway,\nit's hard to do lip reading."},"3334":{"dur":4,"text":"So anyway, and for the rest of this I'll\ntalk about what architecture they use,"},"3338":{"dur":3,"text":"which is, they deem the watch,\nlisten, attend, and spell model."},"3342":{"dur":3,"text":">> Gonna talk about some of these training\nstrategies that might also be helpful for"},"3345":{"dur":0,"text":"your final projects."},"3346":{"dur":2,"text":"There's also the dataset and"},"3348":{"dur":5,"text":"the results was actually surpassing\nlike a professional lip reader."},"3354":{"dur":4,"text":"So, the architecture basically\nbreaks down into three components."},"3358":{"dur":5,"text":"We have a watch component which takes\nin the visual and the listening"},"3363":{"dur":6,"text":"component which takes in the audio and\nthese feed information to the attend, and"},"3369":{"dur":5,"text":"spell module which outputs\nthe prediction one character at a time."},"3375":{"dur":4,"text":"And they also use this with like, just the\nwatch module or just the listen module."},"3382":{"dur":4,"text":"To go into slightly more detail,\nfor the watch module,"},"3386":{"dur":5,"text":"we take a sliding window over\nlike the face centered images and"},"3392":{"dur":3,"text":"feed that into a CNN,\nwhich then the output of"},"3395":{"dur":5,"text":"the CNN gets fed into an LSTM\nmuch size over the time steps."},"3401":{"dur":5,"text":"We output a single state vector S of v,\nas well as the set of"},"3406":{"dur":5,"text":"output vectors L of v and\nthe listen module is very similar."},"3412":{"dur":2,"text":"We take the pre-processed speech and"},"3415":{"dur":5,"text":"we again site over using the LSTM,\nand we have another state vector,"},"3420":{"dur":5,"text":"and another set of output vectors,\nand then in the decoding step."},"3426":{"dur":4,"text":"So we have an LSTM as a really\nsteps of during the decoding and"},"3431":{"dur":5,"text":"the initial hidden state is initialized\nas the concatenation of the two hidden"},"3436":{"dur":4,"text":"states from the two previous\nmodules as well as we have"},"3441":{"dur":4,"text":"like a dual attention mechanism\nwhich takes in the output"},"3446":{"dur":4,"text":"vectors from each of their respective\nmodules, and we take those together, and"},"3451":{"dur":4,"text":"we make our prediction using a softmax\nover a multi-layer procepteron."},"3457":{"dur":3,"text":"And so, one strategy that uses\ncalled curriculum learning."},"3460":{"dur":5,"text":"So ordinarily, when you're training\nthis sequence to sequence models,"},"3465":{"dur":4,"text":"you might be tend to just use\none full sentence at a time."},"3470":{"dur":5,"text":"Tip by what they do on curriculum learning\nis you start with the word length like"},"3475":{"dur":5,"text":"segment and then you can slowly increase\nthe length of your training sequences and"},"3481":{"dur":5,"text":"what happens is you're actually like\nthe idea is you're trying to learn,"},"3487":{"dur":3,"text":"like slowly build up the learning for\nthe model and"},"3490":{"dur":5,"text":"what happens is it ends up converging\nfaster as well as decreasing overfitting."},"3496":{"dur":3,"text":"Another thing that they use\nis called scheduled sampling."},"3500":{"dur":4,"text":"So ordinarily during training,\nyou'll be using"},"3504":{"dur":4,"text":"the ground truth input like\ncharacter sequence, but"},"3509":{"dur":5,"text":"during the test time you\nwouldn't be using that you'd just"},"3514":{"dur":5,"text":"be using your previous prediction\nafter every time step."},"3520":{"dur":3,"text":"So what you do in scheduled sampling is\nkind of like bridge the difference in"},"3523":{"dur":3,"text":"scenarios between training and\ntesting is that you actually just for"},"3527":{"dur":4,"text":"a random small probability,\nlike sample from the previous input"},"3531":{"dur":3,"text":"instead of the ground truth input for\nthat time step during training."},"3538":{"dur":6,"text":"So the dataset was taken from the authors\ncollected it from the BBC News and"},"3544":{"dur":4,"text":"they have like dataset that's much\nlarger than the previous ones"},"3549":{"dur":4,"text":"out there with over 17,000\nvocabulary words and"},"3553":{"dur":4,"text":"the other the quite a bit like processing\nto like some other things on the lips, and"},"3558":{"dur":2,"text":"do like the alignment of the audio,\nand the visuals."},"3563":{"dur":5,"text":"So, just to talk about the results, I\nguess the most eye popping result is that"},"3569":{"dur":5,"text":"they gave the test set to actually like\na company that does like professional"},"3574":{"dur":5,"text":"lip reading and they're only able to get\nabout like one in four words correct or"},"3579":{"dur":5,"text":"as this model was able to get one in two,\nroughly, based on word error rate."},"3584":{"dur":4,"text":"And they also did some other\nexperience as well with looking at,"},"3588":{"dur":3,"text":"if you combine the lips\nversion with the audio,"},"3592":{"dur":5,"text":"you get like a slightly better model which\nshows that using both modalities improves"},"3597":{"dur":4,"text":"the model as well as looking at what\nhappens if you add noise to the model."},"3602":{"dur":1,"text":"Great.\nThanks."},"3603":{"dur":5,"text":">> [APPLAUSE]\n>> Thanks, Michael."},"3609":{"dur":1,"text":"Yeah, so obviously,\na lot of details there."},"3611":{"dur":4,"text":"But again, that's kind of an example of\nwhat's been happening with deep learning"},"3616":{"dur":4,"text":"where you're taking this basic model\narchitecture, things like LSTM and saying,"},"3621":{"dur":3,"text":"here's another problem,\nlet's try it on that as well and"},"3624":{"dur":2,"text":"it turns out to work fantastically well."},"3626":{"dur":1,"text":"Let's say, 20 minutes left."},"3628":{"dur":3,"text":"I'll see how high I can get in teaching\neverything else about it on machine"},"3631":{"dur":0,"text":"translation."},"3632":{"dur":4,"text":"So it's something I did just want\nto explain is so, back here and"},"3636":{"dur":4,"text":"in general, when we've been showing\nmachine translation results."},"3641":{"dur":2,"text":"We've been divvying these\ngraphs that up is good and"},"3644":{"dur":4,"text":"what it's been measuring with these\nnumbers are things called blue scores."},"3648":{"dur":5,"text":"So, I wanted to give you some idea of how\nand why we evaluate machine translation."},"3654":{"dur":6,"text":"So the central thing to know about machine\ntranslation is if you take a paragraph or"},"3660":{"dur":4,"text":"text and give it to ten\ndifferent humans translators,"},"3664":{"dur":3,"text":"you'll get back ten\ndifferent translations."},"3668":{"dur":4,"text":"There's no correct answer\nas to how to translate"},"3673":{"dur":3,"text":"a sentence into another language."},"3676":{"dur":4,"text":"And in practice, most of the time\nall translations are imperfect and"},"3681":{"dur":5,"text":"it's kind of deciding what you wanna pay\nmost attention to is that do you want to"},"3686":{"dur":5,"text":"maximally preserve the metaphor that\nthe person used in the source language or"},"3691":{"dur":3,"text":"do you wanna more directly\nconvey the meaning it conveys,"},"3694":{"dur":5,"text":"because that metaphor won't really be\nfamiliar to people in the target language."},"3700":{"dur":2,"text":"Do you want to choose sort\nof short direct words,"},"3703":{"dur":2,"text":"because it's written in a short,\ndirect style?"},"3706":{"dur":2,"text":"Or do you more want to sort of,"},"3708":{"dur":4,"text":"you choose a longer word that's\na more exact translation?"},"3712":{"dur":3,"text":"There's all of these decisions and\nthings and in some sense a translator is"},"3716":{"dur":2,"text":"optimizing over if we do it\nin machine learning terms,"},"3719":{"dur":2,"text":"but the reality is it's\nsort of not very clear."},"3722":{"dur":1,"text":"There are a lot of choices."},"3723":{"dur":3,"text":"You have lots of syntactic choices\nas whether you make it a passive or"},"3727":{"dur":2,"text":"an active and word order, and so on."},"3729":{"dur":0,"text":"No right answer."},"3730":{"dur":3,"text":"So we just can't have it like a lot\nthings of saying, here's the accuracy,"},"3734":{"dur":2,"text":"that was what you were meant to use."},"3736":{"dur":1,"text":"So, how do you do it?"},"3738":{"dur":4,"text":"So, one way to do MT evaluation\nis to do it manually."},"3742":{"dur":4,"text":"You get human beings to look\nat translations and to say,"},"3746":{"dur":1,"text":"how good they are."},"3748":{"dur":1,"text":"And to this day, basically,"},"3749":{"dur":4,"text":"that's regarded as the gold standard\nof machine translation evaluation,"},"3754":{"dur":4,"text":"because we don't have a better\nway to fully automate things."},"3758":{"dur":4,"text":"So one way of doing that is things\nlike Likert scales where you're"},"3763":{"dur":3,"text":"getting humans to judge\ntranslations to adequacy,"},"3766":{"dur":5,"text":"which is how well they convey the meaning\nof the source and fluency which is for"},"3772":{"dur":4,"text":"how natural the output sentence\nsounds in the target language."},"3776":{"dur":5,"text":"Commonly, a way that's more easily\nmeasurable that people prefer is actually"},"3782":{"dur":5,"text":"if you're comparing systems for goodness\nis that you directly ask human beings"},"3788":{"dur":5,"text":"to do pairwise judgments of which is\nbetter translation A or translation B."},"3793":{"dur":3,"text":"I mean, it turns out that even\nthat is incredibly hard for"},"3797":{"dur":5,"text":"humans to do as someone who has sat around\ndoing this task of human evaluation."},"3803":{"dur":2,"text":"I mean, all the time, it's kind of okay,"},"3806":{"dur":4,"text":"this one made a bad word choice here and\nthis one got the wrong verb form"},"3810":{"dur":3,"text":"there which of these do I\nregard as a worse error."},"3814":{"dur":4,"text":"So it's a difficult thing, but\nwe use the data we can from human beings."},"3818":{"dur":3,"text":"Okay, that's still the best\nthing that we can do."},"3821":{"dur":1,"text":"It has problems."},"3823":{"dur":7,"text":"Basically, it's slow and expensive to get\nhuman beings to judge translation quality."},"3830":{"dur":1,"text":"So what else could we do?"},"3832":{"dur":4,"text":"Well, another obvious idea is to say,\nwell, If we can embed machine"},"3837":{"dur":5,"text":"translation into some task, we can just\nsee which is more easily a valuable."},"3842":{"dur":6,"text":"We could just see which MT system\nlets us do the final task better."},"3849":{"dur":4,"text":"So, we'd like to do question answering\nover foreign language documents."},"3853":{"dur":3,"text":"We'll just to get our question\nanswers correct score, and"},"3857":{"dur":2,"text":"they'll be much easier to measure."},"3859":{"dur":2,"text":"And that's something that you can do, but"},"3862":{"dur":3,"text":"it turns out that that often\nisn't very successful."},"3865":{"dur":3,"text":"Cuz commonly your accuracy\non the downstream task is"},"3869":{"dur":4,"text":"very little affected by many of\nthe fine points of translation."},"3874":{"dur":4,"text":"An extreme example of that is sort of\nlike cross-lingual information retrieval."},"3878":{"dur":2,"text":"When you're just wanting\nto retrieve relevant"},"3881":{"dur":2,"text":"documents to a query in another language."},"3884":{"dur":4,"text":"That providing you can kind of produce\nsome of the main content words in"},"3888":{"dur":5,"text":"the translation, it really doesn't matter\nhow you screw up the details of syntax and"},"3893":{"dur":1,"text":"verb inflection."},"3894":{"dur":1,"text":"It's not really gonna affect your score."},"3897":{"dur":3,"text":"Okay, so what people have\nwanted to have is a direct"},"3902":{"dur":4,"text":"metric that is fast and cheap to apply."},"3906":{"dur":4,"text":"And for a long time, I think no one\nthought there was such a thing."},"3911":{"dur":4,"text":"And so\nthen starting in the very early 2000s,"},"3915":{"dur":4,"text":"people at IBM suggested\nthis first idea of, hey,"},"3920":{"dur":5,"text":"here's a cheap way in which we can\nmeasure word translation quality."},"3926":{"dur":2,"text":"And so they called it the BLEU metric."},"3929":{"dur":4,"text":"And so\nhere was the idea of how they do that."},"3934":{"dur":5,"text":"What they said is let us\nproduce reference translations."},"3939":{"dur":4,"text":"We know that there are many, many possible\nways that something can be translated."},"3944":{"dur":8,"text":"But let's get a human being to\nproduce a reference translation."},"3953":{"dur":4,"text":"So what we are going to do is then we're\ngoing to have a reference translation by"},"3957":{"dur":3,"text":"a human, and\nwe're going to have a machine translation."},"3961":{"dur":4,"text":"And to a first approximation we're\ngoing to say that the machine"},"3966":{"dur":4,"text":"translation is good to the extent\nthat you can find word n-grams."},"3971":{"dur":4,"text":"So sequences of words like three\nwords in a row, two words in a row,"},"3975":{"dur":4,"text":"which also appear in the reference\ntranslation anywhere."},"3980":{"dur":1,"text":"So what are the elements of this?"},"3981":{"dur":6,"text":"So by having multi-word sequences,\nthat's meant to be trying to"},"3988":{"dur":4,"text":"judge whether you have some understanding\nof the sort of right syntax and arguments."},"3993":{"dur":3,"text":"Because you're much more likely\nto match a four word sequence"},"3997":{"dur":3,"text":"if it's not just you've\ngot a bag of keywords."},"4000":{"dur":3,"text":"You actually understand something\nof the syntax of the sentence."},"4004":{"dur":4,"text":"The fact that you can match it anywhere\nis meant to be dealing with the fact that"},"4008":{"dur":3,"text":"human languages normally have\nquite flexible word order."},"4011":{"dur":5,"text":"So it's not adequate to insist that\nthe phrases appear in the same word order."},"4017":{"dur":4,"text":"Of course, in general in English, a lot of\nthe time you can say, last night I went"},"4022":{"dur":4,"text":"to my friend's place, or,\nI went to my friend's place last night."},"4027":{"dur":2,"text":"And it seems like you should\nget credit for last night"},"4029":{"dur":2,"text":"regardless of whether you put it at\nthe beginning or the end of the sentence."},"4033":{"dur":3,"text":"So, that was the general idea\nin slightly more detail."},"4037":{"dur":2,"text":"The BLEU measure is a precision score."},"4040":{"dur":4,"text":"So it's looking at whether\nn-grams that are in the machine"},"4045":{"dur":4,"text":"translation also appear in\nthe reference translation."},"4049":{"dur":2,"text":"There are a couple of fine points then."},"4051":{"dur":5,"text":"You are only allowed to count for\na certain n and n-gram once."},"4056":{"dur":5,"text":"So if in your translation,\nthe airport appears three times,"},"4061":{"dur":2,"text":"but there's only one\nthe airport in the reference,"},"4064":{"dur":3,"text":"you're only allowed to count one of\nthem as correct, not all three of them."},"4068":{"dur":5,"text":"And then there's this other trick that we\nhave, this thing called a brevity penalty."},"4073":{"dur":3,"text":"Because if it's purely\na precision-oriented measure,"},"4077":{"dur":4,"text":"saying is what appears in the machine\ntranslation in the reference."},"4081":{"dur":1,"text":"There are games you could play,"},"4083":{"dur":3,"text":"like you could just translate\nevery passage with the word the."},"4087":{"dur":3,"text":"Because if it's English the word the is\npretty sure to appear somewhere in"},"4091":{"dur":2,"text":"the reference translation,\nand get precision one."},"4094":{"dur":2,"text":"And that seems like it's cheating."},"4096":{"dur":5,"text":"So if you're making what your translation\nis shorter than the human translations,"},"4101":{"dur":3,"text":"you'll lose."},"4105":{"dur":6,"text":"Okay, so more formally, so you're doing\nthis with n-grams up to a certain size."},"4111":{"dur":4,"text":"Commonly it's four so you use single\nwords, pairs of words, triples,"},"4115":{"dur":1,"text":"and four words."},"4116":{"dur":2,"text":"You work out this kind\nof precision of each."},"4118":{"dur":4,"text":"And then you're working out a kind\nof a weighted geometric mean"},"4123":{"dur":1,"text":"of those precisions."},"4124":{"dur":2,"text":"And you multiplying that\nby brevity penalty."},"4127":{"dur":4,"text":"And the brevity penalty penalizes\nyou if your translation"},"4132":{"dur":3,"text":"is shorter than the reference translation."},"4136":{"dur":4,"text":"There are some details here, but\nmaybe I'll just skip them and go ahead."},"4140":{"dur":6,"text":"So there's one other idea then which is,\nwell, what about this big problem that,"},"4146":{"dur":4,"text":"well, there are a lot of different\nways to translate things."},"4151":{"dur":4,"text":"And there's no guarantee that your\ntranslation could be great, and"},"4155":{"dur":3,"text":"it might just not match\nthe human's translation."},"4158":{"dur":5,"text":"And so the answer to that that\nthe original IBM paper suggested"},"4164":{"dur":5,"text":"was what we should do is collect\na bunch of reference translations."},"4170":{"dur":3,"text":"And the suggested number that's\nbeen widely used was four."},"4174":{"dur":5,"text":"And so then, most likely,\nif you're giving a good translation,"},"4179":{"dur":3,"text":"it'll appear in one of\nthe reference translations."},"4183":{"dur":2,"text":"And then, you'll get a matching n-gram."},"4185":{"dur":3,"text":"Now, of course,\nthat's the sort of a statistical argument."},"4189":{"dur":2,"text":"Cuz you might have a really\ngood translation and"},"4191":{"dur":2,"text":"none of the four translators chose it."},"4193":{"dur":3,"text":"And the truth is then in\nthat case you just lose."},"4197":{"dur":4,"text":"And indeed what's happened in more\nrecent work is quite a lot of the time,"},"4202":{"dur":4,"text":"actually, the BLEU measure is only\nrun with one reference translation."},"4207":{"dur":2,"text":"And that's seems a little bit cheap."},"4210":{"dur":4,"text":"And it's certainly the case that if you're\nrunning with one reference translation,"},"4214":{"dur":4,"text":"you're either just lucky or unlucky as to\nwhether you guessed to translate the way"},"4219":{"dur":1,"text":"the translator translates."},"4221":{"dur":4,"text":"But you can make a sort of a statistical\nargument which by and large is valid."},"4225":{"dur":2,"text":"That if you're coming up\nwith good translations,"},"4228":{"dur":4,"text":"providing there's no correlation somehow\nbetween one system and the translator."},"4233":{"dur":4,"text":"That you'd still expect on balance that\nyou'll get a higher score if you're"},"4238":{"dur":2,"text":"consistently giving better translations."},"4240":{"dur":2,"text":"And broadly speaking, that's right."},"4243":{"dur":6,"text":"Though this problem of correlation does\nactually start to rear its head, right?"},"4249":{"dur":4,"text":"That if the reference translator always\ntranslated the things as US, and"},"4254":{"dur":5,"text":"one system translates with US, and the\nother one translates with United States."},"4259":{"dur":1,"text":"Kind of one person will get lucky, and"},"4261":{"dur":2,"text":"the other one will get unlucky\nin a kind of a correlated way."},"4264":{"dur":2,"text":"And that can create problems."},"4267":{"dur":5,"text":"So even though it was very simple\nwhen BLEU was initially introduced,"},"4272":{"dur":4,"text":"it seemed to be miraculously\ngood that it just corresponded"},"4277":{"dur":5,"text":"really well with human judgments\nof translation quality."},"4282":{"dur":4,"text":"Rarely do you see an empirical\ndata set that's as linear as that."},"4286":{"dur":2,"text":"And so this seemed really awesome."},"4289":{"dur":4,"text":"Like many things that\nare surrogate metrics,"},"4293":{"dur":5,"text":"there are a lot of surrogate metrics that\nwork really well If no one is trying"},"4298":{"dur":5,"text":"to optimize them but don't work so well\nonce people are trying to optimize them."},"4303":{"dur":1,"text":"So what happen then was,"},"4305":{"dur":4,"text":"everyone evaluated their systems\non BLEU scores and so therefore,"},"4309":{"dur":4,"text":"all researchers worked on how to make\ntheir systems have better BLEU scores."},"4313":{"dur":4,"text":"And then what happened is this\ncorrelation graph went way down."},"4318":{"dur":4,"text":"And so the truth is now that current,\nand this relates to the sort of"},"4323":{"dur":3,"text":"when I was saying the Google\nresults were exaggerated."},"4326":{"dur":7,"text":"The truth is that current MT systems\nproduce BLEU scores that are very similar"},"4334":{"dur":4,"text":"to human translations for many language\npairs which reflects the fact that"},"4338":{"dur":5,"text":"different human beings are quite creative\nand vary in how they translate sensors."},"4343":{"dur":4,"text":"But in truth, the quality of machine\ntranslation is still well below"},"4347":{"dur":3,"text":"the quality of human translation."},"4351":{"dur":3,"text":"Okay, few minutes left to\nsay a bit more about MT."},"4355":{"dur":2,"text":"I think I can't get through\nall this material, but"},"4357":{"dur":3,"text":"let me just give you a little\nbit of a sense of some of it."},"4362":{"dur":4,"text":"Okay, so one of the big problems you\nhave if you've tried to build something,"},"4366":{"dur":2,"text":"any kind of generation system,"},"4369":{"dur":5,"text":"where you're generating words is you have\na problem that there are a lot of words."},"4375":{"dur":3,"text":"Languages have very large vocabularies."},"4378":{"dur":4,"text":"So from the hidden state,\nwhat we're doing is multiplying by this"},"4382":{"dur":5,"text":"matrix of Softmax parameters,\nwhich is the size of the vocabulary"},"4387":{"dur":4,"text":"times the size of the hidden\nstate doing this Softmax."},"4392":{"dur":2,"text":"And that's giving us\nthe probability of different words."},"4396":{"dur":3,"text":"And so the problem is if you wanna\nhave a very large vocabulary,"},"4399":{"dur":5,"text":"you spend a huge amount of time just doing\nthese Softmaxes over, and over again."},"4404":{"dur":5,"text":"And so, for instance, you saw that in\nthe kind of pictures of the Google system,"},"4409":{"dur":4,"text":"that over half of their\ncomputational power was just going"},"4414":{"dur":4,"text":"into calculating these Softmax so\nthat's being a real problem."},"4418":{"dur":4,"text":"So something people have worked\non quite a lot is how can we"},"4422":{"dur":2,"text":"string the cost of that computation."},"4426":{"dur":3,"text":"Well one thing we can do is say,\nha, let's use a smaller vocabulary."},"4429":{"dur":4,"text":"Let's only use a 50,000 word\nvocabulary for our MT system, and"},"4434":{"dur":2,"text":"some of the early MT\nwork did precisely that."},"4436":{"dur":4,"text":"But the problem is, that if you do that,\nyou start with lively sentences."},"4441":{"dur":4,"text":"And instead what you get is unk,\nunk, unk because all of"},"4446":{"dur":5,"text":"the interesting words in the sentence fall\noutside of your 50,000 word vocabulary."},"4452":{"dur":5,"text":"And those kind of sentences are not very\ngood ones to show that human beings,"},"4458":{"dur":2,"text":"because they don't like them very much."},"4460":{"dur":5,"text":"So, it seems like we need to\nsomehow do better than that."},"4465":{"dur":4,"text":"So, there's been work on, well, how can\nwe more effectively do the softmaxes"},"4470":{"dur":2,"text":"without having to do as much computation."},"4473":{"dur":2,"text":"And so,\nthere have been some ideas on that."},"4476":{"dur":5,"text":"One idea is to sort of have a hierarchical\nSoftmax where we do the standard"},"4481":{"dur":3,"text":"computer scientist trick\nof putting a tree structure"},"4484":{"dur":2,"text":"to improve our amount of computation."},"4486":{"dur":3,"text":"So if you can sort of divide the\nvocabulary into sort of tree pieces and"},"4490":{"dur":4,"text":"divide down branches of the tree,\nwe can do less computation."},"4496":{"dur":4,"text":"Remember, we did noise\ncontrast to destination for"},"4500":{"dur":3,"text":"words of that was a way\nof avoiding computation."},"4503":{"dur":2,"text":"Those are possible ways to do things."},"4506":{"dur":2,"text":"They are not very\nGPU-friendly unfortunately."},"4509":{"dur":4,"text":"Once you start taking branches down\nthe tree, you then can't do the kind"},"4513":{"dur":3,"text":"of nice just bang bang bang type\nof computations down to GPU."},"4517":{"dur":4,"text":"So there's been on work on coming\nup with alternatives to that, and"},"4521":{"dur":3,"text":"I wanted to mention one example of this."},"4524":{"dur":4,"text":"And an idea of this is well,\nmaybe we can actually"},"4529":{"dur":4,"text":"sort of just work with small\nvocabularies at any one time."},"4534":{"dur":4,"text":"So when we're training our models,\nwe could train using subsets of"},"4539":{"dur":5,"text":"the vocabulary because there's a lot\nof rare words but they're rare."},"4545":{"dur":6,"text":"So if you pick any slice of the training\ndata most rare words won't be in it."},"4551":{"dur":3,"text":"Commonly if you look at your\nwhole vocabulary about 40% of"},"4555":{"dur":2,"text":"your word types occur only once."},"4557":{"dur":4,"text":"That means if you cut your\ndata set into 20 pieces,"},"4562":{"dur":2,"text":"19 of those 20 will not contain that word."},"4564":{"dur":4,"text":"And then,\nwe also wanna be smart on testing."},"4569":{"dur":6,"text":"So we wanna be able to, at test time\nas well, generate sort of a smaller"},"4575":{"dur":5,"text":"set of words for our soft max, and so we\ncan be fast at both train and test time."},"4580":{"dur":1,"text":"Well, how can you do that?"},"4582":{"dur":6,"text":"Well, so at training time,\nwe want to have a small vocabulary."},"4588":{"dur":5,"text":"And so we can do that by partitioning the\nvocab, for partitioning the training data,"},"4593":{"dur":4,"text":"each slice of the training data,\nwe'll have a much lower vocabulary."},"4599":{"dur":5,"text":"And then we could partition randomly or\nwe could even smarter and"},"4604":{"dur":4,"text":"we can cut it into pieces\nthat have similar vocabulary."},"4609":{"dur":3,"text":"If we put all the basketball\narticles in one file and"},"4612":{"dur":5,"text":"all the foot walled articles in another\npile, will shrink the vocabulary further."},"4617":{"dur":5,"text":"And so they look at ways of doing that,\nso in practice that they can get down and"},"4623":{"dur":4,"text":"order a magnitude or more in the size\nof the vocab that they need for"},"4627":{"dur":2,"text":"each slice of the data, that's great."},"4633":{"dur":2,"text":"Okay, so what do we do at test time?"},"4635":{"dur":2,"text":"Well, what we wanna do\nit at test time as well,"},"4638":{"dur":5,"text":"when we're actually translating,\nwe want to use as much smaller vocabulary."},"4643":{"dur":2,"text":"Well, here's an idea of\nhow you could do that."},"4646":{"dur":2,"text":"Firstly, we say, they're are just common"},"4648":{"dur":3,"text":"function words that we always\ngonna want to have available."},"4652":{"dur":2,"text":"So we pick the K most frequent words and"},"4655":{"dur":2,"text":"say we're always gonna\nhave them in our Softmax."},"4657":{"dur":2,"text":"But then for the rest of it,"},"4659":{"dur":4,"text":"what we're actually gonna do is\nsort of have a lexicon on the side"},"4664":{"dur":4,"text":"where we're gonna know about likely\ntranslations for each source word."},"4669":{"dur":4,"text":"So that we'll have stored ways that would\nbe reasonable to translate she loves"},"4674":{"dur":2,"text":"cats into French."},"4676":{"dur":3,"text":"And so when we're translating a sentence,\nwe'll look out for"},"4679":{"dur":4,"text":"each word in the source sentence what\nare likely translations of it and"},"4684":{"dur":3,"text":"throw those into our candidates for\nthe Softmax."},"4689":{"dur":5,"text":"And so then we've got a sort\nof a candidate list of words."},"4694":{"dur":2,"text":"And when translating\na particular soft sentence,"},"4697":{"dur":3,"text":"we'll only run our\nSoftmax over those words."},"4701":{"dur":7,"text":"And then again, we can save well over\nan order of magnitude computations."},"4708":{"dur":5,"text":"So, K prime is about 10 or 20 and\nK is sort of a reasonable size vocab."},"4714":{"dur":4,"text":"We can again, sort of cut at least in\nthe order of magnitude the size of"},"4719":{"dur":3,"text":"our soft mixers and\nact as if we had large vocabulary."},"4724":{"dur":5,"text":"There are other ways to do that too,\nwhich are on this slide."},"4730":{"dur":4,"text":"And what I was then going to go on,\nand we'll decide whether it does or"},"4734":{"dur":2,"text":"doesn't happen based on the syllabus."},"4737":{"dur":5,"text":"I mean, you could sort of say,\nwell, that's still insufficient"},"4742":{"dur":4,"text":"because I sort of said that you have\nto deal with a large vocabulary."},"4747":{"dur":6,"text":"And you've sort of told us how to deal\nwith a large vocabulary more efficiently."},"4753":{"dur":4,"text":"But you've still got problems, because\nin any new piece of text you give it,"},"4758":{"dur":4,"text":"you're going to have things like new\nnames turn up, new numbers turn up, and"},"4763":{"dur":3,"text":"you're going to want to\ndeal with those as well."},"4766":{"dur":4,"text":"And so\nit seems like somehow we want to be able"},"4770":{"dur":5,"text":"to just deal with new stuff at test time,\nat translation time."},"4775":{"dur":3,"text":"Which effectively means that\nkind of theoretically we have"},"4779":{"dur":1,"text":"an infinite vocabulary."},"4780":{"dur":3,"text":"And so, there's also been a bunch of\nwork on newer machine translation and"},"4784":{"dur":1,"text":"dealing with that."},"4785":{"dur":3,"text":"But unfortunately, this class time is not"},"4789":{"dur":4,"text":"long enough to tell you about it right\nnow, so I'll stop here for today."},"4793":{"dur":4,"text":"And don't forget, outside you can\ncollect your midterm on the way out."}}