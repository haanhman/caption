{"0.01":{"start":"0.008","dur":"7.431","text":"[SOUND] Stanford University."},"7.44":{"start":"7.439","dur":"3.042","text":"&gt;&gt; We&#39;ll get back started\nagain with CS224N,"},"10.48":{"start":"10.481","dur":"3.979","text":"Natural Language Processing\nwith Deep Learning."},"14.46":{"start":"14.46","dur":"5.644","text":"So, you&#39;re in for a respite,\nor a change of pace today."},"20.10":{"start":"20.104","dur":"5.984","text":"So for today&#39;s lecture, what we&#39;re\nprincipally going to look at is syntax,"},"26.09":{"start":"26.088","dur":"2.641","text":"grammar and dependency parsing."},"28.73":{"start":"28.729","dur":"4.537","text":"So my hope today is to teach\nyou in one lecture enough about"},"33.27":{"start":"33.266","dur":"4.915","text":"dependency grammars and\nparsing that you&#39;ll all be able to do"},"38.18":{"start":"38.181","dur":"4.159","text":"the main part of\nAssignment 2 successfully."},"42.34":{"start":"42.34","dur":"4.46","text":"So quite a bit of the early part of\nthe lecture is giving a bit of background"},"46.80":{"start":"46.8","dur":"2.54","text":"about syntax and dependency grammar."},"49.34":{"start":"49.34","dur":"4.115","text":"And then it&#39;s time to talk about\na particular kind of dependency grammar,"},"53.46":{"start":"53.455","dur":"5.113","text":"transition-based, also dependency parsing,\ntransition-based dependency parsing."},"58.57":{"start":"58.568","dur":"4.551","text":"And then it&#39;s probably only in\nthe last kind of 15 minutes or so"},"63.12":{"start":"63.119","dur":"6.011","text":"of the lecture that we&#39;ll then get back\ninto specifically neural network content."},"69.13":{"start":"69.13","dur":"4.93","text":"Talking about a dependency parser that\nDanqi and I wrote a couple of years ago."},"74.06":{"start":"74.06","dur":"2.627","text":"Okay, so for general reminders,"},"76.69":{"start":"76.687","dur":"4.902","text":"I hope you&#39;re all really aware\nthat Assignment 1 is due today."},"81.59":{"start":"81.589","dur":"4.861","text":"And I guess by this stage you&#39;ve either\nmade good progress or you haven&#39;t."},"86.45":{"start":"86.45","dur":"5.8","text":"But to give my,\nGood housekeeping reminders,"},"92.25":{"start":"92.25","dur":"4.054","text":"I mean it seems like every year there\nare people that sort of blow lots of"},"96.30":{"start":"96.304","dur":"3.516","text":"late days on the first assignment for\nno really good reason."},"99.82":{"start":"99.82","dur":"3.549","text":"And that isn&#39;t such\na clever strategy [LAUGH]."},"103.37":{"start":"103.369","dur":"4.619","text":"So hopefully [LAUGH] you are well\nalong with the assignment, and"},"107.99":{"start":"107.988","dur":"3.672","text":"can aim to hand it in before\nit gets to the weekend."},"112.78":{"start":"112.775","dur":"6.665","text":"Okay, then secondly today is also the day\nthat the new assignment comes out."},"119.44":{"start":"119.44","dur":"3.3","text":"So maybe you won&#39;t look at it\ntill the start of next week but"},"122.74":{"start":"122.74","dur":"2.05","text":"we&#39;ve got it up ready to go."},"124.79":{"start":"124.79","dur":"5.478","text":"And so that&#39;ll involve a couple of new\nthings and in some respects probably for"},"130.27":{"start":"130.268","dur":"5.396","text":"much of it you might not want to start\nit until after next Tuesday&#39;s lecture."},"135.66":{"start":"135.664","dur":"3.703","text":"So two big things will be different for\nthat assignment."},"139.37":{"start":"139.367","dur":"5.997","text":"Big thing number one is we&#39;re gonna do\nassignment number two using TensorFlow."},"145.36":{"start":"145.364","dur":"4.05","text":"And that&#39;s the reason why, quite apart\nfrom exhaustion from assignment one,"},"149.41":{"start":"149.414","dur":"4.049","text":"why you probably you don&#39;t wanna start\nit on the weekend is because on Tuesday,"},"153.46":{"start":"153.463","dur":"3.343","text":"Tuesday&#39;s lecture&#39;s gonna be\nan introduction to TensorFlow."},"156.81":{"start":"156.806","dur":"3.555","text":"So you&#39;ll really be more qualified\nthen to start it after that."},"160.36":{"start":"160.361","dur":"5.084","text":"And then the other big different thing\nin assignment two is we get into"},"165.45":{"start":"165.445","dur":"5.184","text":"some sort of more substantive\nnatural language processing content."},"170.63":{"start":"170.629","dur":"5.064","text":"In particular, you guys are going to build\nneural dependency parsers, and the hope"},"175.69":{"start":"175.693","dur":"4.585","text":"is that you can learn about everything\nthat you need to know to do that today."},"180.28":{"start":"180.278","dur":"2.989","text":"Or perhaps looking at some of\nthe readings on the website,"},"183.27":{"start":"183.267","dur":"3.133","text":"if you don&#39;t get quite\neverything straight from me."},"186.40":{"start":"186.4","dur":"2.53","text":"Couple more comments on things."},"188.93":{"start":"188.93","dur":"4.135","text":"Okay, so for final projects."},"193.07":{"start":"193.065","dur":"4.002","text":"We&#39;re going to sort of post,\nhopefully tomorrow or on the weekend,"},"197.07":{"start":"197.067","dur":"4.209","text":"a kind of an outline of what&#39;s in\nassignment four, so you can have sort of"},"201.28":{"start":"201.276","dur":"4.968","text":"a more informed meaningful choice between\nwhether you want to do assignment four, or"},"206.24":{"start":"206.244","dur":"1.727","text":"come up with a final project."},"207.97":{"start":"207.971","dur":"3.204","text":"The area of assignment four, if you do it,"},"211.18":{"start":"211.175","dur":"4.415","text":"is going to be question answering\nover the SQuAD dataset."},"215.59":{"start":"215.59","dur":"3.99","text":"But we&#39;ve got kind of a page and a half\ndescription to explain what that means, so"},"219.58":{"start":"219.58","dur":"1.41","text":"you can look out for that."},"220.99":{"start":"220.99","dur":"3.93","text":"But if you are interested in\ndoing a final project, again,"},"224.92":{"start":"224.92","dur":"5.93","text":"we&#39;ll encourage people to come and meet\nwith one of the final project mentors or"},"230.85":{"start":"230.85","dur":"4.38","text":"find some other well qualified person\naround here to be a final project mentor."},"235.23":{"start":"235.23","dur":"4.9","text":"So what we&#39;re wanting is that sort of,\neverybody has met with"},"240.13":{"start":"240.13","dur":"3.61","text":"their final project mentor\nbefore putting in an abstract."},"243.74":{"start":"243.74","dur":"2.49","text":"And that means it&#39;d be really great for"},"246.23":{"start":"246.23","dur":"3.532","text":"people to get started doing\nthat as soon as possible."},"249.76":{"start":"249.762","dur":"3.218","text":"I know some of you have already\ntalked to various of us."},"252.98":{"start":"252.98","dur":"5.141","text":"For me personally, I&#39;ve got final\nproject office hours tomorrow"},"258.12":{"start":"258.121","dur":"4.515","text":"from 1 to 3 pm so\nI hope some people will come by for those."},"262.64":{"start":"262.636","dur":"2.796","text":"And again, sort of as Richard mentioned,"},"265.43":{"start":"265.432","dur":"4.904","text":"not everybody can possible have Richard or\nme as the final project mentor."},"270.34":{"start":"270.336","dur":"4.358","text":"And besides, there&#39;s some really big\nadvantages of having some of the PhD"},"274.69":{"start":"274.694","dur":"2.296","text":"student TAs as final project mentors."},"276.99":{"start":"276.99","dur":"4.09","text":"Cuz really, for things like spending\ntime hacking on TensorFlow,"},"281.08":{"start":"281.08","dur":"2.17","text":"they get to do it much more than I do."},"283.25":{"start":"283.25","dur":"2.407","text":"And so, Danqi, Kevin, Ignacio,"},"285.66":{"start":"285.657","dur":"5.723","text":"Arun that they&#39;ve had tons of experience\ndoing NLP research using deep learning."},"291.38":{"start":"291.38","dur":"3.99","text":"And so that they&#39;d also be great mentors,\nand look them up for"},"295.37":{"start":"295.37","dur":"2.93","text":"their final project advice."},"298.30":{"start":"298.3","dur":"5.516","text":"The final thing I just want to touch\non is we clearly had a lot of problems,"},"303.82":{"start":"303.816","dur":"4.975","text":"I realize, at keeping up and\ncoping with people in office hours,"},"308.79":{"start":"308.791","dur":"4.262","text":"and queue status has just\nregularly got out of control."},"313.05":{"start":"313.053","dur":"3.769","text":"I&#39;m sorry that that&#39;s\nbeen kind of difficult."},"316.82":{"start":"316.822","dur":"5.169","text":"I mean honestly we are trying to work and\nwork out ways that we can do this better,"},"321.99":{"start":"321.991","dur":"4.784","text":"and we&#39;re thinking of sort of unveiling\na few changes for doing things for"},"326.78":{"start":"326.775","dur":"1.64","text":"the second assignment."},"328.42":{"start":"328.415","dur":"4.888","text":"If any of you peoples have any better\nadvice as to how things could be"},"333.30":{"start":"333.303","dur":"4.889","text":"organized so that they could work\nbetter feel free to send a message"},"338.19":{"start":"338.192","dur":"3.522","text":"on Piazza with suggestions\nof ways of doing it."},"341.71":{"start":"341.714","dur":"4.921","text":"I guess yesterday I ran down\nPercy Liang and said, Percy,"},"346.64":{"start":"346.635","dur":"3.225","text":"Percy, how do you do it for CS221?"},"349.86":{"start":"349.86","dur":"2.26","text":"Do you have some big\nsecrets to do this better?"},"352.12":{"start":"352.12","dur":"4.96","text":"But unfortunately I seem to come away\nwith no big secrets cuz he sort of said:"},"357.08":{"start":"357.08","dur":"5","text":"&quot;we use queue status and we use the Huang\nbasement&quot;, what else are you meant to do?"},"362.08":{"start":"362.08","dur":"3.535","text":"So I&#39;m still looking for\nthat divine insight [LAUGH] that"},"365.62":{"start":"365.615","dur":"4.225","text":"will tell me how to get this\nproblem better under control."},"369.84":{"start":"369.84","dur":"2.87","text":"So if you&#39;ve got any good ideas,\nfeel free to share."},"372.71":{"start":"372.71","dur":"4.337","text":"But we&#39;ll try to get\nthis as much better under"},"377.05":{"start":"377.047","dur":"4.121","text":"control as we can for the following weeks."},"382.22":{"start":"382.217","dur":"3.819","text":"Okay, any questions, or\nshould I just go into the meat of things?"},"389.63":{"start":"389.631","dur":"0.924","text":"Okay."},"392.77":{"start":"392.772","dur":"5.017","text":"All right, so what we&#39;re going\nto want to do today is work"},"397.79":{"start":"397.789","dur":"5.761","text":"out how to put structures over\nsentences in some human language."},"403.55":{"start":"403.55","dur":"4.18","text":"All the examples I&#39;m going to show is for\nEnglish, but in principle,"},"407.73":{"start":"407.73","dur":"5.71","text":"the same techniques you can apply for\nany language, where these structures"},"413.44":{"start":"413.44","dur":"5.3","text":"are going to sort of reveal\nhow the sentence is made up."},"418.74":{"start":"418.74","dur":"6.22","text":"So that the idea is that sentences and\nparts of sentences have some kind"},"424.96":{"start":"424.96","dur":"4.84","text":"of structure and there are sort of regular\nways that people put sentences together."},"429.80":{"start":"429.8","dur":"5.18","text":"So, we can sort of start off with very\nsimple things that aren&#39;t yet sentences"},"434.98":{"start":"434.98","dur":"5.01","text":"like &quot;the cat&quot; and &quot;a dog&quot;, and they\nseem to kind of have a bit of structure."},"439.99":{"start":"439.99","dur":"3.58","text":"We have an article, or\nwhat linguists often call a determiner,"},"443.57":{"start":"443.57","dur":"1.628","text":"that&#39;s followed by a noun."},"445.20":{"start":"445.198","dur":"3.024","text":"And then, well, for those kind of phrases,"},"448.22":{"start":"448.222","dur":"3.583","text":"which get called noun\nphrases that describe things,"},"451.81":{"start":"451.805","dur":"5.43","text":"you can kind of make them bigger and there\nare sort of rules for how you can do that."},"457.24":{"start":"457.235","dur":"4.965","text":"So you can put adjectives in\nbetween the article and the noun."},"462.20":{"start":"462.2","dur":"6.282","text":"You can say the large dog or a barking dog\nor a cuddly dog, and things like that."},"468.48":{"start":"468.482","dur":"5.398","text":"And, well, you can put things like what I\ncall prepositional phrases after the noun"},"473.88":{"start":"473.88","dur":"5.25","text":"so you can get things like &quot;a large dog\nin a crate&quot; or something like that."},"479.13":{"start":"479.13","dur":"4.88","text":"And so, traditionally what linguists and\nnatural language processors have"},"484.01":{"start":"484.01","dur":"5.5","text":"wanted to do is describe\nthe structure of human languages."},"489.51":{"start":"489.51","dur":"6.09","text":"And they&#39;re effectively two key tools\nthat people have used to do this and"},"495.60":{"start":"495.6","dur":"2.55","text":"one of these key tools and"},"498.15":{"start":"498.15","dur":"5.62","text":"I think in general the only one\nyou have seen a fraction of is"},"503.77":{"start":"503.77","dur":"4.93","text":"to use what in computer science terms what\nis most commonly referred to as context"},"508.70":{"start":"508.7","dur":"4.63","text":"free grammars which are often referred to\nby linguists as phrase structure grammars."},"513.33":{"start":"513.33","dur":"3.77","text":"And is then referred to as\nthe notion of constituency and so"},"517.10":{"start":"517.1","dur":"5.06","text":"for that what we are doing is writing\nthese context free grammar rules and"},"522.16":{"start":"522.16","dur":"3.04","text":"the least if you are Standford\nundergrad or something like that."},"525.20":{"start":"525.2","dur":"1.49","text":"I know that way back in 103,"},"526.69":{"start":"526.69","dur":"5.7","text":"you spent a whole lecture learning about\ncontext-free grammars, and their rules."},"532.39":{"start":"532.39","dur":"4.38","text":"So I could start writing some rules that\nmight start off saying a noun phrase,"},"536.77":{"start":"536.77","dur":"2.11","text":"and go to a determiner or a noun."},"538.88":{"start":"538.88","dur":"3.29","text":"Then I realized that noun phrases\nwould get a bit more complicated."},"542.17":{"start":"542.17","dur":"3.81","text":"And so I came up with this new rule\nthat says- Noun phrase goes to terminal"},"545.98":{"start":"545.98","dur":"4.38","text":"optional adject of noun and then\noptional prepositional phrase wherefore"},"550.36":{"start":"550.36","dur":"3.82","text":"prepositional phrase that&#39;s a preposition\nfollowed by another noun phrase."},"554.18":{"start":"554.18","dur":"3.94","text":"Because, I can say a crate,\nor, a large crate."},"558.12":{"start":"558.12","dur":"2.17","text":"Or, a large crate by the door."},"560.29":{"start":"560.29","dur":"5.17","text":"And then, well I can go along\neven further, and I could say,"},"565.46":{"start":"565.46","dur":"6.33","text":"you know a large barking\ndog by the door in a crate."},"571.79":{"start":"571.79","dur":"3.44","text":"So then I noticed, wow I can put\nin multiple adjectives there and"},"575.23":{"start":"575.23","dur":"4.84","text":"I can stick on multiple prepositional\nphrases, so I&#39;m using that star,"},"580.07":{"start":"580.07","dur":"1.73","text":"the kinda clingy star that you also see,"},"581.80":{"start":"581.8","dur":"4.64","text":"See in regular expressions to say that\nyou can have zero or any number of these."},"586.44":{"start":"586.44","dur":"6.18","text":"And then I can start making a bigger\nthing like, talk to the cuddly dog."},"592.62":{"start":"592.62","dur":"1.97","text":"Or, look for the cuddly dog."},"594.59":{"start":"594.59","dur":"4.45","text":"And, well, now I&#39;ve got a verb\nfollowed by a prepositional phrase."},"599.04":{"start":"599.04","dur":"2.95","text":"And so, I can sort of build\nup a constituency grammar."},"603.67":{"start":"603.67","dur":"6.78","text":"So that&#39;s one way of organizing\nthe structure of sentences and,"},"610.45":{"start":"610.45","dur":"6.27","text":"you know,\nin 20th dragging into 21st century"},"616.72":{"start":"616.72","dur":"4.8","text":"America, this has been\nthe dominant way of doing it."},"621.52":{"start":"621.52","dur":"5.33","text":"I mean it&#39;s what you see mainly in your\nIntro CS class when you get taught"},"627.92":{"start":"627.92","dur":"5.43","text":"about regular languages and context free\nlanguages and context sensitive languages."},"633.35":{"start":"633.35","dur":"5.778","text":"You&#39;re working up the Chomsky\nhierarchy where Noam Chomsky"},"639.13":{"start":"639.128","dur":"5.752","text":"did not actually invent\nthe Chomsky hierarchy to torture"},"644.88":{"start":"644.88","dur":"5.19","text":"CS under grads With formal content\nto fill the SCS 103 class."},"650.07":{"start":"650.07","dur":"3.97","text":"The original purpose of the Chomsky\nhierarchy was actually to understand"},"654.04":{"start":"654.04","dur":"5.58","text":"the complexity of human languages and\nto make arguments about their complexity."},"662.29":{"start":"662.29","dur":"4.73","text":"If you look more broadly, and\nsorry, it&#39;s also dominated,"},"667.02":{"start":"667.02","dur":"5.54","text":"sorta linguistics in America in the last\n50 years through the work of Noam Chomsky."},"672.56":{"start":"672.56","dur":"5","text":"But if you look more broadly than that,\nthis isn&#39;t actually the dominate form"},"677.56":{"start":"677.56","dur":"2.39","text":"of syntactic description\nthat is being used for"},"679.95":{"start":"679.95","dur":"2.43","text":"understanding of\nthe structure of sentences."},"682.38":{"start":"682.38","dur":"0.96","text":"So what else can you do?"},"683.34":{"start":"683.34","dur":"4.05","text":"So there is this other alternative view of\nlinguistic structure which is referred to"},"687.39":{"start":"687.39","dur":"4.39","text":"as Dependency structure and\nwhat your doing with dependency structure."},"691.78":{"start":"691.78","dur":"5.69","text":"Is that you&#39;re describing the structure\nof a sentence by taking each word and"},"697.47":{"start":"697.47","dur":"2.34","text":"saying what it&#39;s a dependent on."},"699.81":{"start":"699.81","dur":"3.28","text":"So, if it&#39;s a word that\nkind of modifies or"},"703.09":{"start":"703.09","dur":"5.37","text":"is an argument of another word that you&#39;re\nsaying, it&#39;s a dependent of that word."},"708.46":{"start":"708.46","dur":"6.09","text":"So, barking dog, barking is a dependent\nof dog, because it&#39;s of a modifier of it."},"714.55":{"start":"714.55","dur":"5.02","text":"Large barking dog, large is a modifier of\ndog as well, so it&#39;s a dependent of it."},"719.57":{"start":"719.57","dur":"5.745","text":"And dog by the door, so the by the door\nis somehow a dependent of dog."},"725.32":{"start":"725.315","dur":"2.03","text":"And we&#39;re putting\na dependency between words,"},"727.35":{"start":"727.345","dur":"4.33","text":"and we normally indicate those\ndependencies with arrows."},"731.68":{"start":"731.675","dur":"4.91","text":"And so we can draw dependency\nstructures over sentences that say"},"736.59":{"start":"736.585","dur":"2.875","text":"how they&#39;re represented as well."},"739.46":{"start":"739.46","dur":"5.04","text":"And when right in the first class,\nI gave examples of ambiguous sentences."},"744.50":{"start":"744.5","dur":"5.39","text":"A lot of those ambiguous sentences, we\ncan think about in terms of dependencies."},"749.89":{"start":"749.89","dur":"4.36","text":"So do you remember this one,\nscientists study whales from space."},"755.48":{"start":"755.48","dur":"2.84","text":"Well that was an ambiguous headline."},"758.32":{"start":"758.32","dur":"2.57","text":"And well, why is it an ambiguous headline?"},"760.89":{"start":"760.89","dur":"3.74","text":"Well it&#39;s ambiguous because\nthere&#39;s sort of two possibilities."},"764.63":{"start":"764.63","dur":"5.05","text":"So in either case there&#39;s\nthe main verb study."},"769.68":{"start":"769.68","dur":"4.07","text":"And it&#39;s the scientist that&#39;s studying,\nthat&#39;s an argument of study, the subject."},"773.75":{"start":"773.75","dur":"3.72","text":"And it&#39;s the whales that are being\nstudied, so that&#39;s an argument of study."},"777.47":{"start":"777.47","dur":"1.33","text":"That&#39;s the object."},"778.80":{"start":"778.8","dur":"5.33","text":"But the big difference is then,\nwhat are you doing with the from space."},"784.13":{"start":"784.13","dur":"6.5","text":"You saying that it&#39;s modifying study,\nor are you saying it&#39;s modifying whales?"},"790.63":{"start":"790.63","dur":"3.32","text":"And like, if you sort of just\nquickly read the headline"},"793.95":{"start":"793.95","dur":"1.56","text":"It sounds like it&#39;s the bottom one, right?"},"795.51":{"start":"795.51","dur":"2.86","text":"It&#39;s whales from space."},"798.37":{"start":"798.37","dur":"2.092","text":"And that sounds really exciting."},"800.46":{"start":"800.462","dur":"3.828","text":"But [LAUGH] what the article was meant to\nbe about was, really, that they were being"},"804.29":{"start":"804.29","dur":"3.12","text":"able to use satellites to\ntrack the movements of whales."},"807.41":{"start":"807.41","dur":"4.23","text":"And so it&#39;s the first one where the,\nfrom space, is modifying."},"811.64":{"start":"811.64","dur":"1.86","text":"How they&#39;re being studied."},"813.50":{"start":"813.5","dur":"5.66","text":"And so thinking about ambiguities of\nsentences can then be thought about,"},"819.16":{"start":"819.16","dur":"4.77","text":"many of them, in terms of these dependency\nstructures as to what&#39;s modifying what."},"823.93":{"start":"823.93","dur":"3.94","text":"And this is just a really common thing"},"827.87":{"start":"827.87","dur":"4.42","text":"in natural language because these kind\nof questions of what modifies what,"},"832.29":{"start":"832.29","dur":"4.16","text":"really dominate a lot of\nquestions of interpretation."},"836.45":{"start":"836.45","dur":"2.52","text":"So, here&#39;s the kind of sentence"},"838.97":{"start":"838.97","dur":"3.98","text":"you find when you&#39;re reading\nthe Wall Street Journal every morning."},"842.95":{"start":"842.95","dur":"5.405","text":"The board approved its acquisition by\nRoyal Trustco Limited of Toronto for"},"848.36":{"start":"848.355","dur":"2.315","text":"$27 a share at its Monthly meeting."},"850.67":{"start":"850.67","dur":"4.39","text":"And as I&#39;ve hopefully indicated by\nthe square brackets, if you look at"},"855.06":{"start":"855.06","dur":"5.07","text":"the structure of this sentence, it sort\nof starts off as subject, verb, object."},"860.13":{"start":"860.13","dur":"1.76","text":"The board approved its acquisition,"},"861.89":{"start":"861.89","dur":"4.52","text":"and then everything after that is a whole\nsequence of prepositional phrases."},"866.41":{"start":"866.41","dur":"6.71","text":"By Royal Trustco Ltd, of Toronto, for\n$27 a share, at its monthly meeting."},"873.12":{"start":"873.12","dur":"6.74","text":"And well, so then there&#39;s the question of,\nwhat&#39;s everyone modifying?"},"879.86":{"start":"879.86","dur":"5.18","text":"So the acquisition is by\nRoyal Trustco Ltd, so that&#39;s,"},"885.04":{"start":"885.04","dur":"5.85","text":"by Royal Trustco Ltd is modifying\nthe thing that immediately precedes that."},"890.89":{"start":"890.89","dur":"5.49","text":"And of Toronto is modifying the company,\nRoyal Trustco Limited,"},"896.38":{"start":"896.38","dur":"3.78","text":"so that&#39;s modifying the thing that\ncomes immediately preceeding it."},"900.16":{"start":"900.16","dur":"2.47","text":"So you might think this is easy,"},"902.63":{"start":"902.63","dur":"4.08","text":"everything just modifies the thing\nthat&#39;s coming immediately before it."},"906.71":{"start":"906.71","dur":"1.72","text":"But that, then stops being true."},"908.43":{"start":"908.43","dur":"3.42","text":"So, what&#39;s for $27 a share modifying?"},"914.06":{"start":"914.06","dur":"2.06","text":"Yeah so\nthat&#39;s modifying the acquisition so"},"916.12":{"start":"916.12","dur":"3.29","text":"then we&#39;re jumping back\na few candidates and"},"919.41":{"start":"919.41","dur":"5.27","text":"saying is modifying acquisition and\nthen actually at it&#39;s monthly meeting."},"924.68":{"start":"924.68","dur":"5.94","text":"That wasn&#39;t the Toronto the Royal\nTrustco Ltd or the acquisition that that"},"930.62":{"start":"930.62","dur":"5.33","text":"was when the approval was happening so\nthat jumps all the way back up to the top."},"935.95":{"start":"935.95","dur":"4.86","text":"So in general the situation is that if\nyou&#39;ve got some stuff like a verb and"},"940.81":{"start":"940.81","dur":"4.3","text":"a noun phrase, then you start\ngetting these prepositional phrases."},"945.11":{"start":"945.11","dur":"4.68","text":"Well, the prepositional\nphrase can be modifying,"},"949.79":{"start":"949.79","dur":"1.97","text":"either this noun phrase or the verb."},"951.76":{"start":"951.76","dur":"3.65","text":"But then when you get to\nthe second prepositional phrase."},"955.41":{"start":"955.41","dur":"3.36","text":"Well, there was another noun phrase\ninside this prepositional phrase."},"958.77":{"start":"958.77","dur":"0.63","text":"So, now there&#39;s."},"959.40":{"start":"959.4","dur":"0.711","text":"Three choices."},"960.11":{"start":"960.111","dur":"4.433","text":"It can be modifying this noun phrase,\nthat noun phrase or the verb phrase."},"964.54":{"start":"964.544","dur":"1.666","text":"And then we get to another one."},"966.21":{"start":"966.21","dur":"2.24","text":"So it&#39;s now got four choices."},"968.45":{"start":"968.45","dur":"5.685","text":"And you don&#39;t get sort of\na completely free choice,"},"974.14":{"start":"974.135","dur":"2.615","text":"cuz you do get a nesting constraint."},"976.75":{"start":"976.75","dur":"5.28","text":"So once I&#39;ve had for $27 a share\nreferring back to the acquisition,"},"982.03":{"start":"982.03","dur":"3.29","text":"the next prepositional phrase has to,\nin general,"},"985.32":{"start":"985.32","dur":"3.52","text":"refer to either the acquisition or\napproved."},"988.84":{"start":"988.84","dur":"3.063","text":"I say in general because\nthere are exceptions to that."},"991.90":{"start":"991.903","dur":"2.067","text":"And I&#39;ll actually talk about that later."},"993.97":{"start":"993.97","dur":"1.783","text":"But most of the time in English,\nit&#39;s true."},"995.75":{"start":"995.753","dur":"2.952","text":"You have to sort of refer to\nthe same one or further back, so"},"998.71":{"start":"998.705","dur":"2.135","text":"you get a nesting relationship."},"1000.84":{"start":"1000.84","dur":"5.072","text":"But I mean, even if you obey that nesting\nrelationship, the result is that you"},"1005.91":{"start":"1005.912","dur":"5.458","text":"get an exponential number of\nambiguities in a sentence based"},"1011.37":{"start":"1011.37","dur":"4.56","text":"on in the number of prepositional phrases\nyou stick on the end of the sentence."},"1015.93":{"start":"1015.93","dur":"5.13","text":"And so the series of the exponential\nseries you get of these Catalan numbers."},"1021.06":{"start":"1021.06","dur":"2.53","text":"And so Catalan numbers actually show up"},"1023.59":{"start":"1023.59","dur":"3.48","text":"in a lot of places in\ntheoretical computer science."},"1027.07":{"start":"1027.07","dur":"5.51","text":"Because any kind of structure\nthat is somehow sort of similar,"},"1032.58":{"start":"1032.58","dur":"3.22","text":"if you&#39;re putting these constraints in,\nyou get Catalan series."},"1035.80":{"start":"1035.8","dur":"2.12","text":"So, are any of you doing CS228?"},"1040.47":{"start":"1040.47","dur":"0.76","text":"Yeah, so"},"1041.23":{"start":"1041.23","dur":"5.17","text":"another place the Catalan series turns up\nis that when you&#39;ve got a vector graph and"},"1046.40":{"start":"1046.4","dur":"5.64","text":"you&#39;re triangulating it, the number of\nways that you can triangulate your vector"},"1052.04":{"start":"1052.04","dur":"5.52","text":"graph is also giving you Catalan numbers."},"1057.56":{"start":"1057.56","dur":"3.882","text":"Okay, so\nhuman languages get very ambiguous."},"1061.44":{"start":"1061.442","dur":"4.464","text":"And we can hope to describe\nthem on the basis of sort of"},"1065.91":{"start":"1065.906","dur":"2.854","text":"looking at these dependencies."},"1068.76":{"start":"1068.76","dur":"1.95","text":"So that&#39;s important concept One."},"1070.71":{"start":"1070.71","dur":"4.89","text":"The other important concept I wanted to\nintroduce at this point is this idea of"},"1075.60":{"start":"1075.6","dur":"6.28","text":"full linguistics having annotated\ndata in the form of treebanks."},"1081.88":{"start":"1081.88","dur":"3.85","text":"This is probably a little\nbit small to see exactly."},"1085.73":{"start":"1085.73","dur":"3.72","text":"But what this is, is we&#39;ve got sentences."},"1089.45":{"start":"1089.45","dur":"3.56","text":"These are actually sentences\nthat come off Yahoo Answers."},"1094.12":{"start":"1094.12","dur":"5.31","text":"And what&#39;s happened is,\nhuman beings have sat around and"},"1099.43":{"start":"1099.43","dur":"5.56","text":"drawn in the syntactic structures of\nthese sentences as dependency graphs and"},"1104.99":{"start":"1104.99","dur":"3.77","text":"those things we refer to as treebanks."},"1108.76":{"start":"1108.76","dur":"4.45","text":"And so a really interesting thing\nthat&#39;s happened starting around"},"1113.21":{"start":"1113.21","dur":"4.91","text":"1990 is that people have devoted a lot of"},"1118.12":{"start":"1118.12","dur":"4.93","text":"resources to building up these kind\nof annotated treebanks and various"},"1123.05":{"start":"1123.05","dur":"4.1","text":"other kinds of annotated linguistic\nresources that we&#39;ll talk about later."},"1127.15":{"start":"1127.15","dur":"5.823","text":"Now in some sense, from the viewpoint of\nsort of modern machine learning in 2017,"},"1132.97":{"start":"1132.973","dur":"2.476","text":"that&#39;s completely unsurprising,"},"1135.45":{"start":"1135.449","dur":"3.746","text":"because all the time what we do\nis say we want labelled data so"},"1139.20":{"start":"1139.195","dur":"4.988","text":"we can take our supervised classifier and\nchug on it and get good results."},"1144.18":{"start":"1144.183","dur":"4.663","text":"But in many ways, it was kind of\na surprising thing that happened,"},"1148.85":{"start":"1148.846","dur":"4.933","text":"which is sort of different to the whole\nof the rest of history, right?"},"1153.78":{"start":"1153.779","dur":"4.482","text":"Cuz for the whole of the rest of\nthe history, it was back in this space of,"},"1158.26":{"start":"1158.261","dur":"3.972","text":"well, to describe linguistic\nstructure what we should be doing"},"1162.23":{"start":"1162.233","dur":"5.197","text":"is writing grammar rules that describe\nwhat happens in linguistic structure."},"1167.43":{"start":"1167.43","dur":"4.55","text":"Where here, we&#39;re no longer even\nattempting to write grammar rules."},"1171.98":{"start":"1171.98","dur":"1.676","text":"We&#39;re just saying, give us some sentences."},"1173.66":{"start":"1173.656","dur":"3.873","text":"And I&#39;m gonna diagram these sentences and\nshow you what their structure is."},"1177.53":{"start":"1177.529","dur":"4.421","text":"And tomorrow give me a bunch more and\nI&#39;ll diagram them for you as well."},"1181.95":{"start":"1181.95","dur":"4.767","text":"And if you think about it, in a way,\nthat initially seems kind of"},"1186.72":{"start":"1186.717","dur":"4.945","text":"a crazy thing to do, cuz it seems\nlike just putting structures over"},"1191.66":{"start":"1191.662","dur":"4.975","text":"sentences one by one seems really,\nreally inefficient and slow."},"1196.64":{"start":"1196.637","dur":"1.485","text":"Whereas, if you&#39;re writing a grammar,"},"1198.12":{"start":"1198.122","dur":"2.001","text":"you&#39;re writing this thing\nthat generalizes, right?"},"1200.12":{"start":"1200.123","dur":"2.898","text":"The whole point of grammar is that\nyou&#39;re gonna write this one small,"},"1203.02":{"start":"1203.021","dur":"0.746","text":"finite grammar."},"1203.77":{"start":"1203.767","dur":"3.088","text":"And it describes an infinite\nnumber of sentences."},"1206.86":{"start":"1206.855","dur":"3.863","text":"And so surely,\nthat&#39;s a big labor saving effort."},"1210.72":{"start":"1210.718","dur":"5.491","text":"But, slightly surprisingly, but maybe it\nmakes sense in terms of what&#39;s happened"},"1216.21":{"start":"1216.209","dur":"5.256","text":"in machine learning, that it&#39;s just turned\nout to be kind of super successful,"},"1221.47":{"start":"1221.465","dur":"3.685","text":"this building of explicit,\nannotated treebanks."},"1225.15":{"start":"1225.15","dur":"3.55","text":"And it ends up giving us a lot of things."},"1228.70":{"start":"1228.7","dur":"2.83","text":"And I sort of mention a few\nof their advantages here."},"1231.53":{"start":"1231.53","dur":"2.951","text":"First, it gives you\na reusability of labor."},"1234.48":{"start":"1234.481","dur":"4.367","text":"But the problem of human beings\nhandwriting grammars is that they tend to,"},"1238.85":{"start":"1238.848","dur":"4.715","text":"in practice, be almost unreusable,\nbecause everybody does it differently and"},"1243.56":{"start":"1243.563","dur":"2.157","text":"has their idea of the grammar."},"1245.72":{"start":"1245.72","dur":"4.37","text":"And people spend years working on one and\nno one else ever uses it."},"1250.09":{"start":"1250.09","dur":"4.38","text":"Where effectively, these treebanks have\nbeen a really reusable tool that lots of"},"1254.47":{"start":"1254.47","dur":"4.43","text":"people have then built on top of to\nbuild all kinds of natural language"},"1258.90":{"start":"1258.9","dur":"4.19","text":"processing tools, of part of speech\ntaggers and parsers and things like that."},"1263.09":{"start":"1263.09","dur":"4.736","text":"They&#39;ve also turned out to be a really\nuseful resource, actually, for linguists,"},"1267.83":{"start":"1267.826","dur":"4.408","text":"because they give a kind of real languages\nare spoken, complete with syntactic"},"1272.23":{"start":"1272.234","dur":"4.956","text":"analyses that you can do all kinds of\nquantitative linguistics on top of."},"1277.19":{"start":"1277.19","dur":"3.37","text":"It&#39;s genuine data that&#39;s broad\ncoverage when people just work"},"1280.56":{"start":"1280.56","dur":"2.67","text":"with their intuitions as to what\nare the grammar rules of English."},"1283.23":{"start":"1283.23","dur":"2.65","text":"They think of some things but\nnot of other things."},"1285.88":{"start":"1285.88","dur":"3.525","text":"And so this is actually a better way to\nfind out all of the things that actually"},"1289.41":{"start":"1289.405","dur":"0.945","text":"happened."},"1290.35":{"start":"1290.35","dur":"2.79","text":"For anything that&#39;s sort\nof probabilistic or"},"1293.14":{"start":"1293.14","dur":"4.27","text":"machine learning, it gives some sort\nof not only what&#39;s possible, but"},"1297.41":{"start":"1297.41","dur":"3.86","text":"how frequent it is and what other\nthings it tends to co-occur with and"},"1301.27":{"start":"1301.27","dur":"3.22","text":"all that kind of distributional\ninformation that&#39;s super important."},"1304.49":{"start":"1304.49","dur":"4.31","text":"And crucially, crucially, crucially,\nand we&#39;ll use this for assignment two,"},"1308.80":{"start":"1308.8","dur":"5.29","text":"it&#39;s also great because it gives you\na way to evaluate any system that you"},"1314.09":{"start":"1314.09","dur":"5.34","text":"built because this gives us what we treat\nas ground truth, gold standard data."},"1319.43":{"start":"1319.43","dur":"1.75","text":"These are the correct answers."},"1321.18":{"start":"1321.18","dur":"5.57","text":"And then we can evaluate any tool on\nhow good it is at reproducing those."},"1326.75":{"start":"1326.75","dur":"2.83","text":"Okay, so that&#39;s the general advertisement."},"1329.58":{"start":"1329.58","dur":"4.77","text":"And what I wanted to do now is sort of\ngo through a bit more carefully for"},"1334.35":{"start":"1334.35","dur":"5.614","text":"sort of 15 minutes, what are dependency\ngrammars and dependency structure?"},"1339.96":{"start":"1339.964","dur":"1.954","text":"So we&#39;ve sort of got that straight."},"1341.92":{"start":"1341.918","dur":"3.802","text":"I guess I&#39;ve maybe failed to say, yeah."},"1345.72":{"start":"1345.72","dur":"3.642","text":"I mentioned there was this sort of\nconstituency context-free grammar"},"1349.36":{"start":"1349.362","dur":"2.618","text":"viewpoint and\nthe dependency grammar viewpoint."},"1353.18":{"start":"1353.18","dur":"2.908","text":"Today, it&#39;s gonna be all dependencies."},"1356.09":{"start":"1356.088","dur":"3.322","text":"And what we&#39;re doing for\nassignment two is all dependencies."},"1359.41":{"start":"1359.41","dur":"3.8","text":"We will get back to some notions of\nconstituency and phrase structure."},"1363.21":{"start":"1363.21","dur":"4.98","text":"You&#39;ll see those coming back in\nlater classes in a few weeks&#39; time."},"1368.19":{"start":"1368.19","dur":"2.54","text":"But this is what we&#39;re\ngoing to be doing today."},"1370.73":{"start":"1370.73","dur":"2.551","text":"And that&#39;s not a completely random choice."},"1373.28":{"start":"1373.281","dur":"4.474","text":"It&#39;s turned out that, unlike what&#39;s\nhappened in linguistics in most of"},"1377.76":{"start":"1377.755","dur":"4.619","text":"the last 50 years, in the last decade\nin natural language processing,"},"1382.37":{"start":"1382.374","dur":"3.969","text":"it&#39;s essentially been swept by\nthe use of dependency grammars,"},"1386.34":{"start":"1386.343","dur":"4.044","text":"that people have found dependency\ngrammars just a really suitable"},"1390.39":{"start":"1390.387","dur":"4.546","text":"framework on which to build semantic\nrepresentations to get out the kind of"},"1394.93":{"start":"1394.933","dur":"3.927","text":"understanding of language that\nthey&#39;d like to get out easily."},"1398.86":{"start":"1398.86","dur":"2.293","text":"They enable the building of very fast,"},"1401.15":{"start":"1401.153","dur":"3.237","text":"efficient parsers,\nas I&#39;ll explain later today."},"1404.39":{"start":"1404.39","dur":"1.926","text":"And so in the last sort of ten years,"},"1406.32":{"start":"1406.316","dur":"4.315","text":"you&#39;ve just sort of seen this huge sea\nchange in natural language processing."},"1410.63":{"start":"1410.631","dur":"4.519","text":"Whereas, if you pick up a conference\nvolume around the 1990s, it was basically"},"1415.15":{"start":"1415.15","dur":"4.153","text":"all phrase structure grammars and one or\ntwo papers on dependency grammars."},"1419.30":{"start":"1419.303","dur":"1.888","text":"And if you pick up a volume now,"},"1421.19":{"start":"1421.191","dur":"5.38","text":"what you&#39;ll find out is that of the papers\nthey&#39;re using syntactic representations,"},"1426.57":{"start":"1426.571","dur":"3.868","text":"kind of 80% of them are using\ndependency representations."},"1430.44":{"start":"1430.439","dur":"1.704","text":"Okay, yes."},"1432.14":{"start":"1432.143","dur":"1.547","text":"&gt;&gt; What&#39;s that,\na phrase structure grammar?"},"1433.69":{"start":"1433.69","dur":"3.22","text":"Phrase structure, what&#39;s the phrase\nstructure grammar, that&#39;s exactly the same"},"1436.91":{"start":"1436.91","dur":"2.995","text":"as the context-free grammar\nwhen a linguist is speaking."},"1439.91":{"start":"1439.905","dur":"5.038","text":"[LAUGH] Yes,\nformerly a context-free grammar."},"1444.94":{"start":"1444.943","dur":"4.277","text":"Okay, so\nwhat does a dependency syntax say?"},"1449.22":{"start":"1449.22","dur":"5.22","text":"So the idea of dependency syntax\nis to say that the sort of model"},"1454.44":{"start":"1454.44","dur":"4.96","text":"of syntax is we have relationships\nbetween lexical items,"},"1459.40":{"start":"1459.4","dur":"3.75","text":"words, and only between lexical items."},"1463.15":{"start":"1463.15","dur":"5.28","text":"They&#39;re binary, asymmetric relations,\nwhich means we draw arrows."},"1468.43":{"start":"1468.43","dur":"3.02","text":"And we call those arrows dependencies."},"1471.45":{"start":"1471.45","dur":"5.12","text":"So the whole, there is a dependency\nanalysis of bills on ports and"},"1476.57":{"start":"1476.57","dur":"4.95","text":"immigration were submitted by\nSenator Brownback, Republican of Kansas."},"1481.52":{"start":"1481.52","dur":"6.309","text":"Okay, so that&#39;s a start,\nnormally hen we do dependency parsing,"},"1487.83":{"start":"1487.829","dur":"3.113","text":"we do a little bit more than that."},"1490.94":{"start":"1490.942","dur":"5.59","text":"So typically we type the dependencies\nby giving them a name for"},"1496.53":{"start":"1496.532","dur":"3.518","text":"some grammatical relationship."},"1500.05":{"start":"1500.05","dur":"5.32","text":"So I&#39;m calling this the subject, and\nit&#39;s actually a passive subject."},"1505.37":{"start":"1505.37","dur":"3.62","text":"And then this is an auxiliary modifier,\nand"},"1508.99":{"start":"1508.99","dur":"5.39","text":"Republican of Kansas is an appositional\nphrase that&#39;s coming off of Brownback."},"1514.38":{"start":"1514.38","dur":"4.57","text":"And so we use this kind of\ntyped dependency grammars."},"1518.95":{"start":"1518.95","dur":"4.43","text":"And interestingly,\nI&#39;m not going to go through it, but"},"1523.38":{"start":"1523.38","dur":"5.52","text":"there&#39;s sort of some interesting\nmath that if you just have this,"},"1528.90":{"start":"1528.9","dur":"2.178","text":"although it&#39;s notationally very different,"},"1531.08":{"start":"1531.078","dur":"6.067","text":"from context-free grammar,\nthese are actually equivalent"},"1537.15":{"start":"1537.145","dur":"4.825","text":"to a restricted kind of context-free\ngrammar with one addition."},"1541.97":{"start":"1541.97","dur":"4.505","text":"But things become sort of a bit more\ndifferent once you put in a typing"},"1546.48":{"start":"1546.475","dur":"4.865","text":"of the dependency labels, where I wont\ngo into that in great detail, right."},"1551.34":{"start":"1551.34","dur":"3.83","text":"So a substantive theory\nof dependency grammar for"},"1555.17":{"start":"1555.17","dur":"4.24","text":"a language,\nwe&#39;re then having to make some decisions."},"1559.41":{"start":"1559.41","dur":"4.2","text":"So what we&#39;re gonna do is when we,\nwe&#39;re gonna draw these arrows"},"1563.61":{"start":"1563.61","dur":"3.94","text":"between two things, and\nI&#39;ll just mention a bit more terminology."},"1567.55":{"start":"1567.55","dur":"6.71","text":"So we have an arrow and its got what we\ncalled the tail end of the arrow, I guess."},"1574.26":{"start":"1574.26","dur":"2.55","text":"And the word up here is sort of the head."},"1576.81":{"start":"1576.81","dur":"6.53","text":"So bills is an argument of submitted, were\nis an auxiliary modifier of submitted."},"1583.34":{"start":"1583.34","dur":"4.45","text":"And so this word here is normally referred\nto as the head, or the governor, or"},"1587.79":{"start":"1587.79","dur":"3.23","text":"the superior, or\nsometimes even the regent."},"1591.02":{"start":"1591.02","dur":"3.18","text":"I&#39;ll normally call it the head."},"1594.20":{"start":"1594.2","dur":"3.1","text":"And then the word at\nthe other end of the arrow,"},"1597.30":{"start":"1597.3","dur":"3.43","text":"the pointy bit,\nI&#39;ll refer to as the dependent,"},"1600.73":{"start":"1600.73","dur":"5.476","text":"but other words that you can sometimes\nsee are modifier, inferior, subordinate."},"1606.21":{"start":"1606.206","dur":"4.164","text":"Some people who do dependency grammar\nreally get into these classist notions"},"1610.37":{"start":"1610.37","dur":"3.83","text":"of superiors and inferiors, but\nI&#39;ll go with heads and dependents."},"1615.79":{"start":"1615.79","dur":"3.244","text":"Okay, so the idea is you\nhave a head of a clause and"},"1619.03":{"start":"1619.034","dur":"2.621","text":"then the arguments of the dependence."},"1621.66":{"start":"1621.655","dur":"3.697","text":"And then when you have a phrase like,"},"1625.35":{"start":"1625.352","dur":"4.858","text":"by Senator Brownback, Republican of Texas."},"1630.21":{"start":"1630.21","dur":"3.85","text":"It&#39;s got a head which is here\nbeing taken as Brownback and"},"1634.06":{"start":"1634.06","dur":"2.16","text":"then it&#39;s got words beneath it."},"1636.22":{"start":"1636.22","dur":"5.15","text":"And so one of the main parts of\ndependency grammars at the end of the day"},"1641.37":{"start":"1641.37","dur":"4.47","text":"is you have to make decisions\nas to which words are heads and"},"1645.84":{"start":"1645.84","dur":"5.4","text":"which words are then the dependents of\nthe heads of any particular structure."},"1651.24":{"start":"1651.24","dur":"4.05","text":"So in these diagrams I&#39;m showing you here,\nand"},"1655.29":{"start":"1655.29","dur":"4.67","text":"the ones I showed you back a few pages,\nwhat I&#39;m actually showing you"},"1659.96":{"start":"1659.96","dur":"3.59","text":"here is analysis according\nto universal dependencies."},"1663.55":{"start":"1663.55","dur":"3.93","text":"So universal dependencies is\na new tree banking effort"},"1667.48":{"start":"1667.48","dur":"2.65","text":"which I&#39;ve actually been\nvery strongly involved in."},"1670.13":{"start":"1670.13","dur":"2.48","text":"That sort of started\na couple of years ago and"},"1672.61":{"start":"1672.61","dur":"3.05","text":"there are pointers in both\nearlier in the slides and"},"1675.66":{"start":"1675.66","dur":"4.29","text":"on the website if you wanna go off and\nlearn a lot about universal dependencies."},"1679.95":{"start":"1679.95","dur":"2","text":"I mean it&#39;s sort of\nan ambitious attempt to try and"},"1681.95":{"start":"1681.95","dur":"4.768","text":"have a common dependency representation\nthat works over a ton of languages."},"1686.72":{"start":"1686.718","dur":"1.402","text":"I could prattle on about it for"},"1688.12":{"start":"1688.12","dur":"4.96","text":"ages, and if by some off chance there&#39;s\ntime at the end of the class I could."},"1693.08":{"start":"1693.08","dur":"4.46","text":"But probably there won&#39;t be so I won&#39;t\nactually tell you a lot about that now."},"1697.54":{"start":"1697.54","dur":"5.16","text":"But I will just mention one thing that\nprobably you&#39;ll notice very quickly."},"1702.70":{"start":"1702.7","dur":"3.48","text":"And we&#39;re also going to be using this\nrepresentation in the assignment that&#39;s"},"1706.18":{"start":"1706.18","dur":"5.94","text":"being given out today,\nthe analysis of universal dependencies"},"1712.12":{"start":"1712.12","dur":"5.23","text":"treats prepositions sort of differently\nto what you might have seen else where."},"1717.35":{"start":"1717.35","dur":"4.65","text":"If you&#39;ve seen any, many accounts of\nEnglish grammar, or heard references in"},"1722.00":{"start":"1722","dur":"4.38","text":"some English classroom,\nto have prepositions, having objects."},"1726.38":{"start":"1726.38","dur":"6.39","text":"In universal dependencies,\nprepositions don&#39;t have any dependents."},"1732.77":{"start":"1732.77","dur":"3.38","text":"Prepositions are treated kind\nof like they were case markers,"},"1736.15":{"start":"1736.15","dur":"3.35","text":"if you know any language like, German, or"},"1739.50":{"start":"1739.5","dur":"5.06","text":"Latin, or Hindi, or\nsomething that has cases."},"1744.56":{"start":"1744.56","dur":"5.02","text":"So that the by is sort of treated as\nif it were a case marker of Brownback."},"1749.58":{"start":"1749.58","dur":"4.19","text":"So this sort of a bleak modifier\nof by Senator Brownback."},"1753.77":{"start":"1753.77","dur":"3.51","text":"And so it&#39;s actually treating\nBrownback here as the head"},"1757.28":{"start":"1757.28","dur":"4.02","text":"with the preposition as sort of like\na case marking dependent of by."},"1761.30":{"start":"1761.3","dur":"4.22","text":"And that was sort of done to get more\nparallelism across different languages"},"1765.52":{"start":"1765.52","dur":"1.04","text":"of the world."},"1766.56":{"start":"1766.56","dur":"2.77","text":"But I&#39;ll just mention that."},"1769.33":{"start":"1769.33","dur":"6.03","text":"Other properties of old dependencies,\nnormally dependencies form a tree."},"1775.36":{"start":"1775.36","dur":"2.528","text":"So there are formal properties\nthat goes along with that."},"1777.89":{"start":"1777.888","dur":"7.782","text":"That means that they&#39;ve got a single-head,\nthey&#39;re acyclic, and they&#39;re connected."},"1785.67":{"start":"1785.67","dur":"3.894","text":"So there is a sort of graph\ntheoretic properties."},"1789.56":{"start":"1789.564","dur":"2.636","text":"Yeah, I sort of mentioned that really"},"1792.20":{"start":"1792.2","dur":"3.19","text":"dependencies have dominated\nmost of the world."},"1795.39":{"start":"1795.39","dur":"2.64","text":"So just very quickly on that."},"1798.03":{"start":"1798.03","dur":"5.27","text":"The famous first linguist was Panini,"},"1803.30":{"start":"1803.3","dur":"5.53","text":"who wrote his Grammar of Sanskrit\naround the fifth century BCE."},"1808.83":{"start":"1808.83","dur":"4.76","text":"Really most of the work that Panini\ndid was kind of on sound systems and"},"1813.59":{"start":"1813.59","dur":"2.55","text":"make ups of words,\nphonology, and morphology,"},"1816.14":{"start":"1816.14","dur":"4.51","text":"when we mentioned linguistic\nlevels in the first class."},"1820.65":{"start":"1820.65","dur":"4.06","text":"And he only did a little bit of\nwork on the structure of sentences."},"1824.71":{"start":"1824.71","dur":"1.942","text":"But the notation that he used for"},"1826.65":{"start":"1826.652","dur":"4.779","text":"structure of sentences was essentially\na dependency grammar of having word"},"1831.43":{"start":"1831.431","dur":"3","text":"relationships being\nmarked as dependencies."},"1837.33":{"start":"1837.331","dur":"0.69","text":"Question?"},"1872.97":{"start":"1872.972","dur":"4.662","text":"Yeah, so the question is,\nwell compare CFGs and PCFGs and"},"1877.63":{"start":"1877.634","dur":"4.855","text":"do they, dependency grammars\nlook strongly lexicalized,"},"1882.49":{"start":"1882.489","dur":"5.886","text":"they&#39;re between words and\ndoes that makes it harder to generalize."},"1888.38":{"start":"1888.375","dur":"2.67","text":"I honestly feel I just\ncan&#39;t do justice to that"},"1891.05":{"start":"1891.045","dur":"2.855","text":"question right now if I&#39;m gonna get\nthrough the rest of the lecture."},"1893.90":{"start":"1893.9","dur":"2.62","text":"But I will make two comments, so I mean,"},"1896.52":{"start":"1896.52","dur":"4.576","text":"there&#39;s certainly the natural way\nto think of dependency grammars,"},"1901.10":{"start":"1901.096","dur":"4.924","text":"they&#39;re strongly lexicalized, you&#39;re\ndrawing relationships between words."},"1906.02":{"start":"1906.02","dur":"3.43","text":"Whereas the simplest way of thinking of\ncontext-free grammars is you&#39;ve got these"},"1909.45":{"start":"1909.45","dur":"2.23","text":"rules in terms of categories like."},"1911.68":{"start":"1911.68","dur":"4.54","text":"Noun phrase goes to determiner noun,\noptional prepositional phrase."},"1916.22":{"start":"1916.22","dur":"4.14","text":"And so, that is a big difference."},"1920.36":{"start":"1920.36","dur":"2.7","text":"But it kind of goes both ways."},"1923.06":{"start":"1923.06","dur":"4.787","text":"So, normally, when actually, natural\nlanguage processing people wanna work with"},"1927.85":{"start":"1927.847","dur":"3.525","text":"context-free grammars,\nthey frequently lexicalize them so"},"1931.37":{"start":"1931.372","dur":"4.154","text":"they can do more precise probabilistic\nprediction, and vice versa."},"1935.53":{"start":"1935.526","dur":"2.714","text":"If you want to do generalization and\ndependency grammar,"},"1938.24":{"start":"1938.24","dur":"3.22","text":"you can still use at least\nnotions of parts of speech"},"1941.46":{"start":"1941.46","dur":"4.112","text":"to give you a level of generalization\nas more like categories."},"1945.57":{"start":"1945.572","dur":"4.158","text":"But nevertheless, the kind of natural\nways of sort of turning them into"},"1949.73":{"start":"1949.73","dur":"3.64","text":"probabilities, and machine learning\nmodels are quite different."},"1953.37":{"start":"1953.37","dur":"2.79","text":"Though, on the other hand,\nthere&#39;s sort of some results, or"},"1956.16":{"start":"1956.16","dur":"1.47","text":"sort of relationships between them."},"1957.63":{"start":"1957.63","dur":"2.74","text":"But I would think I&#39;d better\nnot go on a huge digression."},"1960.37":{"start":"1960.37","dur":"1.05","text":"But you have another question?"},"1964.88":{"start":"1964.877","dur":"4.35","text":"That means to rather than just have\ncategories like noun phrase to have"},"1969.23":{"start":"1969.227","dur":"4.883","text":"categories like a noun phrase headed\nby dog, and so it&#39;s lexicalized."},"1974.11":{"start":"1974.11","dur":"4.574","text":"Let&#39;s leave this for\nthe moment though, please, okay."},"1978.68":{"start":"1978.684","dur":"3.546","text":"[LAUGH]\nOkay, so"},"1982.23":{"start":"1982.23","dur":"3.16","text":"that&#39;s Panini, and\nthere&#39;s a whole big history, right?"},"1985.39":{"start":"1985.39","dur":"4.365","text":"So, essentially for\nLatin grammarians, what they did for"},"1989.76":{"start":"1989.755","dur":"3.79","text":"the syntax of Latin,\nagain, not very developed."},"1993.55":{"start":"1993.545","dur":"1.549","text":"They mainly did morphology, but"},"1995.09":{"start":"1995.094","dur":"3.366","text":"it was essentially a dependency\nkind of analysis that was given."},"1998.46":{"start":"1998.46","dur":"4.645","text":"There was sort of a flowering of Arabic\ngrammarians in the first millennium, and"},"2003.11":{"start":"2003.105","dur":"2.607","text":"they essentially had a dependency grammar."},"2005.71":{"start":"2005.712","dur":"7.238","text":"I mean, by contrast, I mean, really kind\nof context free grammars and constituency"},"2012.95":{"start":"2012.95","dur":"6.35","text":"grammar only got invented almost in\nthe second half of the 20th century."},"2019.30":{"start":"2019.3","dur":"2.34","text":"I mean, it wasn&#39;t actually Chomsky\nthat originally invented them,"},"2021.64":{"start":"2021.64","dur":"4.62","text":"there was a little bit of earlier work in\nBritain, but only kind of a decade before."},"2027.91":{"start":"2027.91","dur":"5.18","text":"So, there was this French\nlinguist Lucien Tesniere,"},"2033.09":{"start":"2033.09","dur":"4.72","text":"he is often referred to as the father\nof modern dependency grammar,"},"2037.81":{"start":"2037.81","dur":"1.51","text":"he&#39;s got a book from 1959."},"2039.32":{"start":"2039.32","dur":"6.75","text":"Dependency grammars have been very popular\nand more sorta free word order languages,"},"2046.07":{"start":"2046.07","dur":"4.35","text":"cuz notions, sort of like context-free\ngrammars work really well for"},"2050.42":{"start":"2050.42","dur":"3.49","text":"languages like English that\nhave very fixed word order, but"},"2053.91":{"start":"2053.91","dur":"5.39","text":"a lot of other languages of the world\nhave much freer word order."},"2059.30":{"start":"2059.3","dur":"4.81","text":"And that&#39;s often more naturally\ndescribed with dependency grammars."},"2064.11":{"start":"2064.11","dur":"4.84","text":"Interestingly, one of the very first\nnatural language parsers developed"},"2068.95":{"start":"2068.95","dur":"4.38","text":"in the US was also a dependency parser."},"2073.33":{"start":"2073.33","dur":"4.16","text":"So, David Hays was one of the first\nUS computational linguists."},"2077.49":{"start":"2077.49","dur":"4.621","text":"And one of the founders of the Association\nfor Computational Linguistics which is our"},"2082.11":{"start":"2082.111","dur":"4.448","text":"main kind of academic association where\nwe publish our conference papers, etc."},"2086.56":{"start":"2086.559","dur":"7.541","text":"And he actually built in 1962,\na dependency parser for English."},"2095.41":{"start":"2095.41","dur":"2.64","text":"Okay, so\na lot of history of dependency grammar."},"2098.05":{"start":"2098.05","dur":"4.28","text":"So, couple of other fine points\nto note about the notation."},"2103.60":{"start":"2103.6","dur":"4.005","text":"People aren&#39;t always consistent in\nwhich way they draw the arrows."},"2107.61":{"start":"2107.605","dur":"4.81","text":"I&#39;m always gonna draw the arrows, so\nthey point, go from a head to a dependent,"},"2112.42":{"start":"2112.415","dur":"2.19","text":"which is the direction\nwhich Tesniere drew them."},"2114.61":{"start":"2114.605","dur":"3.49","text":"But there are some other people who\ndraw the arrows the other way around."},"2118.10":{"start":"2118.095","dur":"2.272","text":"So, they point from\nthe dependent to the head."},"2120.37":{"start":"2120.367","dur":"3.32","text":"And so, you just need to look and\nsee what people are doing."},"2123.69":{"start":"2123.687","dur":"3.92","text":"The other thing that&#39;s very commonly done,\nand we will do in our parses,"},"2127.61":{"start":"2127.607","dur":"4.61","text":"is you stick this pseudo-word,\nwhich might be called ROOT or"},"2132.22":{"start":"2132.217","dur":"5.65","text":"WALL, or some other name like that,\nat the start of the sentence."},"2137.87":{"start":"2137.867","dur":"4.873","text":"And that kind of makes the math and\nformalism easy,"},"2142.74":{"start":"2142.74","dur":"5.53","text":"because, then, every sentence starts with\nroot and something is a dependent of root."},"2148.27":{"start":"2148.27","dur":"4.83","text":"Or, turned around the other way, if you\nthink of what parsing a dependency grammar"},"2153.10":{"start":"2153.1","dur":"3.36","text":"means is for every word in\nthe sentence you&#39;re going to say,"},"2156.46":{"start":"2156.46","dur":"3.3","text":"what is it a dependent of,\nbecause if you do that you&#39;re done."},"2159.76":{"start":"2159.76","dur":"2.59","text":"You&#39;ve got the dependency\nstructure of the sentence."},"2162.35":{"start":"2162.35","dur":"4.89","text":"And what you&#39;re gonna want to say is,\nwell, it&#39;s either gonna be a dependent of"},"2167.24":{"start":"2167.24","dur":"3.12","text":"some other word in the sentence,\nor it&#39;s gonna be a dependent of"},"2170.36":{"start":"2170.36","dur":"4.03","text":"the pseudo-word ROOT, which is meaning\nit&#39;s the head of the entire sentence."},"2176.83":{"start":"2176.83","dur":"5.34","text":"And so, we&#39;ll go through some\nspecifics of dependency parsing"},"2182.17":{"start":"2182.17","dur":"1.84","text":"the second half of the class."},"2184.01":{"start":"2184.01","dur":"3.44","text":"But the kind of thing that you\nshould think about is well,"},"2187.45":{"start":"2187.45","dur":"5.97","text":"how could we decide which\nwords are dependent on what?"},"2193.42":{"start":"2193.42","dur":"4.82","text":"And there are certain various information\nsources that we can think about."},"2198.24":{"start":"2198.24","dur":"4.14","text":"So yeah, it&#39;s sort of totally natural with\nthe dependency representation to just"},"2202.38":{"start":"2202.38","dur":"2.35","text":"think about word relationships."},"2204.73":{"start":"2204.73","dur":"3.331","text":"And that&#39;s great, cuz that&#39;ll fit super\nwell with what we&#39;ve done already in"},"2208.06":{"start":"2208.061","dur":"1.602","text":"distributed word representations."},"2209.66":{"start":"2209.663","dur":"4.086","text":"So actually,\ndoing things this way just fits well"},"2213.75":{"start":"2213.749","dur":"4.221","text":"with a couple of tools we\nalready know how to use."},"2217.97":{"start":"2217.97","dur":"2.953","text":"We&#39;ll want to say well,\ndiscussion of issues,"},"2220.92":{"start":"2220.923","dur":"3.465","text":"is that a reasonable attachment\nas lexical dependency?"},"2224.39":{"start":"2224.388","dur":"3.25","text":"And that&#39;s a lot of the information\nthat we&#39;ll actually use, but"},"2227.64":{"start":"2227.638","dur":"3.852","text":"there&#39;s some other sources of\ninformation that we&#39;d also like to use."},"2231.49":{"start":"2231.49","dur":"5.17","text":"Dependency distance, so sometimes,\nthere are dependency relationships and"},"2236.66":{"start":"2236.66","dur":"4.48","text":"sentences between words that is 20 words\napart when you got some big long sentence,"},"2241.14":{"start":"2241.14","dur":"2.79","text":"and you&#39;re referring that back\nto some previous clause, but"},"2243.93":{"start":"2243.93","dur":"0.94","text":"it&#39;s kind of uncommon."},"2244.87":{"start":"2244.87","dur":"4.05","text":"Most of dependencies are pretty short\ndistance, so you want to prefer that."},"2250.68":{"start":"2250.68","dur":"5.14","text":"Many dependencies don&#39;t, sort of,\nspan certain kinds of things."},"2255.82":{"start":"2255.82","dur":"4.84","text":"So, if you have the kind of dependencies\nthat occur inside noun phrases,"},"2260.66":{"start":"2260.66","dur":"3.81","text":"like adjective modifier,\nthey&#39;re not gonna cross over a verb."},"2264.47":{"start":"2264.47","dur":"5.05","text":"It&#39;s unusual for many kinds of\ndependencies to cross over a punctuation,"},"2269.52":{"start":"2269.52","dur":"3.53","text":"so it&#39;s very rare to have a punctuation\nbetween a verb and a subject and"},"2273.05":{"start":"2273.05","dur":"0.96","text":"things like that."},"2274.01":{"start":"2274.01","dur":"3.07","text":"So, looking at the intervening\nmaterial gives you some clues."},"2277.08":{"start":"2277.08","dur":"5.92","text":"And the final source of information is\nsort of thinking about heads, and thinking"},"2283.00":{"start":"2283","dur":"6.09","text":"how likely they are to have to dependence\nin what number, and on what sides."},"2289.09":{"start":"2289.09","dur":"4.62","text":"So, the kind of information there is,\nright, a word like the,"},"2293.71":{"start":"2293.71","dur":"4.36","text":"is basically not likely to have\nany dependents at all, anywhere."},"2298.07":{"start":"2298.07","dur":"2.53","text":"So, you&#39;d be surprised if it did."},"2301.65":{"start":"2301.65","dur":"5.09","text":"Words like nouns can have dependents, and\nthey can have quite a few dependents,"},"2306.74":{"start":"2306.74","dur":"4.47","text":"but they&#39;re likely to have some kinds like\ndeterminers and adjectives on the left,"},"2311.21":{"start":"2311.21","dur":"3.71","text":"other kinds like prepositional\nphrases on the right"},"2314.92":{"start":"2314.92","dur":"1.97","text":"verbs tend to have a lot of dependence."},"2316.89":{"start":"2316.89","dur":"3.39","text":"So, different kinds of words have\ndifferent kinds of patterns of dependence,"},"2320.28":{"start":"2320.28","dur":"3.13","text":"and so there&#39;s some information\nthere we could hope to gather."},"2325.34":{"start":"2325.338","dur":"5.225","text":"Okay, yeah,\nI guess I&#39;ve already said the first point."},"2330.56":{"start":"2330.563","dur":"2.317","text":"How do we do dependency parsing?"},"2332.88":{"start":"2332.88","dur":"3.79","text":"In principle, it&#39;s kind of really easy."},"2336.67":{"start":"2336.67","dur":"5.19","text":"So, we&#39;re just gonna take every\nword in the sentence and say,"},"2341.86":{"start":"2341.86","dur":"5.78","text":"make a decision as to what word or\nroot this word is a dependent of."},"2347.64":{"start":"2347.64","dur":"2.72","text":"And we do that with a few constraints."},"2350.36":{"start":"2350.36","dur":"5.68","text":"So normally, we require that only\none word can be a dependent of root,"},"2356.04":{"start":"2356.04","dur":"2.42","text":"and we&#39;re not going to allow any cycles."},"2359.62":{"start":"2359.62","dur":"3.51","text":"And if we do both of those things,"},"2363.13":{"start":"2363.13","dur":"4.72","text":"we&#39;re guaranteeing that we make\nthe dependencies of a tree."},"2367.85":{"start":"2367.85","dur":"4.252","text":"And normally,\nwe want to make out dependencies a tree."},"2372.10":{"start":"2372.102","dur":"5.453","text":"And there&#39;s one other property\nI then wanted to mention,"},"2377.56":{"start":"2377.555","dur":"4.894","text":"that if you draw your\ndependencies as I have here, so"},"2382.45":{"start":"2382.449","dur":"5.601","text":"all the dependencies been drawn\nas loops above the words."},"2388.05":{"start":"2388.05","dur":"4.02","text":"It&#39;s different if you&#39;re allowed to\nput some of them below the words."},"2392.07":{"start":"2392.07","dur":"4.254","text":"There&#39;s then a question as to\nwhether you can draw them like this."},"2396.32":{"start":"2396.324","dur":"4.478","text":"So that they have that kind of nice,\nlittle nesting structure, but"},"2400.80":{"start":"2400.802","dur":"2.054","text":"none of them cross each other."},"2402.86":{"start":"2402.856","dur":"5.075","text":"Or whether, like these two that I&#39;ve\ngot here, where they necessarily"},"2407.93":{"start":"2407.931","dur":"4.757","text":"cross each other, and\nI couldn&#39;t avoid them crossing each other."},"2412.69":{"start":"2412.688","dur":"5.199","text":"And what you&#39;ll find is in most languages,\ncertainly English,"},"2417.89":{"start":"2417.887","dur":"4.914","text":"the vast majority of dependency\nrelationships have a nesting"},"2422.80":{"start":"2422.801","dur":"3.819","text":"structure relative to the linear order."},"2426.62":{"start":"2426.62","dur":"3.07","text":"And if a dependency tree is fully nesting,"},"2429.69":{"start":"2429.69","dur":"2.58","text":"it&#39;s referred to as\na projective dependency tree,"},"2432.27":{"start":"2432.27","dur":"5.45","text":"that you can lay it out in this plane,\nand have sort of a nesting relationship."},"2437.72":{"start":"2437.72","dur":"3.45","text":"But there are few structures"},"2441.17":{"start":"2441.17","dur":"3.86","text":"in English where you&#39;d get things\nthat aren&#39;t nested and yet crossing."},"2445.03":{"start":"2445.03","dur":"2.7","text":"And this sentence is\na natural example of one."},"2447.73":{"start":"2447.73","dur":"2.872","text":"So I&#39;ll give a talk\ntomorrow on bootstrapping."},"2450.60":{"start":"2450.602","dur":"4.564","text":"So something that you can do with\nnoun modifiers, especially if they&#39;re"},"2455.17":{"start":"2455.166","dur":"4.491","text":"kind of long words like bootstrapping or\ntechniques of bootstrapping,"},"2459.66":{"start":"2459.657","dur":"4.211","text":"is you can sort of move them towards\nthe end of the sentence, right."},"2463.87":{"start":"2463.868","dur":"3.562","text":"I could have said I&#39;ll give\na talk on bootstrapping tomorrow."},"2467.43":{"start":"2467.43","dur":"4.7","text":"But it sounds pretty natural to say, I&#39;ll\ngive a talk tomorrow on bootstrapping."},"2472.13":{"start":"2472.13","dur":"3.44","text":"But this on bootstrapping is\nstill modifying the talk."},"2475.57":{"start":"2475.57","dur":"4.64","text":"And so that&#39;s referred to by\nlinguists as right extraposition."},"2480.21":{"start":"2480.21","dur":"3.58","text":"And so when you get that kind of\nrightward movement of phrases,"},"2483.79":{"start":"2483.79","dur":"3.08","text":"you then end up with these crossing lines."},"2486.87":{"start":"2486.87","dur":"4.948","text":"And that gives you what&#39;s referred to\nas a non-projective dependency tree."},"2491.82":{"start":"2491.818","dur":"3.982","text":"So, importantly,\nit is still a tree if you sort of"},"2495.80":{"start":"2495.8","dur":"3.65","text":"ignore the constraints of linear order,\nand you&#39;re just drawing it out."},"2499.45":{"start":"2499.45","dur":"4.33","text":"There&#39;s a graph in theoretical computer\nscience, right, it&#39;s still a tree."},"2503.78":{"start":"2503.78","dur":"4.71","text":"It&#39;s only when you consider this extra\nthing of the linear order of the words,"},"2508.49":{"start":"2508.49","dur":"2.61","text":"that you&#39;re then forced\nto have the lines across."},"2511.10":{"start":"2511.1","dur":"3.08","text":"And so that property which you don&#39;t\nactually normally see mentioned in"},"2514.18":{"start":"2514.18","dur":"3.23","text":"theoretical computer science\ndiscussions of graphs"},"2517.41":{"start":"2517.41","dur":"3.42","text":"is then this property that&#39;s\nreferred to projectivity."},"2520.83":{"start":"2520.83","dur":"7.336","text":"Yes.\n&gt;&gt; [INAUDIBLE]"},"2528.17":{"start":"2528.166","dur":"2.414","text":"&gt;&gt; So the questions is is it possible to"},"2530.58":{"start":"2530.58","dur":"3.945","text":"recover the order of the words\nfrom a dependency tree."},"2534.53":{"start":"2534.525","dur":"5.82","text":"So given how I&#39;ve defined dependency\ntrees, the strict answer is no."},"2540.35":{"start":"2540.345","dur":"2.202","text":"They aren&#39;t giving you the order at all."},"2542.55":{"start":"2542.547","dur":"5.299","text":"Now, in practice, people write down\nthe words of a sentence in order and have"},"2547.85":{"start":"2547.846","dur":"5.97","text":"these crossing brackets, right, crossing\narrows when they&#39;re non-projective."},"2553.82":{"start":"2553.816","dur":"3.391","text":"And, of course, it would be a\nstraightforward thing to index the words."},"2557.21":{"start":"2557.207","dur":"4.343","text":"And, obviously, it&#39;s a real thing about\nlanguages that they have linear order."},"2561.55":{"start":"2561.55","dur":"1.39","text":"One can&#39;t deny it."},"2562.94":{"start":"2562.94","dur":"4.21","text":"But as I&#39;ve defined dependency structures,\nyeah,"},"2567.15":{"start":"2567.15","dur":"2.11","text":"you can&#39;t actually recover\nthe order of words from them."},"2570.97":{"start":"2570.97","dur":"5.5","text":"Okay, one more slide before\nwe get to the intermission."},"2576.47":{"start":"2576.47","dur":"2.12","text":"Yeah, so in the second half of the class,"},"2578.59":{"start":"2578.59","dur":"5.77","text":"I&#39;m gonna tell you about\na method of dependency parsing."},"2584.36":{"start":"2584.36","dur":"4.51","text":"I just wanted to say, very quickly,\nthere are a whole bunch"},"2588.87":{"start":"2588.87","dur":"3.44","text":"of ways that people have gone\nabout doing dependency parsing."},"2592.31":{"start":"2592.31","dur":"5.08","text":"So one very prominent way of doing\ndependency parsing is using dynamic"},"2597.39":{"start":"2597.39","dur":"1.06","text":"programming methods,"},"2598.45":{"start":"2598.45","dur":"4.22","text":"which is normally what people have\nused for constituency grammars."},"2602.67":{"start":"2602.67","dur":"4.78","text":"A second way of doing it\nis to use graph algorithms."},"2607.45":{"start":"2607.45","dur":"4.85","text":"So a common way of doing dependency\nparsing, you&#39;re using MST algorithms,"},"2612.30":{"start":"2612.3","dur":"1.63","text":"Minimum Spanning Tree algorithms."},"2613.93":{"start":"2613.93","dur":"3.02","text":"And that&#39;s actually a very\nsuccessful way of doing it."},"2616.95":{"start":"2616.95","dur":"3.775","text":"You can view it as kind of\na constraint satisfaction problem."},"2620.73":{"start":"2620.725","dur":"2.495","text":"And people have done that."},"2623.22":{"start":"2623.22","dur":"4.31","text":"But the way we&#39;re gonna look at it is\nthis fourth way which is, these days,"},"2627.53":{"start":"2627.53","dur":"4","text":"most commonly called transition\nbased-parsing, though when it was first"},"2631.53":{"start":"2631.53","dur":"5.03","text":"introduced, it was quite often called\ndeterministic dependency parsing."},"2636.56":{"start":"2636.56","dur":"5.2","text":"And the idea of this is that\nwe&#39;re kind of greedily going to"},"2641.76":{"start":"2641.76","dur":"6.02","text":"decide which word each\nword is a dependent of,"},"2647.78":{"start":"2647.78","dur":"3.28","text":"guided by having a machine\nlearning classifier."},"2651.06":{"start":"2651.06","dur":"3.8","text":"And this is the method you&#39;re\ngoing to use for assignment two."},"2654.86":{"start":"2654.86","dur":"2.68","text":"So one way of thinking about this is, so"},"2657.54":{"start":"2657.54","dur":"4.68","text":"far in this class,\nwe only have two hammers."},"2662.22":{"start":"2662.22","dur":"5.48","text":"One hammer we have is word vectors, and\nyou can do a lot with word vectors."},"2667.70":{"start":"2667.7","dur":"4.925","text":"And the other hammer we have is\nhow to build a classifier as"},"2672.63":{"start":"2672.625","dur":"4.205","text":"a feedforward neural network\nwith a softmax on top so"},"2676.83":{"start":"2676.83","dur":"3.918","text":"it classifies between two various classes."},"2680.75":{"start":"2680.748","dur":"2.868","text":"And it turns out that if\nthose are your two hammers,"},"2683.62":{"start":"2683.616","dur":"3.854","text":"you can do dependency parsing this way and\nit works really well."},"2687.47":{"start":"2687.47","dur":"4.362","text":"And so, therefore, that&#39;s a great\napproach for using in assignment two."},"2691.83":{"start":"2691.832","dur":"2.736","text":"And it&#39;s not just a great approach for\nassignment two."},"2694.57":{"start":"2694.568","dur":"5.252","text":"Actually method four is the dominant\nway these days of doing"},"2699.82":{"start":"2699.82","dur":"7.42","text":"dependency parsing because it has\nextremely good properties of scalability."},"2707.24":{"start":"2707.24","dur":"5.73","text":"That greedy word there is a way of\nsaying this is a linear time algorithm,"},"2712.97":{"start":"2712.97","dur":"2.1","text":"which none of the other methods are."},"2715.07":{"start":"2715.07","dur":"3.066","text":"So in the modern world\nof web-scale parsing,"},"2718.14":{"start":"2718.136","dur":"3.624","text":"it&#39;s sort of become most\npeople&#39;s favorite method."},"2721.76":{"start":"2721.76","dur":"2.45","text":"So I&#39;ll say more about that very soon."},"2724.21":{"start":"2724.21","dur":"1.641","text":"But before we get to that,"},"2725.85":{"start":"2725.851","dur":"5.16","text":"we have Ajay doing our research spotlight\nwith one last look back at word vectors."},"2733.65":{"start":"2733.651","dur":"1.751","text":"&gt;&gt; Am I on?\nOkay, awesome, so"},"2735.40":{"start":"2735.402","dur":"3.207","text":"let&#39;s take a break from\ndependency parsing and"},"2738.61":{"start":"2738.609","dur":"5.051","text":"talk about something we should\nknow a lot about, word embeddings."},"2743.66":{"start":"2743.66","dur":"5.239","text":"So for today&#39;s research highlight, we&#39;re\ngonna be talking about a paper titled,"},"2748.90":{"start":"2748.899","dur":"5.257","text":"Improving Distributional Similarity with\nLessons Learned from Word Embeddings."},"2754.16":{"start":"2754.156","dur":"1.173","text":"And it&#39;s authored by Levy, et al."},"2758.71":{"start":"2758.708","dur":"5.152","text":"So in class we&#39;ve learned two major\nparadigms for generating word vectors."},"2763.86":{"start":"2763.86","dur":"3.738","text":"We&#39;ve learned count-based\ndistributional models,"},"2767.60":{"start":"2767.598","dur":"6.302","text":"which essentially utilize a co-occurrence\nmatrix to produce your word vectors."},"2773.90":{"start":"2773.9","dur":"3.78","text":"And we&#39;ve learned SVD,\nwhich is Singular Value Decomposition."},"2777.68":{"start":"2777.68","dur":"2.5","text":"And we haven&#39;t really talked about PPMI."},"2780.18":{"start":"2780.18","dur":"1.006","text":"But, in effect,"},"2781.19":{"start":"2781.186","dur":"5.048","text":"it still uses that co-occurrence matrix to\nproduce sparse vector encodings for words."},"2786.23":{"start":"2786.234","dur":"2.513","text":"We&#39;ve also learned neural\nnetwork-based models,"},"2788.75":{"start":"2788.747","dur":"2.578","text":"which you all should have\nlots of experience with now."},"2791.33":{"start":"2791.325","dur":"5.794","text":"And, specifically, we&#39;ve talked\nabout Skip-Gram Negative Sampling,"},"2797.12":{"start":"2797.119","dur":"2.861","text":"as well as CBOW methods."},"2799.98":{"start":"2799.98","dur":"3.615","text":"And GloVe is also a neural\nnetwork-based model."},"2803.60":{"start":"2803.595","dur":"4.947","text":"And the conventional wisdom is that\nneural network-based models are superior"},"2808.54":{"start":"2808.542","dur":"1.638","text":"to count-based models."},"2811.63":{"start":"2811.63","dur":"4.85","text":"However, Levy et al proposed\nthat hyperparameters and"},"2816.48":{"start":"2816.48","dur":"3.69","text":"system design choices are more important,\nnot the embedding algorithms themselves."},"2820.17":{"start":"2820.17","dur":"3.29","text":"So they&#39;re challenging\nthis popular convention."},"2824.63":{"start":"2824.63","dur":"5.11","text":"And so, essentially,\nwhat they do in their paper is"},"2829.74":{"start":"2829.74","dur":"5.536","text":"propose a slew of hyperparameters that,\nwhen implemented and tuned over,"},"2835.28":{"start":"2835.276","dur":"5.904","text":"the count-based distributional models\npretty much approach the performance"},"2841.18":{"start":"2841.18","dur":"3.57","text":"of neural network-based models,\nto the point where there&#39;s no consistent,"},"2844.75":{"start":"2844.75","dur":"2.59","text":"better choice across the different\ntasks that they tried."},"2849.80":{"start":"2849.8","dur":"2.39","text":"And a lot of these\nhyperparameters were actually"},"2852.19":{"start":"2852.19","dur":"5.01","text":"inspired by these neural network-based\nmodels such as Skip-Gram."},"2857.20":{"start":"2857.2","dur":"3.66","text":"So if you recall, which you all\nshould be very familiar with this,"},"2860.86":{"start":"2860.86","dur":"3.335","text":"we have two hyperparameters in Skip-Gram."},"2864.20":{"start":"2864.195","dur":"3.73","text":"We have the number of negative samples\nthat we&#39;re sampling, as well as"},"2867.93":{"start":"2867.925","dur":"3.32","text":"the unigram distributions smoothing\nexponent, which we fixed at 3 over 4."},"2871.25":{"start":"2871.245","dur":"3.67","text":"But it can be thought of as\nmore of a system design choice."},"2877.06":{"start":"2877.06","dur":"3.88","text":"And these can also be transferred\nover to the account based variants."},"2880.94":{"start":"2880.94","dur":"2.8","text":"And I&#39;ll go over those very quickly."},"2883.74":{"start":"2883.74","dur":"4.027","text":"So the single hyper\nparameter that Levy et al.,"},"2887.77":{"start":"2887.767","dur":"4.621","text":"proposed that had the biggest\nimpact in performance was"},"2892.39":{"start":"2892.388","dur":"4.521","text":"Context Distribution Smoothing\nwhich is analogous to"},"2896.91":{"start":"2896.909","dur":"5.231","text":"the unigram distribution\nsmoothing constant 3 over 4 here."},"2903.21":{"start":"2903.21","dur":"4.71","text":"And in effect they both\nachieved the same goal which is"},"2907.92":{"start":"2907.92","dur":"5.81","text":"to sort of smooth out your distribution\nsuch that you&#39;re penalizing rare words."},"2913.73":{"start":"2913.73","dur":"5.046","text":"And using this hyperparameter\nwhich interestingly enough,"},"2918.78":{"start":"2918.776","dur":"4.268","text":"the optimal alpha they\nfound was exactly 3 over 4,"},"2923.04":{"start":"2923.044","dur":"5.356","text":"which is the same as\nthe Skip-Gram Unigram smoothing exponent."},"2928.40":{"start":"2928.4","dur":"3.95","text":"They were able to increase performance\nby an average of three points across"},"2932.35":{"start":"2932.35","dur":"2.64","text":"tasks on average which\nis pretty interesting."},"2936.79":{"start":"2936.79","dur":"1.91","text":"And they also propose Shifted PMI,"},"2938.70":{"start":"2938.7","dur":"2.22","text":"which I&#39;m not gonna get\ninto the details of this."},"2940.92":{"start":"2940.92","dur":"2.4","text":"But this is analogous to\nthe negative sampling,"},"2945.33":{"start":"2945.33","dur":"2.615","text":"choosing the number of\nnegative samples in Skip-Gram."},"2950.64":{"start":"2950.64","dur":"5.64","text":"And they&#39;ve also proposed a total\nof eight hyperparameters in total."},"2956.28":{"start":"2956.28","dur":"4.36","text":"And we&#39;ve described one of them which\nis the Context Distribution Smoothing."},"2962.45":{"start":"2962.45","dur":"1.78","text":"So here&#39;s the results."},"2964.23":{"start":"2964.23","dur":"4.93","text":"And this is a lot of data, and if you&#39;re\nconfused, that&#39;s actually the conclusion"},"2969.16":{"start":"2969.16","dur":"6.33","text":"that I want you to arrive at because\nclearly there&#39;s no trend here."},"2975.49":{"start":"2975.49","dur":"5.98","text":"So, what the authors did was\ntake all four methods, tried"},"2981.47":{"start":"2981.47","dur":"5.18","text":"three different windows, and then test\nall the models across a different task."},"2986.65":{"start":"2986.65","dur":"3.47","text":"And those are split up into word\nsimilarity and analogy task."},"2990.12":{"start":"2990.12","dur":"4.27","text":"And all of these methods are tuned"},"2994.39":{"start":"2994.39","dur":"3.57","text":"to find the best hyperparameters\nto optimize for the performance."},"2997.96":{"start":"2997.96","dur":"5.09","text":"And the best models are bolded, and as you\ncan see there&#39;s no consistent best model."},"3003.05":{"start":"3003.05","dur":"6.18","text":"So, in effect, they&#39;re challenging\nthe popular convention that"},"3009.23":{"start":"3009.23","dur":"6.3","text":"neural network-based models\nare superior to the count-based models."},"3015.53":{"start":"3015.53","dur":"2.506","text":"However, there&#39;s a few\nthings to note here."},"3018.04":{"start":"3018.036","dur":"5.503","text":"Number one, adding hyperparameters\nis never a great thing because"},"3023.54":{"start":"3023.539","dur":"5.124","text":"now you have to train those\nhyperparameters which takes time."},"3028.66":{"start":"3028.663","dur":"4.987","text":"Number two,\nwe still have the issues with count-based"},"3033.65":{"start":"3033.65","dur":"6.476","text":"distributional models specifically\nwith respect to the computational"},"3040.13":{"start":"3040.126","dur":"5.449","text":"issues of storing PPMI counts\nas well as performing SVD."},"3052.27":{"start":"3052.27","dur":"4.122","text":"So the key takeaways here is that the\npaper challenges the conventional wisdom"},"3056.39":{"start":"3056.392","dur":"4.378","text":"that neutral network-based models are in\nfact superior to count-based models."},"3062.60":{"start":"3062.6","dur":"3.055","text":"Number two,\nwhile model design is important,"},"3065.66":{"start":"3065.655","dur":"3.805","text":"hyperparameters are also key for\nachieving good results."},"3069.46":{"start":"3069.46","dur":"3.86","text":"So this implies specifically to\nyou guys especially if you&#39;re"},"3073.32":{"start":"3073.32","dur":"2.78","text":"doing a project instead\nof assignment four."},"3076.10":{"start":"3076.1","dur":"5.09","text":"You might implement the model but\nthat might only take you half way there."},"3081.19":{"start":"3081.19","dur":"4.84","text":"Some models to find your optimal\nhyperparameters might take days or"},"3086.03":{"start":"3086.03","dur":"1.34","text":"even weeks to find."},"3087.37":{"start":"3087.37","dur":"1.4","text":"So don&#39;t discount their importance."},"3089.93":{"start":"3089.93","dur":"5.45","text":"And, finally, my personal interest within\nML is in deep representation learning."},"3095.38":{"start":"3095.38","dur":"4.035","text":"And this paper specifically excites\nme because I think it sort of"},"3099.42":{"start":"3099.415","dur":"4.485","text":"displays that there&#39;s still lots\nof work to be done in the field."},"3104.99":{"start":"3104.99","dur":"4.068","text":"And so, the final takeaway\nis challenge the status quo."},"3109.06":{"start":"3109.058","dur":"1.537","text":"Thank you."},"3110.60":{"start":"3110.595","dur":"5.125","text":"&gt;&gt; [APPLAUSE]"},"3115.72":{"start":"3115.72","dur":"2.9","text":"&gt;&gt; Okay, thanks a lot Ajay."},"3118.62":{"start":"3118.62","dur":"4.59","text":"Okay and so\nnow we&#39;re back to learning about how to"},"3123.21":{"start":"3123.21","dur":"4.278","text":"build a transition based\ndependency parser."},"3127.49":{"start":"3127.488","dur":"5.595","text":"So, maybe in 103 or compilers class,\nformal languages class,"},"3133.08":{"start":"3133.083","dur":"4.032","text":"there&#39;s this notion of\nshift reduced parsing."},"3137.12":{"start":"3137.115","dur":"2.94","text":"How many of you have seen shift\nreduced parsing somewhere?"},"3141.22":{"start":"3141.22","dur":"2.24","text":"A minority it turns out."},"3143.46":{"start":"3143.46","dur":"4.696","text":"They just don&#39;t teach formal languages the\nway they used to in the 1960s in computer"},"3148.16":{"start":"3148.156","dur":"1.02","text":"science anymore."},"3149.18":{"start":"3149.176","dur":"4.08","text":"&gt;&gt; [LAUGH]\n&gt;&gt; You&#39;ll just have to spend more time"},"3153.26":{"start":"3153.256","dur":"0.814","text":"with Jeff Ullman."},"3154.07":{"start":"3154.07","dur":"2.91","text":"Okay, well I won&#39;t assume that\nyou&#39;ve all seen that before."},"3157.99":{"start":"3157.99","dur":"8.59","text":"Okay, essentially what\nwe&#39;re going to have is,"},"3166.58":{"start":"3166.58","dur":"5.13","text":"I&#39;ll just skip these two slides and\ngo straight to the pictures."},"3171.71":{"start":"3171.71","dur":"1.93","text":"Because, they will be\nmuch more understandable."},"3173.64":{"start":"3173.64","dur":"4.143","text":"But before I go on, I&#39;ll just\nmention the picture on this page,"},"3177.78":{"start":"3177.783","dur":"2.019","text":"that&#39;s a picture of Joakim Nivre."},"3179.80":{"start":"3179.802","dur":"3.576","text":"So Joakim Nivre is a computational\nlinguist in Uppsala,"},"3183.38":{"start":"3183.378","dur":"5.382","text":"Sweden who pioneered this approach of\ntransition based dependency parsing."},"3188.76":{"start":"3188.76","dur":"2.61","text":"He&#39;s one of my favorite\ncomputational linguists."},"3191.37":{"start":"3191.37","dur":"4.33","text":"I mean he was also an example,\ngoing along with what Ajay said,"},"3195.70":{"start":"3195.7","dur":"3.93","text":"of sort of doing something unpopular and"},"3199.63":{"start":"3199.63","dur":"3.44","text":"out of the mainstream and\nproving that you can get it to work well."},"3203.07":{"start":"3203.07","dur":"5.097","text":"So at an age when everyone else was trying\nto build sort of fancy dynamic program"},"3208.17":{"start":"3208.167","dur":"5.097","text":"parsers Joakim said no,no, what I&#39;m\ngonna do, is I&#39;m just gonna take each"},"3213.26":{"start":"3213.264","dur":"5.125","text":"successive word and have a straight\nclassifier that says what to do with that."},"3218.39":{"start":"3218.389","dur":"4.402","text":"And go onto the next word completely\ngreedy cuz maybe that&#39;s kinda like what"},"3222.79":{"start":"3222.791","dur":"3.028","text":"humans do with incremental\nsentence processing and"},"3225.82":{"start":"3225.819","dur":"3.171","text":"I&#39;m gonna see how well\nI can make that work."},"3228.99":{"start":"3228.99","dur":"2.68","text":"And it turned out you can\nmake it work really well."},"3231.67":{"start":"3231.67","dur":"4.67","text":"So and then sort of transition based\nparsing has grown to this sort of"},"3236.34":{"start":"3236.34","dur":"2.93","text":"really widespread dominant\nway of doing parsing."},"3239.27":{"start":"3239.27","dur":"5.81","text":"So it&#39;s good to find something different\nto do If everyone else is doing something,"},"3245.08":{"start":"3245.08","dur":"2.96","text":"it&#39;s good to think of something else\nthat might be promising that you"},"3248.04":{"start":"3248.04","dur":"0.91","text":"got an idea from."},"3248.95":{"start":"3248.95","dur":"3.99","text":"And I also like Joakim because he&#39;s\nactually another person that&#39;s really"},"3252.94":{"start":"3252.94","dur":"1.68","text":"interested in human languages and"},"3254.62":{"start":"3254.62","dur":"3.69","text":"linguistics which actually seems\nto be a minority of the field of"},"3258.31":{"start":"3258.31","dur":"2.57","text":"natural language processing\nwhen it comes down to it."},"3260.88":{"start":"3260.88","dur":"4.94","text":"Okay, so here&#39;s some more formalism,\nbut I&#39;ll skip that as well and"},"3265.82":{"start":"3265.82","dur":"3.734","text":"show it to you afterwards and\nI&#39;ll give you the idea of what"},"3269.55":{"start":"3269.554","dur":"5.036","text":"an arc-standard transition-based\ndependency parser does."},"3276.16":{"start":"3276.16","dur":"5.281","text":"So what we&#39;re gonna do is were going\nto have a sentence we want to parse,"},"3281.44":{"start":"3281.441","dur":"5.546","text":"I ate fish, and so we&#39;ve got some rules\nfor parsing which is the transition"},"3286.99":{"start":"3286.987","dur":"4.828","text":"scheme which is written so\nsmall you can&#39;t possibly read it."},"3291.82":{"start":"3291.815","dur":"1.635","text":"And this is how we start."},"3293.45":{"start":"3293.45","dur":"3.32","text":"So we have two things,\nwe have a stack, and"},"3296.77":{"start":"3296.77","dur":"4.03","text":"a stack is kinda got the gray\ncartouche around that."},"3300.80":{"start":"3300.8","dur":"4.83","text":"And we start off parsing any\nsentence by putting it on the stack,"},"3305.63":{"start":"3305.63","dur":"3.32","text":"one thing, which is our root symbol."},"3308.95":{"start":"3308.95","dur":"5.29","text":"Okay and\nthe stack has its top towards the right."},"3314.24":{"start":"3314.24","dur":"3.8","text":"And then we have this other thing\nwhich gets referred to as the buffer."},"3318.04":{"start":"3318.04","dur":"2.497","text":"And the buffer is the orange cartouche and"},"3320.54":{"start":"3320.537","dur":"3.833","text":"the buffer is the sentence\nthat we&#39;ve got to deal with."},"3324.37":{"start":"3324.37","dur":"5.43","text":"And so the thing that we regard as the top\nof the buffer is the thing to the left,"},"3329.80":{"start":"3329.8","dur":"2.56","text":"because we&#39;re gonna be taking\noff excessive words right?"},"3332.36":{"start":"3332.36","dur":"5.22","text":"So the top of both of them is sort of at\nthat intersection point between them."},"3337.58":{"start":"3337.58","dur":"4.98","text":"Okay and so,\nto do parsing under this transition-based"},"3342.56":{"start":"3342.56","dur":"4.45","text":"scheme there are three\noperations that we can perform."},"3347.01":{"start":"3347.01","dur":"6.43","text":"We can perform, they&#39;re called Shift,\nLeft-Arc and Right-Arc."},"3353.44":{"start":"3353.44","dur":"3.77","text":"So the first one that we&#39;re\ngonna do is shift operation."},"3357.21":{"start":"3357.21","dur":"2.04","text":"So shift is really easy."},"3359.25":{"start":"3359.25","dur":"5.25","text":"All we do when we do a shift is we take\nthe word that&#39;s on the top of the buffer"},"3364.50":{"start":"3364.5","dur":"1.63","text":"and put it on the top of the stack."},"3367.53":{"start":"3367.53","dur":"1.785","text":"And then we can shift again and"},"3369.32":{"start":"3369.315","dur":"5.265","text":"we take the word that&#39;s on the top of the\nbuffer and put it on the top of the stack."},"3374.58":{"start":"3374.58","dur":"3.41","text":"Remember the stack,\nthe top is to the right."},"3377.99":{"start":"3377.99","dur":"2.67","text":"The buffer, the top is to the left."},"3380.66":{"start":"3380.66","dur":"1.71","text":"That&#39;s pretty easy, right?"},"3382.37":{"start":"3382.37","dur":"5.902","text":"Okay, so there are two other\noperations left in this arc-standard"},"3388.27":{"start":"3388.272","dur":"5.078","text":"transition scheme which were left arc and\nright arc."},"3393.35":{"start":"3393.35","dur":"5.36","text":"So what left arc and right arc\nare gonna do is we&#39;re going to make"},"3398.71":{"start":"3398.71","dur":"4.19","text":"attachment decisions by adding\na word as the dependent,"},"3402.90":{"start":"3402.9","dur":"2.86","text":"either to the left or to the right."},"3405.76":{"start":"3405.76","dur":"4.37","text":"Okay, so what we do for left arc is"},"3410.13":{"start":"3410.13","dur":"5.01","text":"on the stack we say that\nthe second to the top"},"3415.14":{"start":"3415.14","dur":"5.39","text":"of the stack is a dependent of\nthe thing that&#39;s the top of the stack."},"3420.53":{"start":"3420.53","dur":"6.3","text":"So, I is a dependent of ate, and we remove\nthat second top thing from the stack."},"3426.83":{"start":"3426.83","dur":"2.71","text":"So that&#39;s a left arc operation."},"3429.54":{"start":"3429.54","dur":"3.53","text":"And so now we&#39;ve got a stack\nwith just [root] ate on it."},"3433.07":{"start":"3433.07","dur":"4.99","text":"But we collect up our decisions, so we&#39;ve\nmade a decision that I is a dependent of"},"3438.06":{"start":"3438.06","dur":"4.62","text":"ate, and that&#39;s that said A that I am\nwriting in small print off to the right."},"3442.68":{"start":"3442.68","dur":"3.73","text":"Okay, so\nwe still had our buffer with fish on it."},"3446.41":{"start":"3446.41","dur":"7","text":"So the next thing we&#39;re gonna do is\nshift again and put fish on the stack."},"3453.41":{"start":"3453.41","dur":"2.44","text":"And so at that point our buffer is empty,"},"3455.85":{"start":"3455.85","dur":"2.67","text":"we&#39;ve moved every word on to\nthe stack in our sentence."},"3458.52":{"start":"3458.52","dur":"3.18","text":"And we have on it root ate fish, okay."},"3461.70":{"start":"3461.7","dur":"4.425","text":"So then the third operation we have"},"3466.13":{"start":"3466.125","dur":"4.665","text":"is right arc, and right arc is\njust the opposite of left arc."},"3470.79":{"start":"3470.79","dur":"5.28","text":"So for the right arc operation, we say\nthe thing that&#39;s on the top of the stack"},"3476.07":{"start":"3476.07","dur":"4.87","text":"should be made a dependent of the thing\nthat&#39;s second to top on the stack."},"3480.94":{"start":"3480.94","dur":"4.365","text":"We remove it from the stack and\nwe add an arc saying that."},"3485.31":{"start":"3485.305","dur":"3.355","text":"So we right arc, so"},"3488.66":{"start":"3488.66","dur":"5.04","text":"we say fish is a dependent of ate,\nand we remove fish from the stack."},"3493.70":{"start":"3493.7","dur":"5.96","text":"We add a new dependency saying\nthat fish is a dependent of ate."},"3499.66":{"start":"3499.66","dur":"4.39","text":"And then we right arc one more time so"},"3504.05":{"start":"3504.05","dur":"4.53","text":"then we&#39;re saying that ate is\nthe dependent of the root."},"3508.58":{"start":"3508.58","dur":"4.69","text":"So we pop it off the stack and we&#39;re\njust left with root on the stack, and"},"3513.27":{"start":"3513.27","dur":"5.49","text":"we&#39;ve got one new dependency saying\nthat ate is a dependent of root."},"3518.76":{"start":"3518.76","dur":"4.26","text":"So at this point, And\nI&#39;ll just mention, right,"},"3523.02":{"start":"3523.02","dur":"4.93","text":"in reality there&#39;s,\nI left out writing the buffer in a few of"},"3527.95":{"start":"3527.95","dur":"3.91","text":"those examples there just because it was\ngetting pretty crowded on the slide."},"3531.86":{"start":"3531.86","dur":"3.46","text":"But really the buffer is always there,\nright, it&#39;s not that the buffer"},"3535.32":{"start":"3535.32","dur":"4.15","text":"disappeared and came back again,\nit&#39;s just I didn&#39;t always draw it."},"3539.47":{"start":"3539.47","dur":"1.96","text":"So but in our end state,"},"3541.43":{"start":"3541.43","dur":"4.66","text":"we&#39;ve got one thing on the stack,\nand we&#39;ve got nothing in the buffer."},"3546.09":{"start":"3546.09","dur":"2.75","text":"And that&#39;s the good state\nthat we want to be in if we"},"3548.84":{"start":"3548.84","dur":"2.28","text":"finish parsing our sentence correctly."},"3551.12":{"start":"3551.12","dur":"3.606","text":"And so we say, okay,\nwe&#39;re in the finished state and we stop."},"3554.73":{"start":"3554.726","dur":"4.852","text":"And so that is almost all there"},"3559.58":{"start":"3559.578","dur":"5.052","text":"is to arc-standard\ntransition based parsing."},"3564.63":{"start":"3564.63","dur":"4.19","text":"So if just sort of go back to\nthese slides that I skipped over."},"3570.35":{"start":"3570.35","dur":"4.81","text":"Right, so we have a stack and our buffer,\nand then on the side we have a set of"},"3575.16":{"start":"3575.16","dur":"5.99","text":"dependency arcs A which starts\noff empty and we add things to."},"3581.15":{"start":"3581.15","dur":"4.32","text":"And we have this sort of set of actions\nwhich are kind of legal moves that we can"},"3585.47":{"start":"3585.47","dur":"4.74","text":"make for parsing, and so\nthis was how things are."},"3590.21":{"start":"3590.21","dur":"6.33","text":"So we have a start condition, ROOT on the\nstack, buffer is the sentence, no arcs."},"3596.54":{"start":"3596.54","dur":"3.49","text":"We have the three operations\nthat we can perform."},"3600.03":{"start":"3600.03","dur":"3.441","text":"Here I&#39;ve tried to write\nthem out formally, so"},"3603.47":{"start":"3603.471","dur":"5.929","text":"the sort of vertical bar is sort of\nappends an element to a list operation."},"3609.40":{"start":"3609.4","dur":"6.92","text":"So this is sort of having wi as the first\nword on the buffer, it&#39;s written"},"3616.32":{"start":"3616.32","dur":"3.7","text":"the opposite way around for the stack\nbecause the head&#39;s on the other side."},"3620.02":{"start":"3620.02","dur":"4.07","text":"And so we can sort of do this shift\noperation of moving a word onto the stack"},"3624.09":{"start":"3624.09","dur":"4.93","text":"and these two arc operations\nadd a new dependency."},"3629.02":{"start":"3629.02","dur":"5.16","text":"And then removing one word from the stack\nand our ending condition is one"},"3634.18":{"start":"3634.18","dur":"4.948","text":"thing on the stack which will\nbe the root and an empty buffer."},"3639.13":{"start":"3639.128","dur":"3.342","text":"And so\nthat&#39;s sort of the formal operations."},"3642.47":{"start":"3642.47","dur":"4.55","text":"So the idea of transition based\nparsing is that you have this sort of"},"3647.02":{"start":"3647.02","dur":"5.11","text":"set of legal moves to parse a sentence\nin sort of a shift reduced way."},"3652.13":{"start":"3652.13","dur":"3","text":"I mean this one I referred to as\narc-standard cuz it turns out there"},"3655.13":{"start":"3655.13","dur":"3.97","text":"are different ways you can define\nyour sets of dependencies."},"3659.10":{"start":"3659.1","dur":"3.46","text":"But this is the simplest one,\nthe one we&#39;ll use for the assignment, and"},"3662.56":{"start":"3662.56","dur":"1.74","text":"one that works pretty well."},"3664.30":{"start":"3664.3","dur":"0.5","text":"Question?"},"3666.65":{"start":"3666.654","dur":"1.756","text":"I was gonna get to that."},"3668.41":{"start":"3668.41","dur":"2.763","text":"So I&#39;ve told you the whole\nthing except for"},"3671.17":{"start":"3671.173","dur":"4.227","text":"one thing which is this just gives\nyou a set of possible moves."},"3675.40":{"start":"3675.4","dur":"3.36","text":"It doesn&#39;t say which\nmove you should do when."},"3678.76":{"start":"3678.76","dur":"3.93","text":"And so\nthat&#39;s the remaining thing that&#39;s left."},"3682.69":{"start":"3682.69","dur":"1.2","text":"And I have a slide on that."},"3684.96":{"start":"3684.96","dur":"5.76","text":"Okay, so the only thing that&#39;s left\nis to say, gee, at any point in time,"},"3690.72":{"start":"3690.72","dur":"5.79","text":"like we were here, at any point in time,\nyou&#39;re in some configuration, right."},"3696.51":{"start":"3696.51","dur":"3.95","text":"You&#39;ve got certain things on there,\ncertain things in the stacks,"},"3700.46":{"start":"3700.46","dur":"4.846","text":"certain things in your buffer, you have\nsome set of arcs that you&#39;ve already made."},"3705.31":{"start":"3705.306","dur":"5.584","text":"And which one of these\noperations do I do next?"},"3710.89":{"start":"3710.89","dur":"1.86","text":"And so that&#39;s the final thing."},"3712.75":{"start":"3712.75","dur":"3.793","text":"And the way that you do that,\nthat Nivre proposed,"},"3716.54":{"start":"3716.543","dur":"5.897","text":"is well what we should do is just\nbuild a machine learning classifier."},"3722.44":{"start":"3722.44","dur":"3.69","text":"Since we have a tree bank\nwith parses of sentences,"},"3726.13":{"start":"3726.13","dur":"3.54","text":"we can use those parses\nof sentences to see"},"3729.67":{"start":"3729.67","dur":"5.06","text":"which sequence of operations would\ngive the correct parse of a sentence."},"3734.73":{"start":"3734.73","dur":"2.51","text":"I am not actually gonna go\nthrough that right now."},"3737.24":{"start":"3737.24","dur":"3.43","text":"But if you have the structure\nof a sentence in a tree bank,"},"3740.67":{"start":"3740.67","dur":"5.312","text":"you can sort of work out deterministically\nthe sequence of shifts and"},"3745.98":{"start":"3745.982","dur":"2.798","text":"reducers that you need\nto get that structure."},"3748.78":{"start":"3748.78","dur":"4.194","text":"And it&#39;s indeed unique, right, that for\neach tree structure there&#39;s a sequence of"},"3752.97":{"start":"3752.974","dur":"3.936","text":"shifts and left arcs and right arcs\nthat will give you the right structure."},"3756.91":{"start":"3756.91","dur":"3.66","text":"So you take the tree, you read off\nthe correct operation sequence, and"},"3760.57":{"start":"3760.57","dur":"3.242","text":"therefore you&#39;ve got a supervised\nclassification problem."},"3763.81":{"start":"3763.812","dur":"4.256","text":"Say in this scenario, what you\nshould do next is you should shift,"},"3768.07":{"start":"3768.068","dur":"4.652","text":"and so you&#39;re then building\na classified to try to predict that."},"3772.72":{"start":"3772.72","dur":"7.76","text":"So in the early work that started off\nwith Nivre and others in the mid 2000s,"},"3780.48":{"start":"3780.48","dur":"5.28","text":"this was being done with conventional\nmachine learning classifiers."},"3785.76":{"start":"3785.76","dur":"5.734","text":"So maybe an SVM, maybe a perceptron,\na kind of maxent \/ soft max classifiers,"},"3791.49":{"start":"3791.494","dur":"5.346","text":"various things, but sort of some\nclassified that you&#39;re gonna use."},"3796.84":{"start":"3796.84","dur":"5.07","text":"So if you&#39;re just deciding between\nthe operations, shift left arc,"},"3801.91":{"start":"3801.91","dur":"3.03","text":"right arc,\nyou have got at most three choices."},"3804.94":{"start":"3804.94","dur":"3.87","text":"Occasionally you have less because if\nthere&#39;s nothing left on the buffer"},"3808.81":{"start":"3808.81","dur":"3.79","text":"you can&#39;t shift anymore, so then you&#39;d\nonly have two choices left maybe."},"3812.60":{"start":"3812.6","dur":"4.66","text":"But something I didn&#39;t mention\nwhen I was showing this is when"},"3817.26":{"start":"3817.26","dur":"4.52","text":"I added to the arc set, I didn&#39;t only\nsay that fish is an object of ate."},"3821.78":{"start":"3821.78","dur":"3.96","text":"I said,\nthe dependency is the object of ate."},"3825.74":{"start":"3825.74","dur":"3.53","text":"And so\nif you want to include dependency labels,"},"3829.27":{"start":"3829.27","dur":"5.72","text":"the standard way of doing that is you just\nhave sub types of left arc and right arc."},"3834.99":{"start":"3834.99","dur":"2.43","text":"So rather than having three choices."},"3837.42":{"start":"3837.42","dur":"3.1","text":"If you have a approximately 40\ndifferent dependency labels."},"3840.52":{"start":"3840.52","dur":"4.07","text":"As we will in assignment two and\nin universal dependencies."},"3844.59":{"start":"3844.59","dur":"5.9","text":"You actually end up with the space\nof 81 way classification."},"3850.49":{"start":"3850.49","dur":"4.6","text":"Because you have classes with\nnames like left arc as an object."},"3855.09":{"start":"3855.09","dur":"4.26","text":"Or left arc as an adjectival modifier."},"3859.35":{"start":"3859.35","dur":"2.43","text":"For the assignment,\nyou don&#39;t have to do that."},"3861.78":{"start":"3861.78","dur":"3.27","text":"For the assignment,\nwe&#39;re just doing un-type dependency trees."},"3865.05":{"start":"3865.05","dur":"3.4","text":"Which sort of makes it a bit more\nscalable and easy for you guys."},"3868.45":{"start":"3868.45","dur":"4.43","text":"So it&#39;s only sort of a three way\ndecision is all you&#39;re doing."},"3872.88":{"start":"3872.88","dur":"4.58","text":"In most real applications, it&#39;s really\nhandy to have those dependency labels."},"3877.46":{"start":"3877.46","dur":"1.08","text":"Okay."},"3878.54":{"start":"3878.54","dur":"3.26","text":"And then what do we use as features?"},"3881.80":{"start":"3881.8","dur":"3.71","text":"Well, in the traditional model, you sort\nof looked at all the words around you."},"3885.51":{"start":"3885.51","dur":"2.82","text":"You saw what word was on\nthe top of the stack."},"3888.33":{"start":"3888.33","dur":"2.16","text":"What was the part of speech of that word?"},"3890.49":{"start":"3890.49","dur":"1.25","text":"What was the first word in the buffer?"},"3891.74":{"start":"3891.74","dur":"1.77","text":"What was its parts of speech?"},"3893.51":{"start":"3893.51","dur":"3.65","text":"Maybe it&#39;s good to look at the thing\nbeneath the top of the stack."},"3897.16":{"start":"3897.16","dur":"2.84","text":"And what word and part of speech it is."},"3900.00":{"start":"3900","dur":"1.36","text":"And further ahead in the buffers."},"3901.36":{"start":"3901.36","dur":"1.813","text":"So you&#39;re looking at a bunch of words."},"3903.17":{"start":"3903.173","dur":"3.877","text":"You&#39;re looking at some attributes of those\nwords, such as their part of speech."},"3907.05":{"start":"3907.05","dur":"3.132","text":"And that was giving you\na bunch of features."},"3910.18":{"start":"3910.182","dur":"3.407","text":"Which are the same kind of classic,\ncategorical,"},"3913.59":{"start":"3913.589","dur":"3.497","text":"sparse features of\ntraditional machine learning."},"3917.09":{"start":"3917.086","dur":"3.263","text":"And people were building\nclassifiers over that."},"3920.35":{"start":"3920.349","dur":"1.168","text":"Yeah, Question?"},"3927.80":{"start":"3927.798","dur":"4.702","text":"So yeah, the question is are most\ntreebanks annotated with part of speech?"},"3932.50":{"start":"3932.5","dur":"1.377","text":"And the answer is yes."},"3933.88":{"start":"3933.877","dur":"1.168","text":"Yeah, so I mean."},"3935.05":{"start":"3935.045","dur":"3.098","text":"We&#39;ve barely talked about\npart of speech so far,"},"3938.14":{"start":"3938.143","dur":"3.237","text":"things like living things,\nnouns, and verbs."},"3941.38":{"start":"3941.38","dur":"3.173","text":"So the simplest way of doing\ndependency parsing as you&#39;re"},"3944.55":{"start":"3944.553","dur":"4.028","text":"first writing a part of speech, tag it or\nassign parts of speech to words."},"3948.58":{"start":"3948.581","dur":"4.514","text":"And then you&#39;re doing the syntactic\nstructure of dependency parsing over"},"3953.10":{"start":"3953.095","dur":"3.095","text":"a sequence of word,\npart of speech, tag pairs."},"3956.19":{"start":"3956.19","dur":"3.91","text":"Though there has been other work\nthat&#39;s done joint parsing and"},"3960.10":{"start":"3960.1","dur":"2.64","text":"part of speech tag\nprediction at the same time."},"3962.74":{"start":"3962.74","dur":"3.79","text":"Which actually has some advantages,\nbecause you can kind of explore."},"3966.53":{"start":"3966.53","dur":"3.13","text":"Since the two things are associated,"},"3969.66":{"start":"3969.66","dur":"3.35","text":"you can get some advantages\nfrom doing it jointly."},"3973.01":{"start":"3973.01","dur":"6.05","text":"Okay, on the simplest possible model,\nwhich was what Nivre started to explore."},"3979.06":{"start":"3979.06","dur":"2.285","text":"There was absolutely no search."},"3981.35":{"start":"3981.345","dur":"2.585","text":"You just took the next word,\nran your classifier."},"3983.93":{"start":"3983.93","dur":"4.18","text":"And said, that&#39;s the object of the verb,\nwhat&#39;s the next word?"},"3988.11":{"start":"3988.11","dur":"1.65","text":"Okay, that one&#39;s a noun modifier."},"3989.76":{"start":"3989.76","dur":"3.27","text":"And you went along and\njust made these decisions."},"3993.03":{"start":"3993.03","dur":"4.35","text":"Now you could obviously think,\ngee maybe if I did some more searching and"},"3997.38":{"start":"3997.38","dur":"2.38","text":"explore different alternatives\nI could do a bit better."},"3999.76":{"start":"3999.76","dur":"1.97","text":"And the answer is yes, you can."},"4001.73":{"start":"4001.73","dur":"2.43","text":"So there&#39;s a lot of work\nin dependency parsing."},"4004.16":{"start":"4004.16","dur":"4.54","text":"Which uses various forms of beam search\nwhere you explore different alternatives."},"4008.70":{"start":"4008.7","dur":"3.14","text":"And if you do that, it gets a ton slower."},"4011.84":{"start":"4011.84","dur":"3.62","text":"And gets a teeny bit better in\nterms of your performance results."},"4017.21":{"start":"4017.21","dur":"5.32","text":"Okay, but especially if you start from the\ngreediest end or you have a small beam."},"4022.53":{"start":"4022.53","dur":"5.09","text":"The secret of this type of parsing\nis it gives you extremely fast"},"4027.62":{"start":"4027.62","dur":"1.55","text":"linear time parsing."},"4029.17":{"start":"4029.17","dur":"3.54","text":"Because you&#39;re just going through\nyour corpus, no matter how big."},"4032.71":{"start":"4032.71","dur":"1.52","text":"And say, what&#39;s the next word?"},"4034.23":{"start":"4034.23","dur":"1.043","text":"Okay, attach it there."},"4035.27":{"start":"4035.273","dur":"0.981","text":"What&#39;s the next word?"},"4036.25":{"start":"4036.254","dur":"1.126","text":"Attach it there."},"4037.38":{"start":"4037.38","dur":"2.28","text":"And you keep on chugging through."},"4039.66":{"start":"4039.66","dur":"4.83","text":"So when people, like prominent search\nengines in suburbs south of us,"},"4044.49":{"start":"4044.49","dur":"2.03","text":"want to parse the entire\ncontent of the Web."},"4046.52":{"start":"4046.52","dur":"3.98","text":"They use a parser like this\nbecause it goes super fast."},"4051.82":{"start":"4051.82","dur":"0.5","text":"Okay."},"4053.84":{"start":"4053.84","dur":"4.47","text":"And so, what was shown was these\nkind of greedy dependencies parses."},"4058.31":{"start":"4058.31","dur":"5.91","text":"Their accuracy is slightly below\nthe best dependency parses possible."},"4064.22":{"start":"4064.22","dur":"3.99","text":"But their performance is\nactually kind of close to it."},"4068.21":{"start":"4068.21","dur":"3.57","text":"And the fact that they&#39;re sort of so\nfast and scalable."},"4071.78":{"start":"4071.78","dur":"3.52","text":"More than makes up for\ntheir teeny performance decrease."},"4075.30":{"start":"4075.3","dur":"1.72","text":"So that&#39;s kind of exciting."},"4079.01":{"start":"4079.01","dur":"5.36","text":"Okay, so then for the last few minutes\nI now want to get back to neural nets."},"4084.37":{"start":"4084.37","dur":"1.75","text":"Okay so where are we at the moment?"},"4086.12":{"start":"4086.12","dur":"3.93","text":"So at the moment we have a configuration\nwhere we have a stack and"},"4090.05":{"start":"4090.05","dur":"2.65","text":"a buffer and parts of speech or words."},"4092.70":{"start":"4092.7","dur":"2.21","text":"And as we start to build some structure."},"4099.58":{"start":"4099.58","dur":"3.1","text":"The things that we&#39;ve taken off\nthe stack when we build arcs."},"4102.68":{"start":"4102.68","dur":"3.499","text":"We can kind of sort of think of them as\nstarting to build up a tree as we go."},"4106.18":{"start":"4106.179","dur":"2.821","text":"As I&#39;ve indicated with that example below."},"4109.00":{"start":"4109","dur":"4.8","text":"So, the classic way of doing that\nis you could then say, okay,"},"4113.80":{"start":"4113.8","dur":"1.82","text":"well we&#39;ve got all of these features."},"4115.62":{"start":"4115.62","dur":"4.13","text":"Like top of stack is word good,\nor top of stack is word bad,"},"4119.75":{"start":"4119.75","dur":"2.06","text":"or top of stack is word easy."},"4121.81":{"start":"4121.81","dur":"2.04","text":"Top of stack&#39;s part of\nspeech as adjective."},"4123.85":{"start":"4123.85","dur":"1.89","text":"Top of stack&#39;s word is noun."},"4125.74":{"start":"4125.74","dur":"1.4","text":"And if you start doing that."},"4127.14":{"start":"4127.14","dur":"5.4","text":"When you&#39;ve got a combination of\npositions and words and parts of speech."},"4132.54":{"start":"4132.54","dur":"4.77","text":"You very quickly find that the number\nof features you have in your model"},"4137.31":{"start":"4137.31","dur":"2.29","text":"is sort of order ten million."},"4139.60":{"start":"4139.6","dur":"1.69","text":"Extremely, extremely large."},"4141.29":{"start":"4141.29","dur":"4.96","text":"But you know that&#39;s precisely how these\nkinds of parses were standardly made"},"4146.25":{"start":"4146.25","dur":"1.77","text":"in the 2000s."},"4148.02":{"start":"4148.02","dur":"5.88","text":"So you&#39;re building these huge machine\nlearning classifiers over sparse features."},"4153.90":{"start":"4153.9","dur":"3.08","text":"And commonly you even had features\nthat were conjunctions of things."},"4156.98":{"start":"4156.98","dur":"1.23","text":"As that helped you predict better."},"4158.21":{"start":"4158.21","dur":"4.39","text":"So you had features like the second\nword on the stack is has."},"4162.60":{"start":"4162.6","dur":"2.84","text":"And its tag is present tense verb."},"4165.44":{"start":"4165.44","dur":"1.6","text":"And the top word on the stack is good."},"4167.04":{"start":"4167.04","dur":"1.75","text":"And things like that would be one feature."},"4168.79":{"start":"4168.79","dur":"4.98","text":"And that&#39;s where you easily get\ninto the ten million plus features."},"4173.77":{"start":"4173.77","dur":"3.7","text":"So even doing this already\nworked quite well."},"4177.47":{"start":"4177.47","dur":"4.23","text":"But the starting point\nfrom going on is saying,"},"4181.70":{"start":"4181.7","dur":"3.45","text":"well it didn&#39;t work completely great."},"4186.86":{"start":"4186.86","dur":"1.93","text":"That we wanna do better than that."},"4188.79":{"start":"4188.79","dur":"3.31","text":"And we&#39;ll go on and\ndo that in just a minute."},"4192.10":{"start":"4192.1","dur":"4.64","text":"But before I do that, I should mention\njust the evaluation of dependency parsing."},"4196.74":{"start":"4196.74","dur":"3.75","text":"Evaluation of dependency\nparsing is actually very easy."},"4200.49":{"start":"4200.49","dur":"4.36","text":"Cuz since for each word we&#39;re saying,\nwhat is it a dependent of."},"4204.85":{"start":"4204.85","dur":"4.33","text":"That we&#39;re sort of making choices of\nwhat each word is a dependent of."},"4209.18":{"start":"4209.18","dur":"1.51","text":"And then there&#39;s a right answer."},"4210.69":{"start":"4210.69","dur":"3.43","text":"Which we get from our tree bank,\nwhich is the gold thing."},"4214.12":{"start":"4214.12","dur":"4.71","text":"We&#39;re sort of, essentially,\njust counting how often we are right."},"4218.83":{"start":"4218.83","dur":"1.92","text":"Which is an accuracy measure."},"4220.75":{"start":"4220.75","dur":"3.61","text":"And so, there are two ways\nthat that&#39;s commonly done."},"4224.36":{"start":"4224.36","dur":"5.27","text":"One way is that we just look at\nthe arrows and ignore the labels."},"4229.63":{"start":"4229.63","dur":"4.76","text":"And that&#39;s often referred to as\nthe UAS measure, unlabeled accuracy."},"4234.39":{"start":"4234.39","dur":"3.21","text":"Or we can also pay\nattention to the labels."},"4237.60":{"start":"4237.6","dur":"2.85","text":"And say you&#39;re only right if\nyou also get the label right."},"4240.45":{"start":"4240.45","dur":"4.17","text":"And that&#39;s referred to as the LAS,\nthe labelled accuracy score."},"4244.62":{"start":"4244.62","dur":"0.533","text":"Yes?"},"4253.62":{"start":"4253.62","dur":"4.456","text":"So the question is, don&#39;t you have\nwaterfall effects if you get something"},"4258.08":{"start":"4258.076","dur":"4.044","text":"wrong high up that&#39;ll destroy\neverything else further down?"},"4262.12":{"start":"4262.12","dur":"2.23","text":"You do get some of that."},"4264.35":{"start":"4264.35","dur":"6.26","text":"Because, yes, one decision will\nprevent some other decisions."},"4270.61":{"start":"4270.61","dur":"1.81","text":"It&#39;s typically not so bad."},"4272.42":{"start":"4272.42","dur":"3.8","text":"Because even if you mis-attach something\nlike a prepositional phrase attachment."},"4276.22":{"start":"4276.22","dur":"3.58","text":"You can still get right all of\nthe attachments inside noun"},"4279.80":{"start":"4279.8","dur":"1.94","text":"phrase that&#39;s inside that\nprepositional phrase."},"4281.74":{"start":"4281.74","dur":"1.49","text":"So it&#39;s not so bad."},"4283.23":{"start":"4283.23","dur":"3.05","text":"And I mean actually dependency parsing"},"4286.28":{"start":"4286.28","dur":"4.56","text":"evaluation suffers much less\nbadly from waterfall effects."},"4290.84":{"start":"4290.84","dur":"4.01","text":"Than doing CFG parsing which\nis worse in that respect."},"4295.91":{"start":"4295.91","dur":"0.96","text":"So it&#39;s not so bad."},"4298.54":{"start":"4298.54","dur":"5.569","text":"Okay, I had one slide there\nwhich I think I should skip."},"4309.46":{"start":"4309.461","dur":"5.055","text":"Okay I&#39;ll skip on to Neural ones."},"4314.52":{"start":"4314.516","dur":"6.164","text":"Okay, so, people could build quite good"},"4320.68":{"start":"4320.68","dur":"5.16","text":"machine learning dependency parsers on\nthese kind of categorical features."},"4325.84":{"start":"4325.84","dur":"3.178","text":"But nevertheless,\nthere was a problems of doing that."},"4329.02":{"start":"4329.018","dur":"5.342","text":"So, Problem #1 is the features\nwere just super sparse."},"4334.36":{"start":"4334.36","dur":"4.97","text":"That if you typically might have a tree\nbank that is an order about a million"},"4339.33":{"start":"4339.33","dur":"4.78","text":"words, and if you&#39;re then trying\nto train 15 million features,"},"4344.11":{"start":"4344.11","dur":"3.47","text":"which are kinda different\ncombinations of configurations."},"4347.58":{"start":"4347.58","dur":"4.11","text":"Not surprisingly, a lot of those\nconfigurations, you&#39;ve seen once or twice."},"4351.69":{"start":"4351.69","dur":"4.02","text":"So, you just don&#39;t have any\naccurate model of what happens in"},"4355.71":{"start":"4355.71","dur":"1.21","text":"different configurations."},"4356.92":{"start":"4356.92","dur":"3.28","text":"You just kind of getting these\nweak feature weights, and"},"4360.20":{"start":"4360.2","dur":"2.1","text":"crossing your fingers and\nhoping for the best."},"4362.30":{"start":"4362.3","dur":"2.17","text":"Now, it turns out that\nmodern machine learning,"},"4364.47":{"start":"4364.47","dur":"1.92","text":"crossing your fingers works pretty well."},"4366.39":{"start":"4366.39","dur":"2.99","text":"But, nevertheless,\nyou&#39;re suffering a lot from sparsity."},"4370.73":{"start":"4370.73","dur":"3.43","text":"Okay, the second problem is,\nyou also have an incompleteness problem,"},"4374.16":{"start":"4374.16","dur":"4","text":"because lots of configurations\nyou&#39;ll see it run time, will be"},"4378.16":{"start":"4378.16","dur":"4.19","text":"different configurations that you just\nnever happened to see the configuration."},"4382.35":{"start":"4382.35","dur":"2.86","text":"When exquisite was the second\nword on the stack, and"},"4385.21":{"start":"4385.21","dur":"5.18","text":"the top word of the stack,\nspeech, or something."},"4390.39":{"start":"4390.39","dur":"3.52","text":"Any kind of word pale,\nI&#39;ve only seen a small fraction of them."},"4393.91":{"start":"4393.91","dur":"2.1","text":"Lot&#39;s of things you\ndon&#39;t have features for."},"4396.01":{"start":"4396.01","dur":"2.4","text":"The third one is a little bit surprising."},"4398.41":{"start":"4398.41","dur":"5.07","text":"It turned out that when you looked at\nthese symbolic dependency parsers,"},"4403.48":{"start":"4403.48","dur":"2.43","text":"and you ask what made them slow."},"4405.91":{"start":"4405.91","dur":"4.07","text":"What made them slow\nwasn&#39;t running your SVM,"},"4409.98":{"start":"4409.98","dur":"4.82","text":"or your dot products in your logistic\nregression, or things like that."},"4414.80":{"start":"4414.8","dur":"3.03","text":"All of those things were really fast."},"4417.83":{"start":"4417.83","dur":"4.66","text":"What these parsers were ending up\nspending 95% of their time doing"},"4422.49":{"start":"4422.49","dur":"4.16","text":"is just computing these features, and\nlooking up their weights because you"},"4426.65":{"start":"4426.65","dur":"4.12","text":"had to sort of walk around the stack and\nthe buffer and sort of put together."},"4430.77":{"start":"4430.77","dur":"3.93","text":"A feature name, and then you had to\nlook it up in some big hash table to"},"4434.70":{"start":"4434.7","dur":"2.22","text":"get a feature number and a weight for it."},"4436.92":{"start":"4436.92","dur":"2.48","text":"And all the time is going on that, so"},"4439.40":{"start":"4439.4","dur":"4.798","text":"even though there are linear time,\nthat slowed them down a ton."},"4444.20":{"start":"4444.198","dur":"4.662","text":"So, in a paper in 2014 Danqi and"},"4448.86":{"start":"4448.86","dur":"3.26","text":"I developed this alternative\nwhere we said well,"},"4452.12":{"start":"4452.12","dur":"4.25","text":"let&#39;s just replace that all\nwith a neural net classifier."},"4456.37":{"start":"4456.37","dur":"4.571","text":"So that way, we can have a dense\ncompact feature representation and"},"4460.94":{"start":"4460.941","dur":"1.451","text":"do classification."},"4462.39":{"start":"4462.392","dur":"4.409","text":"So, rather than having our 10\nmillion categorical features,"},"4466.80":{"start":"4466.801","dur":"4.249","text":"we&#39;ll have a relatively modest\nnumber of dense features, and"},"4471.05":{"start":"4471.05","dur":"2.901","text":"we&#39;ll use that to decide our next action."},"4473.95":{"start":"4473.951","dur":"3.119","text":"And so, I want to spend the last\nfew minutes sort of showing"},"4477.07":{"start":"4477.07","dur":"4.025","text":"you how that works, and this is basically\nquestion two of the assignment."},"4482.71":{"start":"4482.71","dur":"5.58","text":"Okay, and basically, just to give you\nthe headline, this works really well."},"4488.29":{"start":"4488.29","dur":"3.63","text":"So, this was sort of the outcome\nthe first Parser MaltParser."},"4491.92":{"start":"4491.92","dur":"2.09","text":"So, it has pretty good UAS and"},"4494.01":{"start":"4494.01","dur":"4.035","text":"LAS and it had this advantage,\nthat it was really fast."},"4498.05":{"start":"4498.045","dur":"4.26","text":"When I said that&#39;s been the preferred\nmethod, I give you some contrast in gray."},"4502.31":{"start":"4502.305","dur":"2.7","text":"So, these are two of\nthe graph base parsers."},"4505.01":{"start":"4505.005","dur":"4.385","text":"So, the graph based parsers have\nbeen somewhat more accurate, but"},"4509.39":{"start":"4509.39","dur":"2.98","text":"they were kind of like two\norders in magnitude slower."},"4512.37":{"start":"4512.37","dur":"3.53","text":"So, if you didn&#39;t wanna parse much stuff\nthan you wanted accuracy, you&#39;d use them."},"4515.90":{"start":"4515.9","dur":"3.43","text":"But if you wanted to parse the web,\nno one use them."},"4519.33":{"start":"4519.33","dur":"4.46","text":"And so,\nthe cool thing was that by doing this as"},"4523.79":{"start":"4523.79","dur":"4.65","text":"neural network dependency parser,\nwe were able to get much better accuracy."},"4528.44":{"start":"4528.44","dur":"4.115","text":"We were able to get accuracy that\nwas virtually as good as the best,"},"4532.56":{"start":"4532.555","dur":"3.035","text":"graph-based parsers at that time."},"4535.59":{"start":"4535.59","dur":"4.29","text":"And we were actually about to build\na parser that works significantly"},"4539.88":{"start":"4539.88","dur":"4.38","text":"faster than MaltParser, because of\nthe fact that it wasn&#39;t spending"},"4544.26":{"start":"4544.26","dur":"2.95","text":"all this time doing feature combination."},"4547.21":{"start":"4547.21","dur":"2.49","text":"It did have to do more\nvector matrix multiplies,"},"4549.70":{"start":"4549.7","dur":"2.09","text":"of course, but that&#39;s a different story."},"4552.88":{"start":"4552.88","dur":"1.31","text":"Okay, so how did we do it?"},"4554.19":{"start":"4554.19","dur":"3.49","text":"Well, so, our starting point was\nthe two tools we have, right?"},"4557.68":{"start":"4557.68","dur":"1.58","text":"Distributed representation."},"4559.26":{"start":"4559.26","dur":"4.03","text":"So, we&#39;re gonna use distributed\nrepresentations of words."},"4563.29":{"start":"4563.29","dur":"4.16","text":"So, similar words have close by vectors,\nwe&#39;ve seen all of that."},"4567.45":{"start":"4567.45","dur":"4.883","text":"We&#39;re also going to use part, in our POS,\nwe use part-of-speech tags and"},"4572.33":{"start":"4572.333","dur":"1.389","text":"dependency labels."},"4573.72":{"start":"4573.722","dur":"3.851","text":"And we also learned distributed\nrepresentations for those."},"4577.57":{"start":"4577.573","dur":"1.692","text":"That&#39;s kind of a cool idea,"},"4579.27":{"start":"4579.265","dur":"4.665","text":"cuz it&#39;s also the case that parts of\nspeech some are more related than others."},"4583.93":{"start":"4583.93","dur":"3.41","text":"So, if you have a fine grain\npart-of-speech set where you have"},"4587.34":{"start":"4587.34","dur":"3.87","text":"plural nouns and proper names as\ndifferent parts of speech from nouns,"},"4591.21":{"start":"4591.21","dur":"3.18","text":"singular, you want to say\nthat they are close together."},"4594.39":{"start":"4594.39","dur":"5.33","text":"So, we also had distributed\nrepresentations for those."},"4599.72":{"start":"4599.72","dur":"3.502","text":"So now,\nwe have the same kind of configuration."},"4603.22":{"start":"4603.222","dur":"4.578","text":"We&#39;re gonna run exactly the same\ntransition based dependency parser."},"4607.80":{"start":"4607.8","dur":"3.44","text":"So, the configuration\nis no different at all."},"4611.24":{"start":"4611.24","dur":"4.64","text":"But what we&#39;re going to extract\nfrom it is the starting point."},"4615.88":{"start":"4615.88","dur":"2.29","text":"We extract certain positions,"},"4618.17":{"start":"4618.17","dur":"4.63","text":"just like Nivre&#39;s MaltParser but\nthen what we&#39;re gonna do is,"},"4622.80":{"start":"4622.8","dur":"5.32","text":"for each of these positions, like top of\nstack, second top of stack, buffer etc."},"4628.12":{"start":"4628.12","dur":"4.49","text":"We&#39;re then going to look then\nup in our bedding matrix, and"},"4632.61":{"start":"4632.61","dur":"2.12","text":"come up with a dense representation."},"4634.73":{"start":"4634.73","dur":"3.18","text":"So, you might be representing\nwords as sort of a 50 or"},"4637.91":{"start":"4637.91","dur":"5.49","text":"100 dimensional word vector representation\nof the kind that we&#39;ve talked about."},"4643.40":{"start":"4643.4","dur":"4.54","text":"And so, we get those representations for\nthe different words as vectors, and"},"4647.94":{"start":"4647.94","dur":"5.8","text":"then what we&#39;re gonna do is just\nconcatenate those into one longer vector."},"4653.74":{"start":"4653.74","dur":"3.54","text":"So, any configuration of the parser\nis just being represented as"},"4657.28":{"start":"4657.28","dur":"1.38","text":"the longest vector."},"4658.66":{"start":"4658.66","dur":"1.326","text":"Well, perhaps not that long,"},"4659.99":{"start":"4659.986","dur":"3.116","text":"our vectors are sort of more\naround 1,000 not 10 million, yeah."},"4671.43":{"start":"4671.426","dur":"2.006","text":"Sorry, the dependency of, right,"},"4673.43":{"start":"4673.432","dur":"3.748","text":"the question is what&#39;s this\ndependency on feeding as an input?"},"4677.18":{"start":"4677.18","dur":"2.64","text":"The dependency I&#39;m feeding\nhere as an import,"},"4679.82":{"start":"4679.82","dur":"5.66","text":"is when I previously built some arcs\nthat are in my arc set, I&#39;m thinking"},"4685.48":{"start":"4685.48","dur":"5.07","text":"maybe it&#39;ll be useful to use those arcs as\nwell, to help predict the next decision."},"4690.55":{"start":"4690.55","dur":"5.69","text":"So, I&#39;m using previous decisions on arcs\nas well to predict my follow-up decisions."},"4697.79":{"start":"4697.79","dur":"1.69","text":"Okay, so how do I do this?"},"4699.48":{"start":"4699.48","dur":"4.78","text":"And this is essentially what\nyou guys are gonna build."},"4704.26":{"start":"4704.26","dur":"4.35","text":"From my configuration,\nI take things out of it."},"4708.61":{"start":"4708.61","dur":"4.46","text":"I get there embedding representations, and"},"4713.07":{"start":"4713.07","dur":"5.58","text":"I can concatenate them together,\nand that&#39;s my input layer."},"4718.65":{"start":"4718.65","dur":"4.827","text":"I then run that through a hidden\nlayer Is a neural network,"},"4723.48":{"start":"4723.477","dur":"4.883","text":"feedforward neural network,\nI then have, from the hidden layer,"},"4728.36":{"start":"4728.36","dur":"4.06","text":"I&#39;ve run that through a Softmax layer,\nand I get an output layer,"},"4732.42":{"start":"4732.42","dur":"6.81","text":"which is a probability distribution of my\ndifferent actions in the standard Softmax."},"4739.23":{"start":"4739.23","dur":"3.47","text":"And of course, I don&#39;t know what\nany of these numbers are gonna be."},"4742.70":{"start":"4742.7","dur":"4.12","text":"So, what I&#39;m gonna be doing is I&#39;m going\nto be using cross-entropy error, and"},"4746.82":{"start":"4746.82","dur":"3.85","text":"then back-propagating\ndown to learn things."},"4750.67":{"start":"4750.67","dur":"5.218","text":"And this is the whole model,\nand it learns super well,"},"4755.89":{"start":"4755.888","dur":"4.203","text":"and it produces a great dependency parser."},"4760.09":{"start":"4760.091","dur":"3.68","text":"I&#39;m running a tiny bit short of time,\nbut let me just,"},"4763.77":{"start":"4763.771","dur":"3.532","text":"I think I&#39;ll have to rush this but\nI&#39;ll just say it."},"4767.30":{"start":"4767.303","dur":"4.827","text":"So, non-linearities, we&#39;ve mentioned\nnon-linearities a little bit."},"4772.13":{"start":"4772.13","dur":"3.91","text":"We haven&#39;t said very much about them, and"},"4776.04":{"start":"4776.04","dur":"4.05","text":"I just want to say a couple more\nsentences on non-linearities."},"4780.09":{"start":"4780.09","dur":"1.57","text":"Something like a softmax."},"4781.66":{"start":"4781.66","dur":"4.52","text":"You can say that using a logistic function\ngives you a probability distribution."},"4786.18":{"start":"4786.18","dur":"3.5","text":"And that&#39;s kind of what you get in\ngeneralized linear models and statistics."},"4789.68":{"start":"4789.68","dur":"2.54","text":"In general, though, you want to say that."},"4792.22":{"start":"4792.22","dur":"2.429","text":"For neural networks."},"4794.65":{"start":"4794.649","dur":"4.562","text":"Having these non-linearities sort of\nlet&#39;s us do function approximation by"},"4799.21":{"start":"4799.211","dur":"4.319","text":"putting together these various\nneurons that have some non-linearity."},"4803.53":{"start":"4803.53","dur":"4.06","text":"We can sorta put together little\npieces like little wavelets to do"},"4807.59":{"start":"4807.59","dur":"1.75","text":"functional approximation."},"4809.34":{"start":"4809.34","dur":"6.21","text":"And the crucial thing to notice is you\nhave to use some non-linearity, right?"},"4815.55":{"start":"4815.55","dur":"4.52","text":"Deep networks are useless unless you put\nsomething in between the layers, right?"},"4820.07":{"start":"4820.07","dur":"4.664","text":"If you just have multiple linear layers\nthey could just be collapsed down into one"},"4824.73":{"start":"4824.734","dur":"3.869","text":"linear layer that the sort of\nproduct of linear transformations,"},"4828.60":{"start":"4828.603","dur":"3.633","text":"affine transformations is just\nan affine transformation."},"4832.24":{"start":"4832.236","dur":"3.708","text":"So deep networks without\nnon-linearities do nothing, okay?"},"4835.94":{"start":"4835.944","dur":"4.656","text":"And so we&#39;ve talked about\nlogistic non-linearities."},"4840.60":{"start":"4840.6","dur":"5.33","text":"A second very commonly used\nnon-linearity is the tanh non-linearity,"},"4845.93":{"start":"4845.93","dur":"4.28","text":"which is tanh is normally\nwritten a bit differently."},"4850.21":{"start":"4850.21","dur":"5.25","text":"But if you sort of actually do your\nlittle bit of math, tanh is really"},"4855.46":{"start":"4855.46","dur":"5.99","text":"the same as a logistic, just sort of\nstretched and moved a little bit."},"4861.45":{"start":"4861.45","dur":"5.56","text":"And so tanh has the advantage that\nit&#39;s sort of symmetric around zero."},"4867.01":{"start":"4867.01","dur":"4.041","text":"And so that often works a lot better\nif you&#39;re putting it in the middle"},"4871.05":{"start":"4871.051","dur":"1.194","text":"of a new neural net."},"4872.25":{"start":"4872.245","dur":"3.48","text":"But in the example I showed you earlier,\nand for"},"4875.73":{"start":"4875.725","dur":"4.09","text":"what you guys will be using for\nthe dependency parser,"},"4879.82":{"start":"4879.815","dur":"5.408","text":"the suggestion to use for the first\nlayer is this linear rectifier layer."},"4885.22":{"start":"4885.223","dur":"3.932","text":"And linear rectifier\nnon-linearities are kind of freaky."},"4889.16":{"start":"4889.155","dur":"2.91","text":"They&#39;re not some interesting curve at all."},"4892.07":{"start":"4892.065","dur":"4.6","text":"Linear rectifiers just map things\nto zero if they&#39;re negative, and"},"4896.67":{"start":"4896.665","dur":"2.565","text":"then linear If they&#39;re positive."},"4899.23":{"start":"4899.23","dur":"4.17","text":"And when these were first introduced,\nI thought these were kind of crazy."},"4903.40":{"start":"4903.4","dur":"3.84","text":"I couldn&#39;t really believe that these\nwere gonna work and do anything useful."},"4907.24":{"start":"4907.24","dur":"3.01","text":"But they&#39;ve turned out to\nbe super successful, so"},"4910.25":{"start":"4910.25","dur":"5.04","text":"in the middle of neural networks, these\ndays often the first thing you try and"},"4915.29":{"start":"4915.29","dur":"6.93","text":"often what works the best is what&#39;s called\nReLU, which is rectified linear unit."},"4922.22":{"start":"4922.22","dur":"4.09","text":"And they just sort of effectively\nhave these nice properties where"},"4926.31":{"start":"4926.31","dur":"3.8","text":"if you&#39;re on the positive\nside the slope is just 1."},"4930.11":{"start":"4930.11","dur":"5.11","text":"Which means that they transmit\nerror in the back propagation step"},"4935.22":{"start":"4935.22","dur":"3.97","text":"really well linearly back\ndown through the network."},"4939.19":{"start":"4939.19","dur":"3.94","text":"And if they go negative that gives enough\nof a non-linearity that they&#39;re just"},"4943.13":{"start":"4943.13","dur":"2.95","text":"sort of being turned off\nin certain configurations."},"4946.08":{"start":"4946.08","dur":"4.22","text":"And so these really non-linearities\nhave just been super, super successful."},"4950.30":{"start":"4950.3","dur":"4.65","text":"And that&#39;s what we suggest that\nyou use in the dependency parser."},"4956.59":{"start":"4956.59","dur":"3.19","text":"Okay, so I should stop now."},"4959.78":{"start":"4959.78","dur":"4.51","text":"But this kind of putting a neural\nnetwork into a transition based"},"4964.29":{"start":"4964.29","dur":"3.18","text":"parser was just a super successful idea."},"4967.47":{"start":"4967.47","dur":"5.28","text":"So if any of you heard about the Google\nannouncements of Parsey McParseface."},"4972.75":{"start":"4972.75","dur":"4.7","text":"And SyntaxNet for their kind of\nopen source dependency parser."},"4977.45":{"start":"4977.45","dur":"3.01","text":"It&#39;s essentially exactly\nthe same idea of this."},"4980.46":{"start":"4980.46","dur":"3.92","text":"Just done with a bigger scaled up,\nbetter optimized neural network."},"4984.38":{"start":"4984.38","dur":"0.74","text":"Okay, thanks a lot."}}