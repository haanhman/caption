{"0.86":{"start":"0.859","dur":"5.081","text":"It\u2019s an interesting top position that I\nfind myself in being the first plenary speaker"},"5.94":{"start":"5.94","dur":"7.25","text":"but also having a hard act to follow in that\nI\u2019m straight after Susan\u2019s pre-conference"},"13.19":{"start":"13.19","dur":"4.92","text":"plenary last night which I know a lot of you\nwere at but not all of you. For those of you"},"18.11":{"start":"18.11","dur":"4.55","text":"who weren\u2019t allow me to give you a summary\nof Susan\u2019s key takeaway points, Hunston"},"22.66":{"start":"22.66","dur":"6.25","text":"2017 that\u2019s the talk last night argues convincingly\nfor the centrality of dinosaur clipart in"},"28.91":{"start":"28.91","dur":"6.87","text":"corpus-linguistic academic discourse and I\nwas convinced when the spirit of the innovation"},"35.78":{"start":"35.78","dur":"4.7","text":"the extinct animal mascot for this presentation\nwas a Gigantopithecus blacki, for those of"},"40.48":{"start":"40.48","dur":"6.09","text":"you do don\u2019t know what a Gigantopithecus\nis, this is a Gigantopithecus, if you\u2019re"},"46.57":{"start":"46.57","dur":"5.1","text":"saying to yourself \u201cby gum that looks like\nan orangutan that\u2019s been caught by the wrong"},"51.67":{"start":"51.67","dur":"5.09","text":"end of shrink ray\u201d, well yes that\u2019s exactly\nthat it is, it\u2019s exactly that."},"56.76":{"start":"56.76","dur":"5.069","text":"So that\u2019s a Gigantopithecus, it went extinct\nabout 100,000 years ago and the spirit of"},"61.83":{"start":"61.829","dur":"3.871","text":"the Gigantopithecus will be haunting my talk\nmuch as the dinosaurs did Susan\u2019s, whenever"},"65.70":{"start":"65.7","dur":"6.98","text":"you see the Gigantopithecus appear that\u2019s\na key point."},"72.68":{"start":"72.68","dur":"9.2","text":"Now onto the introduction that I had before\nI saw Susan\u2019s talk yesterday, ok, so, this"},"81.88":{"start":"81.88","dur":"7.32","text":"as Stephan mentioned in his very flattering\nintroduction I think a lot about methodology,"},"89.20":{"start":"89.2","dur":"6.59","text":"and one of the things about methodology that\nseems to be at the core of the recent interest"},"95.79":{"start":"95.79","dur":"6.71","text":"in topic modelling as well as a whole range\nof other types of methods that have been used"},"102.50":{"start":"102.5","dur":"4.13","text":"over the years is the issue of multidimensional\nspaces especially when we\u2019re dealing with"},"106.63":{"start":"106.63","dur":"6.18","text":"wordform frequency and I will explain what\nI mean by multidimensional in a minute. And"},"112.81":{"start":"112.81","dur":"5.3","text":"LDA, Latant Dirichlet Analysis which is the\nmost commonly used form of top modelling these"},"118.11":{"start":"118.11","dur":"7.48","text":"days is often as we\u2019ve seen not only from\nSusie yesterday but also from assorted papers"},"125.59":{"start":"125.59","dur":"5.8","text":"that have come out over the past few years,\nit\u2019s the most commonly used form of top"},"131.39":{"start":"131.39","dur":"5.11","text":"modelling, it\u2019s often put forward as a solution\nto that problem. It\u2019s used in digital humanities"},"136.50":{"start":"136.5","dur":"4.7","text":"and now as we\u2019ve seen it\u2019s started to\ncome into linguistics as well by some pioneering"},"141.20":{"start":"141.2","dur":"4.82","text":"efforts, so I\u2019m going to talk about these\nissues and then I\u2019m actually going to round"},"146.02":{"start":"146.02","dur":"5.51","text":"about fifteen minutes in, I\u2019m going to transition\ninto presenting a worked example of this kind"},"151.53":{"start":"151.53","dur":"7.15","text":"of analysis that I have done, and I will work\nmy way through this to the question of what"},"158.68":{"start":"158.68","dur":"6.52","text":"else could we do and round up with some final\nthoughts which are not, I will spoil the entire"},"165.20":{"start":"165.2","dur":"7.98","text":"thing, my final thoughts will not be 100%\nfavourable towards LDA, so if that\u2019s what"},"173.18":{"start":"173.18","dur":"4.01","text":"you wanted to know whether I like it or not\nthen you can feel free to leave you know that\u2019s"},"177.19":{"start":"177.19","dur":"1.95","text":"it that\u2019s the end."},"179.14":{"start":"179.14","dur":"5.22","text":"An additional disclaimer regarding the discourse\npragmatics of the preposition \u201ctowards\u201d"},"184.36":{"start":"184.36","dur":"5.29","text":"in academic plenary titling contexts towards\na critical evaluation in my title means this"},"189.65":{"start":"189.65","dur":"5.85","text":"is not a complete critical evaluation it\u2019s\njust you know, going in that direction, so"},"195.50":{"start":"195.5","dur":"3.849","text":"no claim that this is a complete critical\nanalysis."},"199.35":{"start":"199.349","dur":"8.661","text":"Ok so, dimensions, dimensionality of data,\nagain I know I\u2019ve got a very broad audience"},"208.01":{"start":"208.01","dur":"5.1","text":"here in terms of how specialised people are\nin maths and statistics so for some of you"},"213.11":{"start":"213.11","dur":"4.37","text":"this will be new and for some of you this\nwill be a ridiculous over simplification to"},"217.48":{"start":"217.48","dur":"4.19","text":"the latter group I apologise, but let\u2019s\nthink of it this way, let\u2019s say we\u2019ve"},"221.67":{"start":"221.67","dur":"6.6","text":"got a corpus of six texts and we look at the\nfrequency of \u201ca\u201d and the frequency of"},"228.27":{"start":"228.27","dur":"5.23","text":"\u201cthe\u201d in each of the text, right the frequency\nof \u201cthe\u201d the frequency of \u201ca\u201d and"},"233.50":{"start":"233.5","dur":"4.82","text":"we\u2019re interested to see if these two things\ncorrespond, I mean intuitively you might say"},"238.32":{"start":"238.32","dur":"4.16","text":"probably they would if you\u2019ve got lots of\nnoun phrases for instance you\u2019ll have lots"},"242.48":{"start":"242.48","dur":"3.89","text":"of \u201cthe\u201d and lots of \u201ca\u201d, if you have\nfew noun phrases then you\u2019ll have fewer,"},"246.37":{"start":"246.37","dur":"5.31","text":"so we might expect them to kind of trend together\nand so you could plot your frequency of \u201ca\u201d"},"251.68":{"start":"251.68","dur":"5.82","text":"along here, you could plot your frequency\nof \u201cthe\u201d up there, and you could put each"},"257.50":{"start":"257.5","dur":"6.159","text":"text at a dot indicating where it is, so text\none is here that frequency of \u201ca\u201d that"},"263.66":{"start":"263.659","dur":"5.4","text":"frequency of \u201cthe\u201d ok so hopefully we\u2019re\nnow, still at the level of like you know pre-GCSE"},"269.06":{"start":"269.059","dur":"2.89","text":"Maths so everyone\u2019s following me."},"271.95":{"start":"271.949","dur":"4.491","text":"And we can see that there\u2019s actually some\nkind of trend there of course there is because"},"276.44":{"start":"276.44","dur":"3.629","text":"I\u2019ve made up this data to illustrate the\npoint and so I could make up whatever I liked"},"280.07":{"start":"280.069","dur":"5.231","text":"so I made up a trend, but we can see that\nthe two things go together, but you could"},"285.30":{"start":"285.3","dur":"4.769","text":"ask well but what if we think about other\nthings that come at the start of noun phrases"},"290.07":{"start":"290.069","dur":"5.451","text":"like for example the frequency of \u201csome\u201d,\nargh well it\u2019s easy you just add another"},"295.52":{"start":"295.52","dur":"6.549","text":"dimension so now instead of a two way graph\nwe\u2019ve got a three way graph where that represents"},"302.07":{"start":"302.069","dur":"4.19","text":"the frequency of \u201cthe\u201d that represents\nthe frequency of \u201ca\u201d and then going backwards"},"306.26":{"start":"306.259","dur":"5.371","text":"represents the frequency of \u201csome\u201d and\nyou\u2019ve got this kind of off angle third"},"311.63":{"start":"311.63","dur":"4.489","text":"dimension going backwards, yeah, easy."},"316.12":{"start":"316.119","dur":"4.85","text":"But what if we want to look at \u201cthis\u201d\nand \u201cthat\u201d and so on and so forth, well"},"320.97":{"start":"320.969","dur":"4.891","text":"we\u2019re in trouble then because the human\nspaceial visualisation system isn\u2019t very"},"325.86":{"start":"325.86","dur":"6.019","text":"good at more than three dimensions, but theoretically\nif we\u2019re measuring things we can just keep"},"331.88":{"start":"331.879","dur":"3.69","text":"on adding dimensions couldn\u2019t we as long\nas there are words to add we can add more"},"335.57":{"start":"335.569","dur":"4.78","text":"dimensions, we could have say a dimension\nfor every type of determine there is, I don\u2019t"},"340.35":{"start":"340.349","dur":"5.181","text":"know how many there are let\u2019s say 15 that\u2019s\nI have no idea so we\u2019ve got a 15 dimensional"},"345.53":{"start":"345.53","dur":"6.379","text":"space measuring the frequencies of determinates\nand if you can visualise a 15 dimensional"},"351.91":{"start":"351.909","dur":"5","text":"space I guess you\u2019re in the wrong profession,\nthe pure mathematicians are recruiting."},"356.91":{"start":"356.909","dur":"13.32","text":"So, yeah, we can\u2019t really do the graph thing,\nand what if, what if, instead of just looking"},"370.23":{"start":"370.229","dur":"5.72","text":"at determiners you say I want to look at all\nwords, I want to look at every word type in"},"375.95":{"start":"375.949","dur":"7.45","text":"the corpus measure it\u2019s frequency and compare\nthat in my graph of spaces, well that means"},"383.40":{"start":"383.399","dur":"6.55","text":"that you\u2019ve got a dimension in your space\nfor the frequency of every type in your corpus"},"389.95":{"start":"389.949","dur":"5.041","text":"so here are a few corpora that I happen to\nhave available on CQP web, I have a secret"},"394.99":{"start":"394.99","dur":"3.709","text":"view that no-one else has which lists all\nthe type counts in all the corpora so I just"},"398.70":{"start":"398.699","dur":"5.99","text":"got that from there, so there\u2019s a few of\nthese here and you can see that the very,"},"404.69":{"start":"404.689","dur":"5.111","text":"the very smallest number of types comes from\nthe American English 2006 Corpus which are"},"409.80":{"start":"409.8","dur":"6.78","text":"the two greatest there Amanda and Paul they\u2019re\nhere, so 53,000, 54,000 types, that means"},"416.58":{"start":"416.58","dur":"4.169","text":"that if you use, the frequency of each of\nthose as something you want to put on one"},"420.75":{"start":"420.749","dur":"7.45","text":"of these multidimensional graphs you\u2019ve\ngot a 54,000 dimensional space. I don\u2019t"},"428.20":{"start":"428.199","dur":"3.791","text":"think even you know like string theorists\nhave that many dimensions I think they usually"},"431.99":{"start":"431.99","dur":"6.37","text":"top out at about 11, so ok so we\u2019ve got\n54,000 dimensions if we have 54,000 types"},"438.36":{"start":"438.36","dur":"5.76","text":"but bringing in big data, because we just\nheard about it, well big data isn\u2019t actually"},"444.12":{"start":"444.12","dur":"4.99","text":"isn\u2019t really like AmE2006 or BE2006 not\nthe small data by modern standards, big data\u2019s"},"449.11":{"start":"449.11","dur":"5.479","text":"much more like the UK Weber\u2019s corpus 1 billion\nwords, that\u2019s a 50% sample the full thing"},"454.59":{"start":"454.589","dur":"9.591","text":"is 2 billion but in 1 billion words, six million\ntypes so we\u2019ve got 6 million 800,000 dimensions,"},"464.18":{"start":"464.18","dur":"4.469","text":"this gets ridiculously hard to manage, even\nif you had the kind of brain that could visualise"},"468.65":{"start":"468.649","dur":"3.83","text":"this how the hell are you going to analyse\nit, how the hell are you going to pick out"},"472.48":{"start":"472.479","dur":"5.701","text":"the correspondences that are easy to spot\nor not spot with two dimensions, may be even"},"478.18":{"start":"478.18","dur":"4.29","text":"three but beyond that it just becomes impossible."},"482.47":{"start":"482.47","dur":"6.659","text":"So what we can do is we can talk about these\nword frequency things as vectors and a vector"},"489.13":{"start":"489.129","dur":"6.57","text":"simply means a sequence of numbers each of\nwhich represents a distance on another dimension,"},"495.70":{"start":"495.699","dur":"6.511","text":"and when you\u2019re talking about word frequencies\nas the dimensions then the distance along"},"502.21":{"start":"502.21","dur":"4.949","text":"that dimension is the frequency of the word\nright, so the frequency of the word gives"},"507.16":{"start":"507.159","dur":"5.091","text":"you basically a long set of coordinates so\nlet\u2019s say we count the frequency of all"},"512.25":{"start":"512.25","dur":"8.199","text":"our word types and then we take our regular\nnormalised frequencies for that in text one"},"520.45":{"start":"520.449","dur":"5.25","text":"and we get our numbers and we go right the\nway down the alphabet so \u201cA\u201d is 0.03 because"},"525.70":{"start":"525.699","dur":"2.7","text":"it\u2019s \u201cA\u201d it\u2019 really frequent and then\nyou\u2019ve got \u201cAbacus\u201d \u201cAlien\u201d \u201cAntelope\u201d"},"528.40":{"start":"528.399","dur":"5.98","text":"\u201cAardvark\u201d so on and so forth right the\nway down the alphabet, that gives us a vector"},"534.38":{"start":"534.379","dur":"6.201","text":"which positions the text or corpus in the\n50,000 dimensional space, ok, so just like"},"540.58":{"start":"540.58","dur":"4.559","text":"an \u201cx\u201d \u201cy\u201d coordinate two numbers\npositions you in two dimensional space, these"},"545.14":{"start":"545.139","dur":"3.99","text":"50,000 numbers position this text in that\nspace."},"549.13":{"start":"549.129","dur":"4.541","text":"And then we can have another vector for each\nof the texts that we are looking at so we\u2019ve"},"553.67":{"start":"553.67","dur":"4.56","text":"got a vector a vector, a vector, different\ntexts then have a vector that define them,"},"558.23":{"start":"558.23","dur":"5.339","text":"put them in a point in space, each number\nin the vector we can also call a feature frequency"},"563.57":{"start":"563.569","dur":"4.871","text":"and the features in this case are our word\nfrequencies, is everyone following along?"},"568.44":{"start":"568.44","dur":"4.66","text":"I know this is not necessarily the easiest\nconcept, but once you get it, it\u2019s fine."},"573.10":{"start":"573.1","dur":"4.109","text":"And then another thing to introduce from I\nthink I\u2019m up to \u201cA\u201d Level Maths here"},"577.21":{"start":"577.209","dur":"4.841","text":"rather than GCSE Maths, is the idea of a matrix\nfeatures can be more than just words that\u2019s"},"582.05":{"start":"582.05","dur":"4.149","text":"something I want to come back to and basically\nwhen you put your vectors together where you\u2019ve"},"586.20":{"start":"586.199","dur":"4.411","text":"got your type frequencies down, you\u2019ve got\nyour types down here and then your bi-text"},"590.61":{"start":"590.61","dur":"5.68","text":"frequencies across here you\u2019ve got yourself\na feature matrix, right so each word is a"},"596.29":{"start":"596.29","dur":"5.57","text":"feature the texts are then the data objects\nand you\u2019ve got a frequency score for each"},"601.86":{"start":"601.86","dur":"7.079","text":"one right across the way, so if you\u2019ve got\n50,000 words and let\u2019s say 2,000 texts,"},"608.94":{"start":"608.939","dur":"5.82","text":"that\u2019s 100,000 or a million I\u2019ve lost\ncount of the zeros anyway, a huge number of"},"614.76":{"start":"614.759","dur":"1","text":"numbers."},"615.76":{"start":"615.759","dur":"4.291","text":"Ok, so the question is how do we interpret\nthis because we want to know something about"},"620.05":{"start":"620.05","dur":"4.539","text":"how these different frequency features relate\nto one another how they define the texts in"},"624.59":{"start":"624.589","dur":"5.3","text":"this massive conceptual space, how do we actually\nanalyse this, well one of the first steps"},"629.89":{"start":"629.889","dur":"5.341","text":"is to say do we really want 50,000 dimensions\nand the answer is, well how many dimensions"},"635.23":{"start":"635.23","dur":"5.479","text":"is too many, well I don\u2019t know exactly where\nyou\u2019d cut it off but 50,000 is definitely"},"640.71":{"start":"640.709","dur":"4.211","text":"too many dimensions, so the first step that\nyou want to take with this kind of data a"},"644.92":{"start":"644.92","dur":"5.349","text":"feature matrix like that is to come up with\nsome way of saying let\u2019s recast it into"},"650.27":{"start":"650.269","dur":"4.941","text":"fewer dimensions lower complexity data."},"655.21":{"start":"655.21","dur":"4.27","text":"And so that\u2019s what I\u2019m calling that\u2019s\nwhat kind of inspired the title at the top,"},"659.48":{"start":"659.48","dur":"6.099","text":"wordform vectors, wordform matrixes and their\nfrequencies how do we deal with it? Well as"},"665.58":{"start":"665.579","dur":"6.771","text":"Susan was actually explaining in the pre-conference\ntalk yesterday, there are a variety of exploratory"},"672.35":{"start":"672.35","dur":"6.15","text":"methods and have been used over the years\nand many of these are essentially purely quantitative,"},"678.50":{"start":"678.5","dur":"5.579","text":"right, they are number crunching methods which\nuse the tendencies in the numbers that are"},"684.08":{"start":"684.079","dur":"5.851","text":"observed to group features which in this case\nmeans wordform frequencies to capture the"},"689.93":{"start":"689.93","dur":"3.29","text":"trends and the variation across this whole\nmatrix."},"693.22":{"start":"693.22","dur":"5.03","text":"So for instance if we observe that \u201cthe\u201d\nand \u201ca\u201d always get more frequent or less"},"698.25":{"start":"698.25","dur":"7.87","text":"frequent together as in my original graph\nwe can say, ah ha, those actually belong together,"},"706.12":{"start":"706.12","dur":"4.37","text":"they are not too different dimensions that\ncross but at right angles they\u2019re actually"},"710.49":{"start":"710.49","dur":"6.57","text":"moving in parallel so they\u2019re one dimension,\nso there we are we\u2019ve got down from 50,000"},"717.06":{"start":"717.06","dur":"4.23","text":"to 49,999 but the statistical number crunching\nmethods do this systematically and get us"},"721.29":{"start":"721.29","dur":"2.969","text":"down to a much simpler model by grouping things."},"724.26":{"start":"724.259","dur":"4.852","text":"So here are a few of these quantitative number\ncrunching methods that we know about, factor"},"729.11":{"start":"729.111","dur":"5.718","text":"analysis, anyone who\u2019s read Biber 1988 or\nmany of his others on multidimensional analysis"},"734.83":{"start":"734.829","dur":"5.12","text":"will know about factor analysis which reduces\nmany variables such as our word frequencies"},"739.95":{"start":"739.949","dur":"4.39","text":"to just a few factors and the best way to\nthink of what a factor is, it\u2019s a super"},"744.34":{"start":"744.339","dur":"5.29","text":"variable, it\u2019s a variable that has absorbed\nand conquered many lesser variables."},"749.63":{"start":"749.629","dur":"3.921","text":"Cluster analysis is another way of doing it,\nthis is where instead of grouping together"},"753.55":{"start":"753.55","dur":"5.63","text":"the dimensions you use all the dimensions\nat once to calculate distances for the objects"},"759.18":{"start":"759.18","dur":"4.59","text":"apart from one another, so how far away are\nthe texts taking into account all these dimensions"},"763.77":{"start":"763.77","dur":"5.679","text":"from one another and then based on those distances\nyou group them together into different clusters"},"769.45":{"start":"769.449","dur":"4.061","text":"and then you work out how those clusters are\nrelated by distance and eventually you assemble"},"773.51":{"start":"773.51","dur":"4.91","text":"a big tree of clusters of all your data objects,\nour data objects in this case remember are"},"778.42":{"start":"778.42","dur":"1.3","text":"our texts."},"779.72":{"start":"779.72","dur":"6.309","text":"And finally those two are kind of very widely\nused but then this brings us close towards"},"786.03":{"start":"786.029","dur":"5.06","text":"topic modelling we have assorted data mining\nmethods where we use some kind of machine"},"791.09":{"start":"791.089","dur":"5.17","text":"learning of a model to statistically summarise\nall the variables in some way."},"796.26":{"start":"796.259","dur":"3.481","text":"Now there is actually another one that we\ncould stick on there which would be linguistic"},"799.74":{"start":"799.74","dur":"6.68","text":"analysis of the variables, what do I mean\nby that, well, linguistic analysis of the"},"806.42":{"start":"806.42","dur":"4.87","text":"features, in this case the wordforms would\nbe to say use what we already know about language"},"811.29":{"start":"811.29","dur":"4.599","text":"to avoid this massive wordform vector in the\nfirst place, so for instance we know that"},"815.89":{"start":"815.889","dur":"3.851","text":"\u201cthe\u201d and \u201ca\u201d are articles, so why\nmeasure them separately to begin with, stick"},"819.74":{"start":"819.74","dur":"4.17","text":"them together because we know they belong\ntogether, so for instance this is how you"},"823.91":{"start":"823.91","dur":"7.44","text":"can use tagging to or lemmatisation to reduce\nthe number of features, so rather than 50,000"},"831.35":{"start":"831.35","dur":"5.88","text":"word type frequencies you\u2019ve got maybe a\n135 POS tag frequencies, or a couple of hundred"},"837.23":{"start":"837.23","dur":"5.12","text":"semantic tag frequencies, lemmatisation is\nsimilar although obviously for a language"},"842.35":{"start":"842.35","dur":"3.95","text":"like English which doesn\u2019t have very much\ninflection lemmatisation doesn\u2019t reduce"},"846.30":{"start":"846.3","dur":"3.96","text":"your dimensions all that much. But basically\nall of these have the same idea, we\u2019ve got"},"850.26":{"start":"850.26","dur":"5.699","text":"these ways of annotating our data which are\nbased on sophisticated amounts of linguistic"},"855.96":{"start":"855.959","dur":"3.411","text":"knowledge that have been built into the system\nby the people who created them so we could"},"859.37":{"start":"859.37","dur":"4.61","text":"use them to collapse our wordform vector down\ninto a more manageable vector of something"},"863.98":{"start":"863.98","dur":"1.969","text":"else."},"865.95":{"start":"865.949","dur":"8.39","text":"And this keys into a point that Susan was\nmaking yesterday when she described a topic"},"874.34":{"start":"874.339","dur":"5.87","text":"model I think was a na\u00efve linguist, a linguistically\nna\u00efve method, so essentially this then would"},"880.21":{"start":"880.209","dur":"6.93","text":"be your non-na\u00efve approach to addressing\nthe wordform frequencies, whereas the previous"},"887.14":{"start":"887.139","dur":"4.981","text":"set data mining, factor analysis, cluster\nanalysis, that would be all be na\u00efve because"},"892.12":{"start":"892.12","dur":"3.75","text":"it\u2019s purely chunked down as I say crunching\nthe numbers."},"895.87":{"start":"895.87","dur":"4.351","text":"So yes, but let\u2019s assume that we\u2019re putting\naside linguistic knowledge for now, and we"},"900.22":{"start":"900.221","dur":"7.199","text":"just want to do some non-number crunching,\nwe could do some machine learning, some data"},"907.42":{"start":"907.42","dur":"5.43","text":"mining and it could be unsupervised, in machine\nlearning, unsupervised has a specialised meaning,"},"912.85":{"start":"912.85","dur":"4.39","text":"it means that there isn\u2019t a target analysis\nthat we know is correct that we\u2019re trying"},"917.24":{"start":"917.24","dur":"5.639","text":"to get our machine learning system to produce,\ninstead we\u2019re trying to get it to produce"},"922.88":{"start":"922.879","dur":"3.981","text":"something and we don\u2019t necessarily know\nwhat until we get it."},"926.86":{"start":"926.86","dur":"4.599","text":"Topic modelling is an example of this and\nthis is where I have to put in one of my provisos"},"931.46":{"start":"931.459","dur":"5.451","text":"there\u2019s all kinds of different topic modelling\nthings, today I am not going to attempt to"},"936.91":{"start":"936.91","dur":"5.65","text":"even address any of them other than Latant\nDirichlet Analysis because there\u2019s just"},"942.56":{"start":"942.56","dur":"1","text":"not enough time."},"943.56":{"start":"943.56","dur":"3.779","text":"But you may have heard of something called\nSymantec Vector Space models if you\u2019ve been"},"947.34":{"start":"947.339","dur":"3.931","text":"listening to the kinds of people who talk\nabout these things it\u2019s very similar in"},"951.27":{"start":"951.27","dur":"2.629","text":"principle but different in detail."},"953.90":{"start":"953.899","dur":"6.25","text":"So yes Laten Dirichlet Analysis (LDA) what\nis it and why do we care? Well I\u2019m sure"},"960.15":{"start":"960.149","dur":"6.211","text":"you\u2019ve heard about topic modelling, LDA\nis one form of it as I\u2019ve said, topic modelling"},"966.36":{"start":"966.36","dur":"5.039","text":"is what I consider to be the epitome of the\n\u201cbig data\u201d approach because it\u2019s data"},"971.40":{"start":"971.399","dur":"5.36","text":"mining right it\u2019s in we go we just chunk\nwe crunch through our numbers roll, roll,"},"976.76":{"start":"976.759","dur":"5.421","text":"roll, roll, roll, something comes out at the\nend and you know it\u2019s great, it\u2019s machine"},"982.18":{"start":"982.18","dur":"4.86","text":"learning it\u2019s number crunching and it was\noriginally developed for computer science"},"987.04":{"start":"987.04","dur":"5.419","text":"purposes so for instance looking at product\nreviews on sites like Amazon, if you model"},"992.46":{"start":"992.459","dur":"3.35","text":"the topics you can find out what each review\nis talking about, that\u2019s what they were"},"995.81":{"start":"995.809","dur":"7.351","text":"doing, and it has become part of the standard\nmachine learning toolkit, but what you see"},"1003.16":{"start":"1003.16","dur":"4.561","text":"is that topic modelling once it gets away\nfrom the people who invented it, you see it"},"1007.72":{"start":"1007.721","dur":"5.428","text":"also being applied to relatively small amounts\nof data, and especially in humanities studies,"},"1013.15":{"start":"1013.149","dur":"6.591","text":"there\u2019s a trend and since I\u2019m at a CL\nConference rather than a humanities conference"},"1019.74":{"start":"1019.74","dur":"7.909","text":"I can say this, there is a trend when humanists,\nhistorians or literary scholars, when they"},"1027.65":{"start":"1027.649","dur":"4.791","text":"get to know, I should be doing something digital,\nso they go across to the computer scientist"},"1032.44":{"start":"1032.44","dur":"3.41","text":"and they say I\u2019ve got all these texts what\ncan I do, and the computer scientists say"},"1035.85":{"start":"1035.85","dur":"4.66","text":"ah ha, use topic modelling, there you are\ndone, problem solved."},"1040.51":{"start":"1040.51","dur":"7.03","text":"I\u2019ve seen this dynamic happen not necessarily\nexactly like that but I\u2019ve seen it happen,"},"1047.54":{"start":"1047.54","dur":"4.21","text":"any computer scientists in this room are excused\nfrom that over broad generalisation simply"},"1051.75":{"start":"1051.75","dur":"3.51","text":"by virtue of turning up at CL you have proven\nthat you\u2019re not the sort of person to do"},"1055.26":{"start":"1055.26","dur":"6.22","text":"that kind of thing, however there are lots\nof computer scientists out there who would."},"1061.48":{"start":"1061.48","dur":"7.23","text":"And so, there\u2019s been a big move in digital\nhumanities to look at topic modelling for"},"1068.71":{"start":"1068.71","dur":"6.76","text":"relatively small amounts of texts, the classic\none which you may have heard of was published"},"1075.47":{"start":"1075.47","dur":"7.08","text":"on line rather than in a journal, a study\nof a early 19th century midwife\u2019s diary"},"1082.55":{"start":"1082.55","dur":"6.45","text":"entries via topic modelling, and I\u2019ve forgotten\nthe surname of the guy who did it unfortunately,"},"1089.00":{"start":"1089","dur":"2.63","text":"and of course because it wasn\u2019t published\nI don\u2019t have a reference, if anyone wants"},"1091.63":{"start":"1091.63","dur":"5.19","text":"to find that then just let me know and I\u2019ll\ndig up the blog post for you."},"1096.82":{"start":"1096.82","dur":"4.43","text":"Anyway so it\u2019s spread kind of through the\ndigital humanities in this way for people"},"1101.25":{"start":"1101.25","dur":"4.21","text":"looking at their historical text collections\nit\u2019s also now being used in CL as we\u2019ve"},"1105.46":{"start":"1105.46","dur":"6.41","text":"seen with the Murakami et al paper which Susan\nintroduced yesterday, and I should say at"},"1111.87":{"start":"1111.87","dur":"4.71","text":"this point that if it sounds like I\u2019m being\ncritical I am not, I was, I\u2019ve read the"},"1116.58":{"start":"1116.58","dur":"4.12","text":"Murakami et al paper it\u2019s not out yet but\nI found it on a Birmingham Research Archive"},"1120.70":{"start":"1120.7","dur":"7.79","text":"Survey so I read it there and I thought this\nis the best topic modelling paper I have read"},"1128.49":{"start":"1128.49","dur":"4.49","text":"because all the topic modelling presentations\nthat I\u2019ve seen at digital humanities conferences"},"1132.98":{"start":"1132.98","dur":"6.51","text":"over the past few years I got ooopphh at right,\nbut unfortunately, sorry Susan there is kind"},"1139.49":{"start":"1139.49","dur":"4.79","text":"of a sting in the tale sorry Paul sorry everyone\nbut I think that even getting everything right"},"1144.28":{"start":"1144.28","dur":"3.54","text":"in topic modelling there\u2019s still some hard\nproblems that you run up against at the end"},"1147.82":{"start":"1147.82","dur":"2.38","text":"of the day, I\u2019m going to come to that."},"1150.20":{"start":"1150.2","dur":"5.43","text":"So how does discourse and topics \u201cwork\u201d\nin topic modelling and LDA what is a topic?"},"1155.63":{"start":"1155.63","dur":"4.69","text":"Well Susan said this yesterday it\u2019s not\na topic in the linguistic sense it\u2019s purely"},"1160.32":{"start":"1160.32","dur":"5.589","text":"a model in the sense of a group of weightings\nwhich satisfies a stochastic model, what does"},"1165.91":{"start":"1165.909","dur":"6.471","text":"that mean? Well it means that there is a kind\nof generative theory of how discourse works"},"1172.38":{"start":"1172.38","dur":"3.21","text":"which I shall now explain."},"1175.59":{"start":"1175.59","dur":"6.23","text":"Topic modelling works on the following assumptions,\na writer is someone who sits down and decides"},"1181.82":{"start":"1181.82","dur":"9","text":"my text will consist of words from this bin\nof words this bin of words, this bin of words"},"1190.82":{"start":"1190.82","dur":"4.81","text":"and this bin of words, and I\u2019ll take half\nmy text from this bin of words, half my text"},"1195.63":{"start":"1195.63","dur":"5.43","text":"from this bin of words and then the other\nbins not you know just one or two, so they"},"1201.06":{"start":"1201.06","dur":"6.13","text":"walk over to the bin and they pick out from\nthe bin as many words as they need so if they\u2019re"},"1207.19":{"start":"1207.19","dur":"7.42","text":"writing a 1,000 word text and they want 50%\nfrom this bin of words they pick out 500 words"},"1214.61":{"start":"1214.61","dur":"4.309","text":"from the bin at random, now all the words\nyou can imagine them being in a big urn written"},"1218.92":{"start":"1218.919","dur":"5.1","text":"on little scrabble tiles, right, and some\nof the words are duplicated right, so some"},"1224.02":{"start":"1224.019","dur":"6.331","text":"words appear in the bin a huge urn like hundreds\nand hundreds of times and other words occur"},"1230.35":{"start":"1230.35","dur":"4.15","text":"on only one tile so they\u2019re less likely\nto be pulled out, but the writer pulls out"},"1234.50":{"start":"1234.5","dur":"5.29","text":"500 tiles and then goes off to the other bins\nand they pick out 100 here, 300 here whatever,"},"1239.79":{"start":"1239.79","dur":"4.22","text":"and then they take them back to the desk dump\nthem on the table that\u2019s a text, that\u2019s"},"1244.01":{"start":"1244.01","dur":"7.299","text":"how writing works in this view, ok, so we\u2019ve\ngot bins our bins our urns are our topics"},"1251.31":{"start":"1251.309","dur":"5.441","text":"and writing is done by pulling randomly from\nbins in the proportions that were decided"},"1256.75":{"start":"1256.75","dur":"5.22","text":"on at the start, ok that\u2019s the theory of\nhow writing works. That\u2019s the generative"},"1261.97":{"start":"1261.97","dur":"1.57","text":"theory of discourse."},"1263.54":{"start":"1263.54","dur":"6.619","text":"The way topic model system works is alright\ngiven that that\u2019s how writers work we know"},"1270.16":{"start":"1270.159","dur":"7.231","text":"what the text looks like when they\u2019ve finished\nbased on number crunching can we work out"},"1277.39":{"start":"1277.39","dur":"7.59","text":"what words were in the bins to start with,\nso the topic model uses problemolistic algorithms"},"1284.98":{"start":"1284.98","dur":"5.69","text":"to come up with a best guess as to what word\ntiles are in each of these bins that would"},"1290.67":{"start":"1290.67","dur":"3.31","text":"have given rise to the text that the writer\ncame up with yeah."},"1293.98":{"start":"1293.98","dur":"7.64","text":"Now the way, oh dear, I\u2019ve gone passed one,\nyes so it works backwards, right, the idea"},"1301.62":{"start":"1301.62","dur":"5.08","text":"is we know the text and we want to know what\nthe bins are, the bins, I\u2019m saying bins"},"1306.70":{"start":"1306.7","dur":"3.42","text":"because I want to avoid the word topic for\nthe moment."},"1310.12":{"start":"1310.12","dur":"5.98","text":"Now the fact that it\u2019s working backwards\nis interesting it has implications which I\u2019m"},"1316.10":{"start":"1316.1","dur":"3.569","text":"going to come to in a minute but that\u2019s\nwhat topic modelling is, you give it the text,"},"1319.67":{"start":"1319.669","dur":"4.651","text":"you say find out what the bins the urns would\nhave to contain, the computer crunches it"},"1324.32":{"start":"1324.32","dur":"4.479","text":"and says you\u2019re bins would have to contain\nthese things or here\u2019s my best guess as"},"1328.80":{"start":"1328.799","dur":"4.401","text":"to what the bins contain based on the text\nand that list of bins and what they have in"},"1333.20":{"start":"1333.2","dur":"2.079","text":"them is your topic model."},"1335.28":{"start":"1335.279","dur":"6.981","text":"So topic modelling have two problems, the\nfirst is it\u2019s easy to do wrong or to misunderstand"},"1342.26":{"start":"1342.26","dur":"4.82","text":"and it\u2019s like pie charts in Microsoft Excel,\nas some people will know I hate pie charts,"},"1347.08":{"start":"1347.08","dur":"4.82","text":"there\u2019s almost never a good reason to use\na pie chart unfortunately Microsoft Excel\u2019s"},"1351.90":{"start":"1351.9","dur":"4.019","text":"interface is set up so that when you reach\nfor a chart pie charts are right there at"},"1355.92":{"start":"1355.919","dur":"5.37","text":"the top of the list so everyone goes, \u201coh\npie charts poof\u201d pie charts are extraordinarily"},"1361.29":{"start":"1361.289","dur":"6.12","text":"easy to misunderstand or do wrong, similarly\ntopic modelling, again I\u2019m thinking about"},"1367.41":{"start":"1367.409","dur":"5.421","text":"the Murakami et al paper in that paper when\nyou read it, it comes out soon, you\u2019ll see"},"1372.83":{"start":"1372.83","dur":"5.4","text":"that they do things like checking out multiple\ndifferent topic models with different numbers"},"1378.23":{"start":"1378.23","dur":"6.93","text":"of topics i.e. working on 50 bins, 60 bins,\n70 bins and see which one works best, most"},"1385.16":{"start":"1385.16","dur":"5.3","text":"topic modelling analysis don\u2019t do that,\nthey think carefully about the stock list,"},"1390.46":{"start":"1390.46","dur":"5.14","text":"a stock list is when you take out the function\nworks, most topic models just use the built"},"1395.60":{"start":"1395.6","dur":"3.91","text":"in stock lists and say \u201cyeah that\u2019s fine\u201d\nbut the Murakami et al paper actually things"},"1399.51":{"start":"1399.51","dur":"5.26","text":"carefully about it, there\u2019s all these kinds\nof things in that paper which most topic modelling"},"1404.77":{"start":"1404.77","dur":"2.879","text":"analyses do not do."},"1407.65":{"start":"1407.649","dur":"5.78","text":"So you can get past these by being careful\nlike Susan and Paul and crew were, but even"},"1413.43":{"start":"1413.429","dur":"7.911","text":"if you don\u2019t do it wrong there are two hard\ndrawbacks and they are randomness and transparency."},"1421.34":{"start":"1421.34","dur":"5.589","text":"Problems in the concept arise from this transparency\nissue, the issue in interpreting the topic"},"1426.93":{"start":"1426.929","dur":"3.71","text":"modelling and I have to thank Mathew Gillings\nhere who is writing a paper about this but"},"1430.64":{"start":"1430.639","dur":"4.471","text":"he let me read it in advance because he did\na bit study not of the actual topic modelling"},"1435.11":{"start":"1435.11","dur":"4.76","text":"but of how the interpretation of it works,\nso there\u2019s all kinds of issues in what can"},"1439.87":{"start":"1439.87","dur":"4.059","text":"go wrong when you\u2019re sitting there with\nthe output in front of you and trying to interpret"},"1443.93":{"start":"1443.929","dur":"1.061","text":"it."},"1444.99":{"start":"1444.99","dur":"3.12","text":"So that\u2019s one problem, another problem is\nthat you\u2019ve got arbitrary parameters in"},"1448.11":{"start":"1448.11","dur":"5.99","text":"the set-up, that is the topic modelling procedure\ncannot decide how many topics there should"},"1454.10":{"start":"1454.1","dur":"6.169","text":"be you have to tell it as I said before and\nit\u2019s astonishing when I\u2019ve looked in papers"},"1460.27":{"start":"1460.269","dur":"5.502","text":"people recommend kind of standard numbers\nthat are completely different like 20 for"},"1465.77":{"start":"1465.771","dur":"3.658","text":"instance is often recommended as a reasonable\nplace to start but then if you look at another"},"1469.43":{"start":"1469.429","dur":"5.301","text":"sort it will say oh well you should use somewhere\nbetween 100 and 400 topics as I said the Murakami"},"1474.73":{"start":"1474.73","dur":"4.5","text":"et al paper does ranges between I think is\nit about 50 and 200 that you did have some"},"1479.23":{"start":"1479.23","dur":"5.409","text":"feeling that something around that you looked\nat lots of different intervals."},"1484.64":{"start":"1484.639","dur":"3.771","text":"So you know there\u2019s lots of different versions\non this and Murakami et al there\u2019s another"},"1488.41":{"start":"1488.41","dur":"3.84","text":"parameter which is the number of iterations\nagain most people don\u2019t tinker with that"},"1492.25":{"start":"1492.25","dur":"3.86","text":"they just accept the single, the number of\niterations is how many times is the algorithm"},"1496.11":{"start":"1496.11","dur":"5.77","text":"going to go round and round and round in trying\nto work backwards towards the original bins."},"1501.88":{"start":"1501.88","dur":"4.779","text":"And these things range a whole lot more than\nsay a factor analysis, in a factor analysis"},"1506.66":{"start":"1506.659","dur":"3.421","text":"you\u2019ve got to pick the number of factors\nyou want to get out of the factor analysis,"},"1510.08":{"start":"1510.08","dur":"4.27","text":"do you want five or six or seven or eight,\nbut you\u2019ve only got a few numbers to choose,"},"1514.35":{"start":"1514.35","dur":"4.03","text":"we can see that the number of bins that you\nhave in a topic model ranges way more than"},"1518.38":{"start":"1518.38","dur":"5.09","text":"that, right and if you look at 50 and you\nlook at 60 which you might think yeah I\u2019ve"},"1523.47":{"start":"1523.47","dur":"4.48","text":"done well but what about 51, 52, 53, 54, if\nyou don\u2019t look you don\u2019t know but you"},"1527.95":{"start":"1527.95","dur":"3.92","text":"can\u2019t possibly look at that many different\ntopic models."},"1531.87":{"start":"1531.87","dur":"5.529","text":"So it\u2019s very difficult to have a warranted\nvalue, stock lists as I\u2019ve said almost all"},"1537.40":{"start":"1537.399","dur":"5.16","text":"topic modelling uses this, Murakami et al\nas I said is one of the few papers that I\u2019ve"},"1542.56":{"start":"1542.559","dur":"3.99","text":"seen that thinks hard about a stock list,\nI don\u2019t like stock lists but they think"},"1546.55":{"start":"1546.549","dur":"5.97","text":"hard about it. Matt\u2019s paper forthcoming\nshows that it really does make a big difference"},"1552.52":{"start":"1552.519","dur":"1.311","text":"whether you use a stock list or not."},"1553.83":{"start":"1553.83","dur":"6.819","text":"So we\u2019ve got to think again about what does\na topic mean, I kind of skimmed over that"},"1560.65":{"start":"1560.649","dur":"3.791","text":"well when people interpret topics they say\nthings like \u201cwell it\u2019s not necessarily"},"1564.44":{"start":"1564.44","dur":"4.719","text":"a type of content\u201d like topic would suggest\nit could also be a style, it could also be"},"1569.16":{"start":"1569.159","dur":"6.781","text":"a register, in terms of the algorithm as I\nsaid it\u2019s just a bin, a bin of words, and"},"1575.94":{"start":"1575.94","dur":"3.69","text":"I heard this from a CS researcher who I\u2019m\nnot going to name because I can\u2019t remember"},"1579.63":{"start":"1579.63","dur":"4.19","text":"the quote verbatim but it was at a conference\nwe should probably call it discourse modelling"},"1583.82":{"start":"1583.82","dur":"4.02","text":"rather than topic modelling since the groups\nof words it puts together are more like discourses"},"1587.84":{"start":"1587.84","dur":"6.35","text":"than they are like topics and my, I was just\ngoing whaaaaaa if there is a discourse analyst"},"1594.19":{"start":"1594.19","dur":"4.38","text":"in this room at this moment they would be\njust going bonkers and saying no a discourse"},"1598.57":{"start":"1598.57","dur":"5.5","text":"is not words that have been allocated together\nby a machine learning model."},"1604.07":{"start":"1604.07","dur":"4.18","text":"So yeah it would be a low blow however great\nthe temptation, to imply that when many data"},"1608.25":{"start":"1608.25","dur":"3.99","text":"miners say \u201ctopic\u201d or \u201csemantic\u201d what\nthey really mean is \u201cany algorithm that"},"1612.24":{"start":"1612.24","dur":"3.89","text":"spares me from having to understand, or care,\nwhat terms like \u2018topic\u2019 or \u2018semantic\u2019"},"1616.13":{"start":"1616.13","dur":"6.02","text":"actually mean, so I won\u2019t say that."},"1622.15":{"start":"1622.15","dur":"5.509","text":"Randomness, this working backwards procedure\nsomething that people don\u2019t always grasp"},"1627.66":{"start":"1627.659","dur":"6.5","text":"is that it involves randomness, the first\nstep is to randomly make a guess rolling dice"},"1634.16":{"start":"1634.159","dur":"7.041","text":"as to what words are in each bin then that\u2019s\nevaluated the probability weightings are adjusted"},"1641.20":{"start":"1641.2","dur":"4.699","text":"and then it\u2019s randomised again, and then\nagain and then again, and then again, so it\u2019s"},"1645.90":{"start":"1645.899","dur":"3.811","text":"random at each iteration but the assumption\nis that because you\u2019re adjusting the weightings"},"1649.71":{"start":"1649.71","dur":"6.02","text":"the randomness is going to be converging to\nsome kind of underlying reality. What this"},"1655.73":{"start":"1655.73","dur":"5.14","text":"means is that LDA is non-deterministic and\nI have to credit Derek Bridge here who again"},"1660.87":{"start":"1660.87","dur":"3.899","text":"2017 that\u2019s not a paper it was a talk he\nreally did underline for me how important"},"1664.77":{"start":"1664.769","dur":"5.76","text":"that was and in the room at the time I realised\nthat many people were doing topic modelling"},"1670.53":{"start":"1670.529","dur":"3.761","text":"this came as a surprise and for some of them\nit was a nasty surprise, I was in a room full"},"1674.29":{"start":"1674.29","dur":"3.04","text":"of historians and they were like oh topic\nmodelling, topic modelling we\u2019ve got to"},"1677.33":{"start":"1677.33","dur":"3.329","text":"do this it\u2019s digital humanities and then\nDerek Bridge stood up and he gave this talk"},"1680.66":{"start":"1680.659","dur":"3.35","text":"because he\u2019s a topic modelling person he\ngave this talk to explain how it actually"},"1684.01":{"start":"1684.009","dur":"6.16","text":"works and by the end of it the historians\nwere like, well obviously we can\u2019t do topic"},"1690.17":{"start":"1690.169","dur":"4.581","text":"modelling then if it\u2019s non-deterministic\nbut what can we do instead what else is there?"},"1694.75":{"start":"1694.75","dur":"4.529","text":"This was the conversation that was going on\nin that room, what does non-deterministic"},"1699.28":{"start":"1699.279","dur":"3.201","text":"mean in this concept, there\u2019s an element\nof randomness because we start with random"},"1702.48":{"start":"1702.48","dur":"2.919","text":"topics it\u2019s going to get randomised as we\ngo along."},"1705.40":{"start":"1705.399","dur":"5.52","text":"I\u2019m way overtime already so, Gigantopithecus\nsays, \u201cFor scholars in some disciplines,"},"1710.92":{"start":"1710.919","dur":"5.911","text":"a method being non-deterministic involving\nrandomness is an instant, absolute dealbreaker\u201d"},"1716.83":{"start":"1716.83","dur":"4.59","text":"now because we do quantitative stuff in corpus\nlinguistics maybe it\u2019s not such an instant"},"1721.42":{"start":"1721.42","dur":"3.9","text":"dealbreaker for us but it\u2019s certainly something\nthat we should be aware of and I\u2019m going"},"1725.32":{"start":"1725.32","dur":"1.23","text":"to illustrate it later."},"1726.55":{"start":"1726.55","dur":"6.099","text":"Now the practical bit, Andrew does a topic\nmodel, I use Mallet for this I am underlining"},"1732.65":{"start":"1732.649","dur":"4.641","text":"this because again lots of the best papers,\nMurakami et al use the systems built into"},"1737.29":{"start":"1737.29","dur":"4.979","text":"R, but I was thinking about the digital humanities\nalmost all the digital humanists who do this"},"1742.27":{"start":"1742.269","dur":"5.231","text":"sort of thing that I\u2019ve seen they use Mallet\nso I use Mallet, it\u2019s easy to use by the"},"1747.50":{"start":"1747.5","dur":"1.389","text":"way it\u2019s not difficult."},"1748.89":{"start":"1748.889","dur":"5.591","text":"Data, the data that I use for all of this\nwas the J section of FLOB as described by"},"1754.48":{"start":"1754.48","dur":"7.87","text":"Hundt et al and is Mariana here? Oh yes I\njust wanted to say thank you so much for the"},"1762.35":{"start":"1762.35","dur":"6.5","text":"FLOB manual it was such a great help in this,\nanyway so Genre J the learned genre academic"},"1768.85":{"start":"1768.85","dur":"6.07","text":"writing but perhaps more science than humanities\narts because in the FLOB brown scheme humanities"},"1774.92":{"start":"1774.92","dur":"5.06","text":"arts writing also turns up under Belles Lettres.\nThis is 80 texts, each 2,000 words, period"},"1779.98":{"start":"1779.98","dur":"6.679","text":"1991, UK English. Why FLOB J, because it\u2019s\ntractable, it\u2019s a relatively small amount"},"1786.66":{"start":"1786.659","dur":"4.481","text":"of data so I could keep control of what was\ngoing on and do checks and it\u2019s familiarly"},"1791.14":{"start":"1791.14","dur":"2.89","text":"I would guess that there\u2019s probably very\nfew people from this room who\u2019ve never done"},"1794.03":{"start":"1794.03","dur":"2.59","text":"anything with FLOB ever."},"1796.62":{"start":"1796.62","dur":"5.1","text":"So what I decided to do was a simple LDA analysis\nof FLOB using all the default settings, I"},"1801.72":{"start":"1801.72","dur":"5.16","text":"wasn\u2019t going to do what Susan and crew did\nbecause again the point was to work through"},"1806.88":{"start":"1806.88","dur":"6.221","text":"the way that the average user of topic modelling\ndoes. And then I cunningly deleted half of"},"1813.10":{"start":"1813.101","dur":"6.418","text":"my slide so what this did say was that I was\ndoing my analysis by the standard procedure"},"1819.52":{"start":"1819.519","dur":"4.79","text":"of getting the topics, looking at the top\nwords of each topic, eyeballing them and then"},"1824.31":{"start":"1824.309","dur":"3.86","text":"trying to characterise their content, again\nif you read the Murakami et al paper they"},"1828.17":{"start":"1828.169","dur":"4.14","text":"go way beyond that but standard approach is\nthrough interpretation do not."},"1832.31":{"start":"1832.309","dur":"5.651","text":"So here\u2019s my topic model, I asked for 20\ntopics because that\u2019s the default it also"},"1837.96":{"start":"1837.96","dur":"5.95","text":"seems reasonable, 80 texts, 20 topics, seems\na reasonable thing and I said show me the"},"1843.91":{"start":"1843.91","dur":"7.64","text":"top 20 words for every topic, and this is\nwhat it gave me, now I don\u2019t know if you\u2019re"},"1851.55":{"start":"1851.55","dur":"5.089","text":"counting in your head but the slide doesn\u2019t\nactually fit all 20 words so here it is split"},"1856.64":{"start":"1856.639","dur":"11.081","text":"across two slides, topic zero to 9, topics\n10 to 19. Gigantopithecus says, that even"},"1867.72":{"start":"1867.72","dur":"5.72","text":"with just 80 texts and just 20 topics and\njust 20 words per topic and totally ignoring"},"1873.44":{"start":"1873.44","dur":"3.849","text":"everything that\u2019s hiding behind this because\nas we\u2019ll see there\u2019s a huge amount hiding"},"1877.29":{"start":"1877.289","dur":"6.411","text":"behind this, it\u2019s still pretty difficult\nto get all the data into eyeballable shape"},"1883.70":{"start":"1883.7","dur":"7.469","text":"to represent to an audience or indeed a readership\nand again the Murakami et al paper has wonderful"},"1891.17":{"start":"1891.169","dur":"5.34","text":"visualisations to try and get all of this\ninto the paper, most topic modelling studies"},"1896.51":{"start":"1896.509","dur":"1.831","text":"don\u2019t do that."},"1898.34":{"start":"1898.34","dur":"4.85","text":"Anyway, so I went down each one I read the\nwords and I tried to come up with some kind"},"1903.19":{"start":"1903.19","dur":"5.19","text":"of covering term for those 20 words, again\nthis is what people do when they\u2019re interpreting"},"1908.38":{"start":"1908.38","dur":"5.58","text":"topic models so I came up with material physics\noh and I love this number one academic argument"},"1913.96":{"start":"1913.96","dur":"4.321","text":"made, time, work, general, means set, part,\nfat, control common point and then you\u2019ve"},"1918.28":{"start":"1918.281","dur":"4.469","text":"got things later on which are about evidence\nand argument, that was really nice and it\u2019s"},"1922.75":{"start":"1922.75","dur":"3.27","text":"not a topic in the normal sense it\u2019s more\nlike one of these discoursey things."},"1926.02":{"start":"1926.02","dur":"6.029","text":"Economics, then some which were kind of compounds,\nlinguistics and child mortality ended up together,"},"1932.05":{"start":"1932.049","dur":"5.6","text":"transmission electronics, aircraft materials,\nart history, law and finance, chemistry, home"},"1937.65":{"start":"1937.649","dur":"5.11","text":"gender sex, health, that seemed to all be\ntogether, government environment this was"},"1942.76":{"start":"1942.759","dur":"5.201","text":"another argumentative one amounts of data,\nlarge system form, high, found raised, topic,"},"1947.96":{"start":"1947.96","dur":"4.179","text":"I didn\u2019t look at anything beyond this because\nagain that\u2019s not what is done and the digital"},"1952.14":{"start":"1952.139","dur":"2.261","text":"managers they only look at this."},"1954.40":{"start":"1954.4","dur":"5.31","text":"You\u2019ll see that there are three gaps, those\nwere the hard ones, I didn\u2019t think that"},"1959.71":{"start":"1959.71","dur":"5.209","text":"was bad, 17 easy, 3 hard well let\u2019s take\na look at two hard ones topic 11, those are"},"1964.92":{"start":"1964.919","dur":"5.12","text":"the words, language, knowledge, sense, theory,\nprinciple, relationship classes, reality meaning"},"1970.04":{"start":"1970.039","dur":"5.76","text":"play rules, narrative, definition class, fact\nform accept world game object. I was kind"},"1975.80":{"start":"1975.799","dur":"4.26","text":"of getting a sense of a bit of Wittgenstein\nbut also a bit of kind of formal linguistics"},"1980.06":{"start":"1980.059","dur":"5.161","text":"so I was like is this philosophy of semantics\nor something like that, that was my best attempt."},"1985.22":{"start":"1985.22","dur":"5.4","text":"Number 8, design, thalidomide, patients, aircraft,\nweight requirements, patent Searle, I sure"},"1990.62":{"start":"1990.62","dur":"5.62","text":"Searle belonged in 11 but never mind, performance,\nmusic, gvhd, clinical mass, and I\u2019m like"},"1996.24":{"start":"1996.24","dur":"7.2","text":"medicine, music, aircraft design, what, what\nI don\u2019t know."},"2003.44":{"start":"2003.44","dur":"6.65","text":"And topic 17, if you have a suggestion I\u2019d\nbe delighted to hear it I just couldn\u2019t"},"2010.09":{"start":"2010.09","dur":"6.929","text":"come up with anything for that one. Linguistics,\nanimals, discharge, animal and animals, so"},"2017.02":{"start":"2017.019","dur":"4.311","text":"there\u2019s something animals going on there,\nbut everything else doesn\u2019t fit, so yeah"},"2021.33":{"start":"2021.33","dur":"3.24","text":"I just called that hodgepodge."},"2024.57":{"start":"2024.57","dur":"3.969","text":"Now the idea is that you can then relate this\nto the texts so I thought ok I will pick two"},"2028.54":{"start":"2028.539","dur":"4.741","text":"texts at random and I did it by numbers I\ntook J25 and J50 not knowing in advance what"},"2033.28":{"start":"2033.28","dur":"4.48","text":"they were going to be and I did the analysis,\nI looked at because the other thing you get"},"2037.76":{"start":"2037.76","dur":"4.95","text":"from topic modelling is a list of the topics\nmost strongly associated with each text, so"},"2042.71":{"start":"2042.71","dur":"5.25","text":"here\u2019s J25 which is a paper from a Psychology\nJournal called the \u2018The effects of flicker"},"2047.96":{"start":"2047.96","dur":"5.54","text":"on eye movement control\u2019 what did the model\nsay were the bins that that Author had reached"},"2053.50":{"start":"2053.5","dur":"5.721","text":"for most heavily, well unfortunately, you\nknow fate being a cruel mistress, the top"},"2059.22":{"start":"2059.221","dur":"8.619","text":"and strongest topic for that was the Hodgepodge,\nnow let\u2019s go back obviously it\u2019s about"},"2067.84":{"start":"2067.84","dur":"6.34","text":"eye flicker and reading you know experimental\npsychology obviously right, right, ok, then"},"2074.18":{"start":"2074.18","dur":"5.32","text":"we\u2019ve got education, social, psychological\nthat makes more sense and then the academic"},"2079.50":{"start":"2079.5","dur":"6.54","text":"argument one ok, those two made sense, and\nthen I went for 50, and I found unfortunately"},"2086.04":{"start":"2086.04","dur":"6.36","text":"that that also is eeeerrr, so \u2018Rights\u2019\nit\u2019s a paper on Embryology but it\u2019s law"},"2092.40":{"start":"2092.4","dur":"5.11","text":"as well and so we have the academic argument\nand the amounts of data, amounts of data evidence"},"2097.51":{"start":"2097.51","dur":"3.6","text":"so that seemed like academic discourse, but\nthen we also had the one that I would struggle"},"2101.11":{"start":"2101.11","dur":"4.42","text":"for and called it philosophy of semantics\nand then the education one again, that seemed"},"2105.53":{"start":"2105.53","dur":"4.48","text":"to me to be a bit better in terms of getting\nthat text. Incidentally these are the statistically"},"2110.01":{"start":"2110.01","dur":"2.64","text":"strongest topics in each one."},"2112.65":{"start":"2112.65","dur":"4.3","text":"Now often people that do topic modelling they\nsay well if it goes wrong you should have"},"2116.95":{"start":"2116.95","dur":"3.92","text":"used more topics or you should use less topics,\nso I considered maybe I didn\u2019t use enough"},"2120.87":{"start":"2120.87","dur":"5.691","text":"topics so for example Topic 3 has got some\nEnglish and foreign ology in there but it\u2019s"},"2126.56":{"start":"2126.561","dur":"5.239","text":"also got infant mortality in there and words\nlike long which I suppose could be either"},"2131.80":{"start":"2131.8","dur":"3.89","text":"you could be calling it a long period of infant\nmortality or a long vowel so I\u2019m not quite"},"2135.69":{"start":"2135.69","dur":"4.41","text":"sure what\u2019s going on there, so maybe if\nI\u2019d had more topics those two things would"},"2140.10":{"start":"2140.1","dur":"6.73","text":"have been split apart. But then you\u2019ve got\nto think well this is only the top 20 every"},"2146.83":{"start":"2146.83","dur":"5.03","text":"topic has a score for every word in the corpus\nbut you only see the top 20, well maybe the"},"2151.86":{"start":"2151.86","dur":"5.22","text":"things below the top 20 although individually\nnot so strongly associated maybe cumulatively"},"2157.08":{"start":"2157.08","dur":"3.82","text":"they are up to a big difference and maybe\nI should have been looking at that, but again"},"2160.90":{"start":"2160.9","dur":"4.22","text":"that\u2019s not what topic modellers do generally."},"2165.12":{"start":"2165.12","dur":"3.75","text":"Linguistics and Economics seem to end up split\nacross two topics which suggests maybe I\u2019m"},"2168.87":{"start":"2168.87","dur":"3.98","text":"using too many topics of economics is being\nsplit into two places and linguistics is being"},"2172.85":{"start":"2172.85","dur":"4.23","text":"split into two places it sounds like things\nthat belong together have been dragged apart,"},"2177.08":{"start":"2177.08","dur":"3.85","text":"so you can see there I\u2019ve got economics\nin two of my topics and I\u2019ve got linguistics"},"2180.93":{"start":"2180.93","dur":"4.42","text":"in two and then they\u2019re in the Hodgepodge\nas well, so maybe there are actually too many"},"2185.35":{"start":"2185.35","dur":"3.4","text":"topics so, oh I\u2019ve got evidence both ways."},"2188.75":{"start":"2188.75","dur":"4.49","text":"Now as I\u2019ve said the cumulative effect of\nthings below the cut off, my cut off is the"},"2193.24":{"start":"2193.24","dur":"4.89","text":"top 20 words because that\u2019s what\u2019s usually\ndone, but what about what\u2019s below it can"},"2198.13":{"start":"2198.13","dur":"4.21","text":"add up to a lot, we don\u2019t know if the things\nwe haven\u2019t looked at might collectively"},"2202.34":{"start":"2202.34","dur":"3.9","text":"sway the analysis by definition we don\u2019t\nknow what the effect would be of the things"},"2206.24":{"start":"2206.24","dur":"1.62","text":"we haven\u2019t looked at."},"2207.86":{"start":"2207.86","dur":"4.76","text":"Now this problem I should say is one that\nwe as corpus linguists have kind of let slide"},"2212.62":{"start":"2212.62","dur":"3.411","text":"for a long while because people look at the\ntop of frequency lists they look at the top"},"2216.03":{"start":"2216.031","dur":"3.679","text":"of key words lists, they look at the top of\nn-gram lists particularly in lexical bundle"},"2219.71":{"start":"2219.71","dur":"5.36","text":"analysis and the huge problem is that although\neverything below the top is infrequent individually"},"2225.07":{"start":"2225.07","dur":"4.65","text":"it can cumulatively add up to something as\nimportant as something at the top. So that\u2019s"},"2229.72":{"start":"2229.72","dur":"4.88","text":"something that we\u2019ve been kind of lacks\non is corpus linguistics as corpus linguists."},"2234.60":{"start":"2234.6","dur":"3.13","text":"So here\u2019s a little quote from Gillings about\nthe interpretation of that, the labelling"},"2237.73":{"start":"2237.73","dur":"4.56","text":"of topics is inherently a decision made by\nthe researcher alone, for example depending"},"2242.29":{"start":"2242.29","dur":"3.95","text":"on the individual researcher a topic about\nbirds could be labelled either as birds, wildlife"},"2246.24":{"start":"2246.24","dur":"5.38","text":"or animals so if you thought my labels were\nshonky maybe they were but Matthew pointed"},"2251.62":{"start":"2251.62","dur":"5.52","text":"out that this is essentially subjective, there\u2019s\nno agreed taxonomy in fact as Susan was saying"},"2257.14":{"start":"2257.14","dur":"3.43","text":"yesterday the whole point is that you don\u2019t\nhave a taxonomy and it generates one, but"},"2260.57":{"start":"2260.57","dur":"4.74","text":"then that creates problems for interpretation.\nIt\u2019s a judgement call as Underwood noted,"},"2265.31":{"start":"2265.31","dur":"8.06","text":"Underwood, Underwood did the it was Underwood\nwho did the Midwife from the 1800\u2019s, ultimately"},"2273.37":{"start":"2273.37","dur":"4.1","text":"it\u2019s difficult to suggest that a lower number\nof topics results in a broad topic range because"},"2277.47":{"start":"2277.47","dur":"4.91","text":"topic levels are so subjective in the first\nplace, so the Gigantopitecus says, there are"},"2282.38":{"start":"2282.38","dur":"4.321","text":"lots of almost arbitrary parameters here,\nis accepting the default good enough, well"},"2286.70":{"start":"2286.701","dur":"4.109","text":"I just did accept the default, if you didn\u2019t\nthink that that was good enough then you\u2019ve"},"2290.81":{"start":"2290.81","dur":"1.71","text":"got your answer."},"2292.52":{"start":"2292.52","dur":"4.89","text":"Confirmation bias is a risk in interpretation,\nit\u2019s easy to ignore top wordforms or whole"},"2297.41":{"start":"2297.41","dur":"4.3","text":"topics that fail to jump out as salient, you\nmay have noticed in my analysis that there"},"2301.71":{"start":"2301.71","dur":"3.52","text":"are words in some of the topics that don\u2019t\nfit my headings, I was just skimming over"},"2305.23":{"start":"2305.23","dur":"3.21","text":"them, again you\u2019ll see a lot of that in\nthe literature."},"2308.44":{"start":"2308.44","dur":"3.06","text":"The huge number of words and weightings to\nreport makes it easy to sweep intractable"},"2311.50":{"start":"2311.5","dur":"5.74","text":"items under the rug, that\u2019s because the\ntopic model is not those top 20 words the"},"2317.24":{"start":"2317.24","dur":"4.81","text":"topic model is a huge number of association\nstatistics between the words and the topics"},"2322.05":{"start":"2322.05","dur":"4.12","text":"where every word has an association strength\nof every topic."},"2326.17":{"start":"2326.17","dur":"4.56","text":"So here is the actual model which I printed\nout in a text file this is not [38:50] you\u2019ve"},"2330.73":{"start":"2330.73","dur":"5.31","text":"got line numbers down the side, so topic zero,\nexperimental approach nuclear, baa baa baa"},"2336.04":{"start":"2336.04","dur":"5.06","text":"baa baa, right the way down and when you get\nto line 14,000 odd we get to jointly which"},"2341.10":{"start":"2341.1","dur":"3.98","text":"is the last term and then we got to topic\n1 and we start the lexicon again."},"2345.08":{"start":"2345.08","dur":"4.24","text":"And then we start the lexicon again and we\nkeep on going ba da da da da, by the time"},"2349.32":{"start":"2349.32","dur":"6.93","text":"you get to 171,000 lines you\u2019ve got to topic\n11 and finally you get to line 293,080 and"},"2356.25":{"start":"2356.25","dur":"4.33","text":"you get jointly with topic 19 which is your\nvery last and that is the actual topic model,"},"2360.58":{"start":"2360.58","dur":"5.49","text":"that\u2019s the topic model those top 20 words\nare not the topic model they are a slice of"},"2366.07":{"start":"2366.07","dur":"6.44","text":"the very top of the topic model, these 300,000\nnumbers are your actual topic model."},"2372.51":{"start":"2372.51","dur":"3.641","text":"Now you can see that most of the numbers are\n0.01 which is the lowest possible score, it"},"2376.15":{"start":"2376.151","dur":"6.209","text":"means one scrabble tile somewhere at the bottom\nof the urn, but that\u2019s all there. For the"},"2382.36":{"start":"2382.36","dur":"6.8","text":"20 topic model that 200 odd the overwhelming\nmajority 274,000 out of 293 and 0.01. But"},"2389.16":{"start":"2389.16","dur":"6.27","text":"do people ever look at that, should they?\nIf not are we ignoring most of the actual"},"2395.43":{"start":"2395.43","dur":"1.53","text":"topic model?"},"2396.96":{"start":"2396.96","dur":"4.58","text":"Now again model I\u2019ve criticised topic but\nwhat about model, and usually model means"},"2401.54":{"start":"2401.54","dur":"3.99","text":"a description of a phenomenon that is simplified\nin some ways to help us understand how that"},"2405.53":{"start":"2405.53","dur":"4.27","text":"phenomenon actually works, so for instance\nif you talk about the planets going in circles"},"2409.80":{"start":"2409.8","dur":"4.3","text":"around the sun that\u2019s a simplification because\nthey\u2019re not really exact circles they\u2019re"},"2414.10":{"start":"2414.1","dur":"4.69","text":"ellipses but by talking about circles going\naround the sun you can start to understand"},"2418.79":{"start":"2418.79","dur":"4.03","text":"things about orbits you can start to understand\nthings about gravity, that\u2019s a scientific"},"2422.82":{"start":"2422.82","dur":"5.58","text":"model, a topic model is not like this, a topic\nmodel is a description of a phenomenon that"},"2428.40":{"start":"2428.4","dur":"5.64","text":"we know works completely differently, right\nwe know that writers don\u2019t pull tiles out"},"2434.04":{"start":"2434.04","dur":"4.82","text":"of bins, they sit there and they go through\narguments or tell stories, so we know that"},"2438.86":{"start":"2438.86","dur":"6.83","text":"the topic model is not what is happening,\nit\u2019s a model that is not aiming to tell"},"2445.69":{"start":"2445.69","dur":"5.73","text":"us what, how to understand the process of\ndiscourse production rather it\u2019s just creating"},"2451.42":{"start":"2451.42","dur":"4.43","text":"computational systems that can simulate the\noutcome, that\u2019s not the same thing."},"2455.85":{"start":"2455.85","dur":"5.18","text":"Now I had to get POS tagging in her somewhere,\nas Stephan mentioned it\u2019s almost impossible"},"2461.03":{"start":"2461.03","dur":"4.25","text":"for me to talk for more than about 30 minutes\nwithout talking about impart speech tagging."},"2465.28":{"start":"2465.28","dur":"4.12","text":"Impart speech tagging problemastic part speech\ntagging we use something called a Markov model"},"2469.40":{"start":"2469.4","dur":"4.83","text":"which is a model in the same sense, the Markov\nmodel of part speech assumption basically"},"2474.23":{"start":"2474.23","dur":"5.86","text":"says well it\u2019s a random guess based on transition\nprobabilities and emission probabilities which"},"2480.09":{"start":"2480.09","dur":"5.19","text":"I haven\u2019t got time to explain but basically\nthe Markov model is assuming that from one"},"2485.28":{"start":"2485.28","dur":"5.24","text":"word to the next is kind of a random transition\nproducing text as we go and it uses a similar"},"2490.52":{"start":"2490.52","dur":"4.66","text":"kind of backwards working assumptions to try\nand work out the underlying parts of speech"},"2495.18":{"start":"2495.18","dur":"5.3","text":"from the words it sees. So again you know\nit\u2019s not a totally unfamiliar to linguistics"},"2500.48":{"start":"2500.48","dur":"4.47","text":"we\u2019ve been using Markov models for years,\nno-one has ever suggested that the human brain"},"2504.95":{"start":"2504.95","dur":"4.85","text":"produces text by use of a Markov model but\nalso we don\u2019t look at the probabilities"},"2509.80":{"start":"2509.8","dur":"3.9","text":"in the Markov model to give us insights into\ngrammar we know it\u2019s just a clever hack"},"2513.70":{"start":"2513.7","dur":"4.99","text":"it\u2019s not actually telling us anything about\ngrammar in the brain. I wish I had like five"},"2518.69":{"start":"2518.69","dur":"3.78","text":"more slides about Markov models to really\nmake that point but I didn\u2019t want to test"},"2522.47":{"start":"2522.47","dur":"3.67","text":"your patience too much."},"2526.14":{"start":"2526.14","dur":"4.66","text":"Replication since I\u2019ve mentioned scientific\nmodels I should mention replicability because"},"2530.80":{"start":"2530.8","dur":"6.62","text":"scientific respectability requires that analysis\nshould be replicable, this is where the randomness"},"2537.42":{"start":"2537.42","dur":"5.032","text":"at its core of the LDA approach can come back\nto bite us, so I thought well I\u2019ve done"},"2542.45":{"start":"2542.452","dur":"4.628","text":"my analysis I\u2019ll do it again, and when I\nsay I did it again, I didn\u2019t change the"},"2547.08":{"start":"2547.08","dur":"4.59","text":"data I didn\u2019t change the settings, I literally\npasted in the command onto the command line"},"2551.67":{"start":"2551.67","dur":"5","text":"again and pressed enter, right so bear in\nmind two key strokes produced what your about"},"2556.67":{"start":"2556.67","dur":"6.09","text":"to see, I repeated the topic modelling run\nof Mallet from that same imported corpus and"},"2562.76":{"start":"2562.76","dur":"6","text":"I copied the top 20 words into the top two\nanalysis and I found this time that I had"},"2568.76":{"start":"2568.76","dur":"6.14","text":"19 topics that were easily interpretable so\ninfant mortality plus misc others, nations,"},"2574.90":{"start":"2574.9","dur":"4.39","text":"history, classics, marine biology, theoretical\nlinguistic, circuitry, bio chemists, blah,"},"2579.29":{"start":"2579.29","dur":"4.8","text":"blah, blah blah, and one Hodgepodge that I\ncouldn\u2019t interpret thalidomide patients,"},"2584.09":{"start":"2584.09","dur":"6.41","text":"boundary, so medi medicine but also pyrite\ngrain sites, so anyway."},"2590.50":{"start":"2590.5","dur":"5.49","text":"I found in this case the linguistics instead\nof being you know kind of unsatisfactory linguistics"},"2595.99":{"start":"2595.99","dur":"4.63","text":"was dealt with great this time we had one\nthat was clearly about language theory grammar"},"2600.62":{"start":"2600.62","dur":"5.71","text":"and acquisition topic 6, and we had one that\nwas clearly about phonetics, maybe except"},"2606.33":{"start":"2606.33","dur":"4.86","text":"for game and play but otherwise very obviously\nabout phonetics, and linguistics words weren\u2019t"},"2611.19":{"start":"2611.19","dur":"5.47","text":"really popping up elsewhere, and that linguistics\nphonetics was one of the very few topics that"},"2616.66":{"start":"2616.66","dur":"5.19","text":"appeared both when I did it the first time\nand when I did it the second time, economics"},"2621.85":{"start":"2621.85","dur":"4.33","text":"also was more or less consistent, marine biology\nwas consistent and aircraft manufacturing"},"2626.18":{"start":"2626.18","dur":"4.11","text":"was consistent, what else was consistent between\nthe first time I did it and the second time"},"2630.29":{"start":"2630.29","dur":"5.53","text":"I did it? Crickets chirping, I didn\u2019t have\na sound effect because I didn\u2019t know if"},"2635.82":{"start":"2635.82","dur":"4.35","text":"it would work here but I have a picture of\na cricket."},"2640.17":{"start":"2640.17","dur":"5.83","text":"So the amounts of data evidence discourse\nof you know presenting your evidence gone,"},"2646.00":{"start":"2646","dur":"3.73","text":"the academic argument one all the words in\nit seem to be dispersed across the other topics"},"2649.73":{"start":"2649.73","dur":"5.29","text":"it was gone, chemistry wasn\u2019t gone but it\nhad transformed because of changes in the"},"2655.02":{"start":"2655.02","dur":"6.18","text":"words inside it, it was clearly now biochemistry\nwhereas before it hadn\u2019t been clearly biochemistry."},"2661.20":{"start":"2661.2","dur":"5.76","text":"So there\u2019s not a lot of consistency here,\nand this is a slide I added I was inspired"},"2666.96":{"start":"2666.96","dur":"4.28","text":"by Susan here she was talking about how the\nother words in a topic alongside a word can"},"2671.24":{"start":"2671.24","dur":"4.73","text":"disambiguate it, well this is one that I spotted\nit was the word \u201cFormula\u201d when I did my"},"2675.97":{"start":"2675.97","dur":"3.34","text":"first analysis there\u2019s where formula was,\npressure, temperature, reaction, resistance,"},"2679.31":{"start":"2679.31","dur":"7.83","text":"paper, continuum, I thought it\u2019s a physics\nformula, then I did it in the second analysis"},"2687.14":{"start":"2687.14","dur":"4.93","text":"formula popped up in this topic, point, position,\ntext, line, female, part, book, reader, definition,"},"2692.07":{"start":"2692.07","dur":"4.45","text":"points, now this is harder but looking at\nthis and we seem to have maths and reading"},"2696.52":{"start":"2696.52","dur":"5.42","text":"mixed together so formula is more like a mathematical\nformula here, now maybe that\u2019s close enough"},"2701.94":{"start":"2701.94","dur":"5.31","text":"for you it wasn\u2019t close enough for me I\u2019m\nafraid, so Gigantopithecus says, even if you\u2019re"},"2707.25":{"start":"2707.25","dur":"4.26","text":"on board with nondeterministic methods that\ncan produce a different result every time"},"2711.51":{"start":"2711.51","dur":"4.73","text":"you might well find that this amount of variability\nmay give you pause, even if you\u2019re happy"},"2716.24":{"start":"2716.24","dur":"4.85","text":"with a little bit of variability this is a\nlot of variability."},"2721.09":{"start":"2721.09","dur":"3.21","text":"So if we decide we don\u2019t like it what else\ncan we do with these massive lists of word"},"2724.30":{"start":"2724.3","dur":"5.95","text":"frequencies, well again at least I can talk,\nso saying something about Susan and Paul\u2019s"},"2730.25":{"start":"2730.25","dur":"4.36","text":"work that doesn\u2019t involve what Susan talked\nabout yesterday, the Murakami et al paper"},"2734.61":{"start":"2734.61","dur":"5.53","text":"is one of a pair of papers, and in the other\nwhich is due out later this year the same"},"2740.14":{"start":"2740.14","dur":"4.48","text":"approach is applied to multidimensional analysis\nusing Bi9ber\u2019s approach. For those of you"},"2744.62":{"start":"2744.62","dur":"4.69","text":"who don\u2019t know what multidimensional analysis\nis you take 150 frequency counts or features"},"2749.31":{"start":"2749.31","dur":"4.85","text":"that have been designed in advance informed\nby linguistic theory that\u2019s crucial using"},"2754.16":{"start":"2754.16","dur":"5.49","text":"POS tags and word sequences sorry POS tags,\nPOS sequences and word lists that mean something"},"2759.65":{"start":"2759.65","dur":"6","text":"semantically and then you do a factor analysis\non them to boil down those 150 variables down"},"2765.65":{"start":"2765.65","dur":"5.36","text":"into 5 or 6, and then the factors you interpret\nthem functionally and you say these are the"},"2771.01":{"start":"2771.01","dur":"3.39","text":"dimensions and variation that\u2019s Bi9ber\u2019s\nmodel in a nutshell."},"2774.40":{"start":"2774.4","dur":"3.39","text":"Now you might say well what\u2019s the difference\nthen if you\u2019ve got statistics in there like"},"2777.79":{"start":"2777.79","dur":"5.59","text":"factor analysis, factor analysis, factor analysis\nis deterministic put in the same inputs you"},"2783.38":{"start":"2783.38","dur":"2.79","text":"will always get the same outputs."},"2786.17":{"start":"2786.17","dur":"6.35","text":"The Hunston et al paper which is in IJCL later\nthis year I believe, the idea was that this"},"2792.52":{"start":"2792.52","dur":"5.14","text":"and the Murakami et al were both about the\nsame data but from two prospectives, multidimensional"},"2797.66":{"start":"2797.66","dur":"5.64","text":"analysis on the one hand, deterministic linguistically\ninformed LDA on the other hand much more bottom"},"2803.30":{"start":"2803.3","dur":"5.82","text":"up, but I thought well, is there not something\nin-between these two, you know is it really"},"2809.12":{"start":"2809.12","dur":"4.86","text":"a choice between embracing LDA with all its\nproblems or else doing something that is very,"},"2813.98":{"start":"2813.98","dur":"6.47","text":"very theory heavy. And I thought um, must\nwe go if we don\u2019t want to do Biber style"},"2820.45":{"start":"2820.45","dur":"5.47","text":"multidimensional is the only alternative to\ngo all the way to LDA, can we get at text"},"2825.92":{"start":"2825.92","dur":"4.63","text":"level co-occurrence patterns without those\nissues? And this was something that was again"},"2830.55":{"start":"2830.55","dur":"3.95","text":"really embracing by Susan\u2019s talk yesterday\nI keep going on about it because I was sitting"},"2834.50":{"start":"2834.5","dur":"3.65","text":"there thinking, please Susan don\u2019t say anything\nthat is going to make my entire talk completely"},"2838.15":{"start":"2838.15","dur":"4.57","text":"pointless tomorrow and she didn\u2019t, once\nor twice she came close, but she didn\u2019t"},"2842.72":{"start":"2842.72","dur":"4.071","text":"anyway and I was thinking Susan really emphasised\nthat topic modelling is in a way it\u2019s like"},"2846.79":{"start":"2846.791","dur":"5.459","text":"collocation but we\u2019re looking at not co-occurrences\ndirectly next to one another but co-occurrence"},"2852.25":{"start":"2852.25","dur":"4.23","text":"within a text and I thought well if that\u2019s\nwhat you\u2019re looking at you can still use"},"2856.48":{"start":"2856.48","dur":"5.29","text":"co-occurrence statistics if you want, they\u2019ll\nstill work. The reason for this Stefan is"},"2861.77":{"start":"2861.77","dur":"6.54","text":"that the basic understanding of collocation\nand here\u2019s Stefan\u2019s version of it there"},"2868.31":{"start":"2868.31","dur":"4.25","text":"are other versions of it but I like this one.\nYou have a generalised contingency table for"},"2872.56":{"start":"2872.56","dur":"5.56","text":"collocation and it\u2019s using this notation\nwhere U is the node and V is the collocate,"},"2878.12":{"start":"2878.12","dur":"5.86","text":"and you look at the collorcurrences of U and\nV and not U and not V. Now basically you say"},"2883.98":{"start":"2883.98","dur":"5.21","text":"how many times do these occur nearby how many\ntimes they occur not nearby? The definition"},"2889.19":{"start":"2889.19","dur":"6.13","text":"of what counts as nearby for this statistical\nmodel is entirely independent of the statistics,"},"2895.32":{"start":"2895.32","dur":"5.9","text":"right, in the same text could be defined as\nnearby and that table will still work all"},"2901.22":{"start":"2901.22","dur":"5.04","text":"the collocation statistics must still work\nwe can still use the stats all of the association"},"2906.26":{"start":"2906.26","dur":"5.6","text":"measures for collocation that we use are deterministic\nsame input leads to same output. I haven\u2019t"},"2911.86":{"start":"2911.86","dur":"5.02","text":"done that because I was interested in something\nelse so that\u2019s where I\u2019m going to finish"},"2916.88":{"start":"2916.88","dur":"1","text":"I think."},"2917.88":{"start":"2917.88","dur":"5.01","text":"Factor analysis as I\u2019ve said underlies multidimensional\nanalysis, cluster analysis is the one that"},"2922.89":{"start":"2922.89","dur":"3.2","text":"I\u2019ve talked about least, but it\u2019s where\nI\u2019m going to end, we\u2019ve seen it being"},"2926.09":{"start":"2926.09","dur":"7.88","text":"used in Stephan Gries\u2019 papers, the behavioural\nprofiles method uses cluster analysis across"},"2933.97":{"start":"2933.97","dur":"5.31","text":"feature frequencies derived from detailed\nanalysis of a concordance and if you haven\u2019t"},"2939.28":{"start":"2939.28","dur":"6.92","text":"read those papers on behavioural profiles,\ndo, they\u2019re very, very interesting, they"},"2946.20":{"start":"2946.2","dur":"2.7","text":"have been hugely influential for me."},"2948.90":{"start":"2948.9","dur":"5.971","text":"And also I have an App for that, I have a\nlittle thing on the web which if you want"},"2954.87":{"start":"2954.871","dur":"4.619","text":"to do some cluster analysis it\u2019s all put\non the web anyone can use it called the UCREL"},"2959.49":{"start":"2959.49","dur":"3.57","text":"Clustertool where you just paste in the data\nand you get yourself a nice little cluster"},"2963.06":{"start":"2963.06","dur":"3.24","text":"analysis and it\u2019s all terribly jolly."},"2966.30":{"start":"2966.3","dur":"5.251","text":"And I also thought well I want to use some\nlinguistic knowledge I want to proposal in"},"2971.55":{"start":"2971.551","dur":"5.319","text":"a way of doing it I want to use some of those\nlinguistic knowledge tricks to reduce the"},"2976.87":{"start":"2976.87","dur":"7.231","text":"dimensionality, and this is where I reach\nfor the USAS system, is Paul here? Argh there"},"2984.10":{"start":"2984.101","dur":"5.509","text":"you are, give a cheer for USAS, hooray, so\nI took my 80 texts and I tagged them with"},"2989.61":{"start":"2989.61","dur":"5.35","text":"CLAWS power speech and I tried them with USAS\nsemantic categories, I then simplified the"},"2994.96":{"start":"2994.96","dur":"5.01","text":"semantic tag slightly because the USAS semantic\nsystem is very complex so I boiled it down"},"2999.97":{"start":"2999.97","dur":"5.75","text":"to 209 semantic categories based on that semantic\ntagging, I then wrote a little programme the"},"3005.72":{"start":"3005.72","dur":"5.2","text":"entire run of which can be seen here to scan\nthrough the USAS output and generate semantic"},"3010.92":{"start":"3010.92","dur":"4.55","text":"tag frequencies, so I had my 209 frequencies\nfor the frequencies of the semtags in every"},"3015.47":{"start":"3015.47","dur":"4.37","text":"one of the 80 texts, so again we\u2019re dealing\nwith vector frequencies but already we\u2019ve"},"3019.84":{"start":"3019.84","dur":"3.89","text":"reduced the problem by using the semantic\ntags, basically what that\u2019s doing it providing"},"3023.73":{"start":"3023.73","dur":"4.58","text":"a kind of thesaurus to group together words\nthat are semantically similar in advance before"},"3028.31":{"start":"3028.31","dur":"7.43","text":"we start the analysis. That\u2019s what it looks\nlike when it pops out, so here are the FLOB"},"3035.74":{"start":"3035.74","dur":"9.02","text":"texts, J1, 2, 3, 4, 5, down to 19 and here\nare the semantic tags A10, A11, A12, 13, 14,"},"3044.76":{"start":"3044.76","dur":"3.78","text":"15 the A\u2019s are not terribly interesting\nsemantic tags the interesting ones happen"},"3048.54":{"start":"3048.54","dur":"3.13","text":"between like B and S."},"3051.67":{"start":"3051.67","dur":"3.99","text":"So yes this is a spreadsheet and this is just\nthe top left corner, we\u2019ve got a data matrix,"},"3055.66":{"start":"3055.66","dur":"4.3","text":"we can apply any of the statistical methods\nthat can be applied to a data matrix, I could"},"3059.96":{"start":"3059.96","dur":"3.563","text":"have done factor analysis I could have done\ncollocation stats I decided to do cluster"},"3063.52":{"start":"3063.523","dur":"1","text":"analysis."},"3064.52":{"start":"3064.523","dur":"5.177","text":"And that\u2019s what popped out of my tool, all\nI did literally was I copied and pasted that"},"3069.70":{"start":"3069.7","dur":"6.12","text":"spreadsheet into the UCREL cluster tool pressed\none button and that was generated it\u2019s basically"},"3075.82":{"start":"3075.82","dur":"3.65","text":"a little web tool that calls R in the background\nto generate the image."},"3079.47":{"start":"3079.47","dur":"4.61","text":"So that\u2019s the J text clustered by semantic\ntags, and instantly we can see that in terms"},"3084.08":{"start":"3084.08","dur":"4.95","text":"of semantics we\u2019ve got one little outlay\nof blob here and then two big categories,"},"3089.03":{"start":"3089.03","dur":"5.47","text":"now it\u2019s hard to read what the texts actually\nare but thankfully the tool also provides"},"3094.50":{"start":"3094.5","dur":"7.09","text":"a textual version of this laid on its side,\nit looks like this, can you see that this"},"3101.59":{"start":"3101.59","dur":"5.36","text":"is the tree this is the top of the tree and\nthe its laid sideways and you can see what"},"3106.95":{"start":"3106.95","dur":"4.95","text":"texts are close together, now I didn\u2019t have\ntime to go through all 80 texts and cross"},"3111.90":{"start":"3111.9","dur":"5.76","text":"reference them to the manual to work out what\nwas going on in terms of the bibliography"},"3117.66":{"start":"3117.66","dur":"7.32","text":"but if you read through the FLOB J manual\nfile, you will find that similar sounding"},"3124.98":{"start":"3124.98","dur":"5.79","text":"kinds of fields tend to be together in the\nsequence, and I find it very interesting that"},"3130.77":{"start":"3130.77","dur":"7.81","text":"a lot of these units are grouping together\ntexts with similar numbers, 52, 53, 56, 60,"},"3138.58":{"start":"3138.58","dur":"7.27","text":"here\u2019s another one low numbers up there,\nnumbers in the 30\u2019s and 60\u2019s here, 22,"},"3145.85":{"start":"3145.85","dur":"7.01","text":"14, and 15 are there, 70\u2019s, 46, 76, so that\u2019s\nlike 70\u2019s there\u2019s clearly something going"},"3152.86":{"start":"3152.86","dur":"4.58","text":"on here that is reflecting the actual kind\nof disciplines that we\u2019re seeing there."},"3157.44":{"start":"3157.44","dur":"4.39","text":"And there\u2019s another one and you can see\nthis is going right the way through this dendagram,"},"3161.83":{"start":"3161.83","dur":"9.74","text":"the dendagram is the entire output in about\n300\/400 lines of text instead of 300,000 parameters."},"3171.57":{"start":"3171.57","dur":"5.42","text":"One in detail just one, again you can see\nthat they\u2019re all quite close together, and"},"3176.99":{"start":"3176.99","dur":"5.07","text":"so I picked some common words from the title\nso this is again this is from Marianna\u2019s"},"3182.06":{"start":"3182.06","dur":"5.74","text":"manual, so you can see that it\u2019s putting\ntogether 10 and 20, hexagons, conics, cloud"},"3187.80":{"start":"3187.8","dur":"4.75","text":"droplet, deposition and velocity so there\u2019s\nsomething going on with the geometry there,"},"3192.55":{"start":"3192.55","dur":"4.98","text":"17 thalidomide treatment for chronic graft\nversus host disease, that explains thalidomide"},"3197.53":{"start":"3197.53","dur":"5.59","text":"it just kept popping up in weird places in\nthe topic novel, and that\u2019s then associated"},"3203.12":{"start":"3203.12","dur":"7.07","text":"with 9 and 12, you can see there which is\nproteolysis of factor chain polypeptides and"},"3210.19":{"start":"3210.19","dur":"5.02","text":"definition of surface exposed epitopes, this\nis kind of like molecular chemistry of various"},"3215.21":{"start":"3215.21","dur":"3.61","text":"sorts, molecular bio-chemistry, we can see\nwhat\u2019s going on there in terms of these"},"3218.82":{"start":"3218.82","dur":"5.64","text":"semantic tags, and the distances between texts\ndefined by that semantic tag matrix."},"3224.46":{"start":"3224.46","dur":"5.31","text":"Now if I\u2019d had more time I said this was\ntowards I would have done things like switch"},"3229.77":{"start":"3229.77","dur":"4.63","text":"it round and cluster the semantic tags rather\nthan clustering the text to see which semantic"},"3234.40":{"start":"3234.4","dur":"3.67","text":"groupings are related to one another, you\ncan do the cluster analysis both ways, you"},"3238.07":{"start":"3238.07","dur":"4.63","text":"can cluster the features as well as the objects.\nI would have done the same with factor analysis"},"3242.70":{"start":"3242.7","dur":"5.14","text":"to see which semantic tags grouped together\nin factors Biber style and I would in fact"},"3247.84":{"start":"3247.84","dur":"3.46","text":"probably then have tried to do, try being\nthe operative word, because I\u2019ve never done"},"3251.30":{"start":"3251.3","dur":"4.26","text":"it before but I\u2019ve read Biber, I would have\ntried to do a full multidimensional analysis"},"3255.56":{"start":"3255.56","dur":"3.89","text":"there and I\u2019m sure that everyone in this\nroom has already got binging in their mind"},"3259.45":{"start":"3259.45","dur":"3.09","text":"other things that you could do along these\nlines."},"3262.54":{"start":"3262.54","dur":"4.69","text":"So how does this sort of thing stack up against\nLDA this is where I really start being in"},"3267.23":{"start":"3267.23","dur":"6.86","text":"the realm of opinion, well when we start comparing\nthe results that we get out, we see some similarities,"},"3274.09":{"start":"3274.09","dur":"7.76","text":"we see major divisions cutting across disciplines,\nI found in one of the branches of the dendogram"},"3281.85":{"start":"3281.85","dur":"6.25","text":"I found haematology and mathematics together\nnot quite sure why, and a similarity between,"},"3288.10":{"start":"3288.1","dur":"5.23","text":"that I found between doing this topic modelling\nanalysis and this clustering analysis on the"},"3293.33":{"start":"3293.33","dur":"4.39","text":"other hand is that if you want to actually\nget decent interpretation you need to be prepared"},"3297.72":{"start":"3297.72","dur":"4.43","text":"to go digging in the data of Murakami et al\nthere is no way away from that, and there"},"3302.15":{"start":"3302.15","dur":"4.831","text":"are arbitrary parameters, sorry about the\ntypos I forgot to proof read this slide, there"},"3306.98":{"start":"3306.981","dur":"5.049","text":"are arbitrary parameters, the cut is how deep\nin the dendrogram we decide to go and that\u2019s"},"3312.03":{"start":"3312.03","dur":"4.22","text":"arbitrary there\u2019s no mathematics can tell\nyou how deep in the tree you need to go, similarly"},"3316.25":{"start":"3316.25","dur":"2.82","text":"there\u2019s no mathematics can tell you the\nnumber of topics."},"3319.07":{"start":"3319.07","dur":"5.17","text":"So there are similar issues in both these\nkinds of ways of reducing this massive vector"},"3324.24":{"start":"3324.24","dur":"4.95","text":"this massive matrix but the differences are\nthat this cluster analysis I think it\u2019s"},"3329.19":{"start":"3329.19","dur":"4.28","text":"far more transparent, the whole thing fits\non one screen, the text dendrogram few hundred"},"3333.47":{"start":"3333.47","dur":"5.21","text":"lines it\u2019s not 20 things called from the\ntop of this massive set of parameters. It\u2019s"},"3338.68":{"start":"3338.68","dur":"5.32","text":"more reproducible because it\u2019s deterministic\nand we\u2019re directly interpreting text groups"},"3344.00":{"start":"3344","dur":"2.28","text":"rather than interpreting topics."},"3346.28":{"start":"3346.28","dur":"5.34","text":"So Gigantopithecus says obviously this is\njust start, more could be done, and more will"},"3351.62":{"start":"3351.62","dur":"4.58","text":"be done, some final thoughts, and this is\nwhere I wax philosophical."},"3356.20":{"start":"3356.2","dur":"4.56","text":"We\u2019ve always been saying to other linguists\ncorpus linguistics is not just number crunching"},"3360.76":{"start":"3360.76","dur":"4.57","text":"it\u2019s essentially qualitative as well as\nquantitative and I said quotes from some textbooks"},"3365.33":{"start":"3365.33","dur":"3.2","text":"including Hunston would be good here if possible\nbecause I wanted to give credit to Susan but"},"3368.53":{"start":"3368.53","dur":"4.82","text":"unfortunately I forgot that I wouldn\u2019t have\naccess to my University Library while I was"},"3373.35":{"start":"3373.35","dur":"2.07","text":"her in Birmingham, so sorry."},"3375.42":{"start":"3375.42","dur":"3.51","text":"And yet here is the field embracing topic\nmodelling which is really blind black box"},"3378.93":{"start":"3378.93","dur":"4.02","text":"number crunching, so did we perhaps not mean\nthat first thing that CL is not just number"},"3382.95":{"start":"3382.95","dur":"4.21","text":"crunching, were we kind of like just covering\nourselves all the time, if we\u2019re serious"},"3387.16":{"start":"3387.16","dur":"2.91","text":"about this we need to be especially on our\nguard with black box methods and statistical"},"3390.07":{"start":"3390.07","dur":"4.23","text":"machine learning without any linguistic knowledge\nin the loop which is the methods like topic"},"3394.30":{"start":"3394.3","dur":"2.36","text":"modelling and LDA."},"3396.66":{"start":"3396.66","dur":"3.38","text":"Data mining research is I think like machine\nlearning methods because they don\u2019t require"},"3400.04":{"start":"3400.04","dur":"4.73","text":"knowledge of the domain. You can have a standard\ntoolkit, if you\u2019re a data mining person"},"3404.77":{"start":"3404.77","dur":"4.14","text":"and you\u2019ve got your toolkit of algorithms\nyou can use it anywhere on any domain but"},"3408.91":{"start":"3408.91","dur":"4.91","text":"we\u2019re linguists we have domain knowledge\nwe probably shouldn\u2019t pretend that we don\u2019t"},"3413.82":{"start":"3413.82","dur":"5.08","text":"and even without that knowledge we already\nhave plenty of tools in our bag for dealing"},"3418.90":{"start":"3418.9","dur":"2.01","text":"with this sort of thing."},"3420.91":{"start":"3420.91","dur":"4.55","text":"Gigantopithecus says, as far as he\u2019s concerned\nthe concordance should still be our weapon"},"3425.46":{"start":"3425.46","dur":"5.72","text":"of first and last resort the place where quantification\nand interpretation meet."},"3431.18":{"start":"3431.18","dur":"4.04","text":"So not a conclusion because there\u2019s more\nto do, obviously more can be done but I\u2019m"},"3435.22":{"start":"3435.22","dur":"5.21","text":"not a fan of LDA and hopefully you now understand\nwhy. Too many things can go wrong in the interpretation"},"3440.43":{"start":"3440.43","dur":"3.3","text":"and we have plenty of exploratory methods\nin our toolkit already."},"3443.73":{"start":"3443.73","dur":"3.12","text":"Most importantly, remember what your Gigantopithecus\ntaught you."}}