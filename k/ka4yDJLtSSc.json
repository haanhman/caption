{"0":{"dur":5,"text":"It\u2019s an interesting top position that I\nfind myself in being the first plenary speaker"},"5":{"dur":7,"text":"but also having a hard act to follow in that\nI\u2019m straight after Susan\u2019s pre-conference"},"13":{"dur":4,"text":"plenary last night which I know a lot of you\nwere at but not all of you. For those of you"},"18":{"dur":4,"text":"who weren\u2019t allow me to give you a summary\nof Susan\u2019s key takeaway points, Hunston"},"22":{"dur":6,"text":"2017 that\u2019s the talk last night argues convincingly\nfor the centrality of dinosaur clipart in"},"28":{"dur":6,"text":"corpus-linguistic academic discourse and I\nwas convinced when the spirit of the innovation"},"35":{"dur":4,"text":"the extinct animal mascot for this presentation\nwas a Gigantopithecus blacki, for those of"},"40":{"dur":6,"text":"you do don\u2019t know what a Gigantopithecus\nis, this is a Gigantopithecus, if you\u2019re"},"46":{"dur":5,"text":"saying to yourself \u201cby gum that looks like\nan orangutan that\u2019s been caught by the wrong"},"51":{"dur":5,"text":"end of shrink ray\u201d, well yes that\u2019s exactly\nthat it is, it\u2019s exactly that."},"56":{"dur":5,"text":"So that\u2019s a Gigantopithecus, it went extinct\nabout 100,000 years ago and the spirit of"},"61":{"dur":3,"text":"the Gigantopithecus will be haunting my talk\nmuch as the dinosaurs did Susan\u2019s, whenever"},"65":{"dur":6,"text":"you see the Gigantopithecus appear that\u2019s\na key point."},"72":{"dur":9,"text":"Now onto the introduction that I had before\nI saw Susan\u2019s talk yesterday, ok, so, this"},"81":{"dur":7,"text":"as Stephan mentioned in his very flattering\nintroduction I think a lot about methodology,"},"89":{"dur":6,"text":"and one of the things about methodology that\nseems to be at the core of the recent interest"},"95":{"dur":6,"text":"in topic modelling as well as a whole range\nof other types of methods that have been used"},"102":{"dur":4,"text":"over the years is the issue of multidimensional\nspaces especially when we\u2019re dealing with"},"106":{"dur":6,"text":"wordform frequency and I will explain what\nI mean by multidimensional in a minute. And"},"112":{"dur":5,"text":"LDA, Latant Dirichlet Analysis which is the\nmost commonly used form of top modelling these"},"118":{"dur":7,"text":"days is often as we\u2019ve seen not only from\nSusie yesterday but also from assorted papers"},"125":{"dur":5,"text":"that have come out over the past few years,\nit\u2019s the most commonly used form of top"},"131":{"dur":5,"text":"modelling, it\u2019s often put forward as a solution\nto that problem. It\u2019s used in digital humanities"},"136":{"dur":4,"text":"and now as we\u2019ve seen it\u2019s started to\ncome into linguistics as well by some pioneering"},"141":{"dur":4,"text":"efforts, so I\u2019m going to talk about these\nissues and then I\u2019m actually going to round"},"146":{"dur":5,"text":"about fifteen minutes in, I\u2019m going to transition\ninto presenting a worked example of this kind"},"151":{"dur":7,"text":"of analysis that I have done, and I will work\nmy way through this to the question of what"},"158":{"dur":6,"text":"else could we do and round up with some final\nthoughts which are not, I will spoil the entire"},"165":{"dur":7,"text":"thing, my final thoughts will not be 100%\nfavourable towards LDA, so if that\u2019s what"},"173":{"dur":4,"text":"you wanted to know whether I like it or not\nthen you can feel free to leave you know that\u2019s"},"177":{"dur":1,"text":"it that\u2019s the end."},"179":{"dur":5,"text":"An additional disclaimer regarding the discourse\npragmatics of the preposition \u201ctowards\u201d"},"184":{"dur":5,"text":"in academic plenary titling contexts towards\na critical evaluation in my title means this"},"189":{"dur":5,"text":"is not a complete critical evaluation it\u2019s\njust you know, going in that direction, so"},"195":{"dur":3,"text":"no claim that this is a complete critical\nanalysis."},"199":{"dur":8,"text":"Ok so, dimensions, dimensionality of data,\nagain I know I\u2019ve got a very broad audience"},"208":{"dur":5,"text":"here in terms of how specialised people are\nin maths and statistics so for some of you"},"213":{"dur":4,"text":"this will be new and for some of you this\nwill be a ridiculous over simplification to"},"217":{"dur":4,"text":"the latter group I apologise, but let\u2019s\nthink of it this way, let\u2019s say we\u2019ve"},"221":{"dur":6,"text":"got a corpus of six texts and we look at the\nfrequency of \u201ca\u201d and the frequency of"},"228":{"dur":5,"text":"\u201cthe\u201d in each of the text, right the frequency\nof \u201cthe\u201d the frequency of \u201ca\u201d and"},"233":{"dur":4,"text":"we\u2019re interested to see if these two things\ncorrespond, I mean intuitively you might say"},"238":{"dur":4,"text":"probably they would if you\u2019ve got lots of\nnoun phrases for instance you\u2019ll have lots"},"242":{"dur":3,"text":"of \u201cthe\u201d and lots of \u201ca\u201d, if you have\nfew noun phrases then you\u2019ll have fewer,"},"246":{"dur":5,"text":"so we might expect them to kind of trend together\nand so you could plot your frequency of \u201ca\u201d"},"251":{"dur":5,"text":"along here, you could plot your frequency\nof \u201cthe\u201d up there, and you could put each"},"257":{"dur":6,"text":"text at a dot indicating where it is, so text\none is here that frequency of \u201ca\u201d that"},"263":{"dur":5,"text":"frequency of \u201cthe\u201d ok so hopefully we\u2019re\nnow, still at the level of like you know pre-GCSE"},"269":{"dur":2,"text":"Maths so everyone\u2019s following me."},"271":{"dur":4,"text":"And we can see that there\u2019s actually some\nkind of trend there of course there is because"},"276":{"dur":3,"text":"I\u2019ve made up this data to illustrate the\npoint and so I could make up whatever I liked"},"280":{"dur":5,"text":"so I made up a trend, but we can see that\nthe two things go together, but you could"},"285":{"dur":4,"text":"ask well but what if we think about other\nthings that come at the start of noun phrases"},"290":{"dur":5,"text":"like for example the frequency of \u201csome\u201d,\nargh well it\u2019s easy you just add another"},"295":{"dur":6,"text":"dimension so now instead of a two way graph\nwe\u2019ve got a three way graph where that represents"},"302":{"dur":4,"text":"the frequency of \u201cthe\u201d that represents\nthe frequency of \u201ca\u201d and then going backwards"},"306":{"dur":5,"text":"represents the frequency of \u201csome\u201d and\nyou\u2019ve got this kind of off angle third"},"311":{"dur":4,"text":"dimension going backwards, yeah, easy."},"316":{"dur":4,"text":"But what if we want to look at \u201cthis\u201d\nand \u201cthat\u201d and so on and so forth, well"},"320":{"dur":4,"text":"we\u2019re in trouble then because the human\nspaceial visualisation system isn\u2019t very"},"325":{"dur":6,"text":"good at more than three dimensions, but theoretically\nif we\u2019re measuring things we can just keep"},"331":{"dur":3,"text":"on adding dimensions couldn\u2019t we as long\nas there are words to add we can add more"},"335":{"dur":4,"text":"dimensions, we could have say a dimension\nfor every type of determine there is, I don\u2019t"},"340":{"dur":5,"text":"know how many there are let\u2019s say 15 that\u2019s\nI have no idea so we\u2019ve got a 15 dimensional"},"345":{"dur":6,"text":"space measuring the frequencies of determinates\nand if you can visualise a 15 dimensional"},"351":{"dur":5,"text":"space I guess you\u2019re in the wrong profession,\nthe pure mathematicians are recruiting."},"356":{"dur":13,"text":"So, yeah, we can\u2019t really do the graph thing,\nand what if, what if, instead of just looking"},"370":{"dur":5,"text":"at determiners you say I want to look at all\nwords, I want to look at every word type in"},"375":{"dur":7,"text":"the corpus measure it\u2019s frequency and compare\nthat in my graph of spaces, well that means"},"383":{"dur":6,"text":"that you\u2019ve got a dimension in your space\nfor the frequency of every type in your corpus"},"389":{"dur":5,"text":"so here are a few corpora that I happen to\nhave available on CQP web, I have a secret"},"394":{"dur":3,"text":"view that no-one else has which lists all\nthe type counts in all the corpora so I just"},"398":{"dur":5,"text":"got that from there, so there\u2019s a few of\nthese here and you can see that the very,"},"404":{"dur":5,"text":"the very smallest number of types comes from\nthe American English 2006 Corpus which are"},"409":{"dur":6,"text":"the two greatest there Amanda and Paul they\u2019re\nhere, so 53,000, 54,000 types, that means"},"416":{"dur":4,"text":"that if you use, the frequency of each of\nthose as something you want to put on one"},"420":{"dur":7,"text":"of these multidimensional graphs you\u2019ve\ngot a 54,000 dimensional space. I don\u2019t"},"428":{"dur":3,"text":"think even you know like string theorists\nhave that many dimensions I think they usually"},"431":{"dur":6,"text":"top out at about 11, so ok so we\u2019ve got\n54,000 dimensions if we have 54,000 types"},"438":{"dur":5,"text":"but bringing in big data, because we just\nheard about it, well big data isn\u2019t actually"},"444":{"dur":4,"text":"isn\u2019t really like AmE2006 or BE2006 not\nthe small data by modern standards, big data\u2019s"},"449":{"dur":5,"text":"much more like the UK Weber\u2019s corpus 1 billion\nwords, that\u2019s a 50% sample the full thing"},"454":{"dur":9,"text":"is 2 billion but in 1 billion words, six million\ntypes so we\u2019ve got 6 million 800,000 dimensions,"},"464":{"dur":4,"text":"this gets ridiculously hard to manage, even\nif you had the kind of brain that could visualise"},"468":{"dur":3,"text":"this how the hell are you going to analyse\nit, how the hell are you going to pick out"},"472":{"dur":5,"text":"the correspondences that are easy to spot\nor not spot with two dimensions, may be even"},"478":{"dur":4,"text":"three but beyond that it just becomes impossible."},"482":{"dur":6,"text":"So what we can do is we can talk about these\nword frequency things as vectors and a vector"},"489":{"dur":6,"text":"simply means a sequence of numbers each of\nwhich represents a distance on another dimension,"},"495":{"dur":6,"text":"and when you\u2019re talking about word frequencies\nas the dimensions then the distance along"},"502":{"dur":4,"text":"that dimension is the frequency of the word\nright, so the frequency of the word gives"},"507":{"dur":5,"text":"you basically a long set of coordinates so\nlet\u2019s say we count the frequency of all"},"512":{"dur":8,"text":"our word types and then we take our regular\nnormalised frequencies for that in text one"},"520":{"dur":5,"text":"and we get our numbers and we go right the\nway down the alphabet so \u201cA\u201d is 0.03 because"},"525":{"dur":2,"text":"it\u2019s \u201cA\u201d it\u2019 really frequent and then\nyou\u2019ve got \u201cAbacus\u201d \u201cAlien\u201d \u201cAntelope\u201d"},"528":{"dur":5,"text":"\u201cAardvark\u201d so on and so forth right the\nway down the alphabet, that gives us a vector"},"534":{"dur":6,"text":"which positions the text or corpus in the\n50,000 dimensional space, ok, so just like"},"540":{"dur":4,"text":"an \u201cx\u201d \u201cy\u201d coordinate two numbers\npositions you in two dimensional space, these"},"545":{"dur":3,"text":"50,000 numbers position this text in that\nspace."},"549":{"dur":4,"text":"And then we can have another vector for each\nof the texts that we are looking at so we\u2019ve"},"553":{"dur":4,"text":"got a vector a vector, a vector, different\ntexts then have a vector that define them,"},"558":{"dur":5,"text":"put them in a point in space, each number\nin the vector we can also call a feature frequency"},"563":{"dur":4,"text":"and the features in this case are our word\nfrequencies, is everyone following along?"},"568":{"dur":4,"text":"I know this is not necessarily the easiest\nconcept, but once you get it, it\u2019s fine."},"573":{"dur":4,"text":"And then another thing to introduce from I\nthink I\u2019m up to \u201cA\u201d Level Maths here"},"577":{"dur":4,"text":"rather than GCSE Maths, is the idea of a matrix\nfeatures can be more than just words that\u2019s"},"582":{"dur":4,"text":"something I want to come back to and basically\nwhen you put your vectors together where you\u2019ve"},"586":{"dur":4,"text":"got your type frequencies down, you\u2019ve got\nyour types down here and then your bi-text"},"590":{"dur":5,"text":"frequencies across here you\u2019ve got yourself\na feature matrix, right so each word is a"},"596":{"dur":5,"text":"feature the texts are then the data objects\nand you\u2019ve got a frequency score for each"},"601":{"dur":7,"text":"one right across the way, so if you\u2019ve got\n50,000 words and let\u2019s say 2,000 texts,"},"608":{"dur":5,"text":"that\u2019s 100,000 or a million I\u2019ve lost\ncount of the zeros anyway, a huge number of"},"614":{"dur":1,"text":"numbers."},"615":{"dur":4,"text":"Ok, so the question is how do we interpret\nthis because we want to know something about"},"620":{"dur":4,"text":"how these different frequency features relate\nto one another how they define the texts in"},"624":{"dur":5,"text":"this massive conceptual space, how do we actually\nanalyse this, well one of the first steps"},"629":{"dur":5,"text":"is to say do we really want 50,000 dimensions\nand the answer is, well how many dimensions"},"635":{"dur":5,"text":"is too many, well I don\u2019t know exactly where\nyou\u2019d cut it off but 50,000 is definitely"},"640":{"dur":4,"text":"too many dimensions, so the first step that\nyou want to take with this kind of data a"},"644":{"dur":5,"text":"feature matrix like that is to come up with\nsome way of saying let\u2019s recast it into"},"650":{"dur":4,"text":"fewer dimensions lower complexity data."},"655":{"dur":4,"text":"And so that\u2019s what I\u2019m calling that\u2019s\nwhat kind of inspired the title at the top,"},"659":{"dur":6,"text":"wordform vectors, wordform matrixes and their\nfrequencies how do we deal with it? Well as"},"665":{"dur":6,"text":"Susan was actually explaining in the pre-conference\ntalk yesterday, there are a variety of exploratory"},"672":{"dur":6,"text":"methods and have been used over the years\nand many of these are essentially purely quantitative,"},"678":{"dur":5,"text":"right, they are number crunching methods which\nuse the tendencies in the numbers that are"},"684":{"dur":5,"text":"observed to group features which in this case\nmeans wordform frequencies to capture the"},"689":{"dur":3,"text":"trends and the variation across this whole\nmatrix."},"693":{"dur":5,"text":"So for instance if we observe that \u201cthe\u201d\nand \u201ca\u201d always get more frequent or less"},"698":{"dur":7,"text":"frequent together as in my original graph\nwe can say, ah ha, those actually belong together,"},"706":{"dur":4,"text":"they are not too different dimensions that\ncross but at right angles they\u2019re actually"},"710":{"dur":6,"text":"moving in parallel so they\u2019re one dimension,\nso there we are we\u2019ve got down from 50,000"},"717":{"dur":4,"text":"to 49,999 but the statistical number crunching\nmethods do this systematically and get us"},"721":{"dur":2,"text":"down to a much simpler model by grouping things."},"724":{"dur":4,"text":"So here are a few of these quantitative number\ncrunching methods that we know about, factor"},"729":{"dur":5,"text":"analysis, anyone who\u2019s read Biber 1988 or\nmany of his others on multidimensional analysis"},"734":{"dur":5,"text":"will know about factor analysis which reduces\nmany variables such as our word frequencies"},"739":{"dur":4,"text":"to just a few factors and the best way to\nthink of what a factor is, it\u2019s a super"},"744":{"dur":5,"text":"variable, it\u2019s a variable that has absorbed\nand conquered many lesser variables."},"749":{"dur":3,"text":"Cluster analysis is another way of doing it,\nthis is where instead of grouping together"},"753":{"dur":5,"text":"the dimensions you use all the dimensions\nat once to calculate distances for the objects"},"759":{"dur":4,"text":"apart from one another, so how far away are\nthe texts taking into account all these dimensions"},"763":{"dur":5,"text":"from one another and then based on those distances\nyou group them together into different clusters"},"769":{"dur":4,"text":"and then you work out how those clusters are\nrelated by distance and eventually you assemble"},"773":{"dur":4,"text":"a big tree of clusters of all your data objects,\nour data objects in this case remember are"},"778":{"dur":1,"text":"our texts."},"779":{"dur":6,"text":"And finally those two are kind of very widely\nused but then this brings us close towards"},"786":{"dur":5,"text":"topic modelling we have assorted data mining\nmethods where we use some kind of machine"},"791":{"dur":5,"text":"learning of a model to statistically summarise\nall the variables in some way."},"796":{"dur":3,"text":"Now there is actually another one that we\ncould stick on there which would be linguistic"},"799":{"dur":6,"text":"analysis of the variables, what do I mean\nby that, well, linguistic analysis of the"},"806":{"dur":4,"text":"features, in this case the wordforms would\nbe to say use what we already know about language"},"811":{"dur":4,"text":"to avoid this massive wordform vector in the\nfirst place, so for instance we know that"},"815":{"dur":3,"text":"\u201cthe\u201d and \u201ca\u201d are articles, so why\nmeasure them separately to begin with, stick"},"819":{"dur":4,"text":"them together because we know they belong\ntogether, so for instance this is how you"},"823":{"dur":7,"text":"can use tagging to or lemmatisation to reduce\nthe number of features, so rather than 50,000"},"831":{"dur":5,"text":"word type frequencies you\u2019ve got maybe a\n135 POS tag frequencies, or a couple of hundred"},"837":{"dur":5,"text":"semantic tag frequencies, lemmatisation is\nsimilar although obviously for a language"},"842":{"dur":3,"text":"like English which doesn\u2019t have very much\ninflection lemmatisation doesn\u2019t reduce"},"846":{"dur":3,"text":"your dimensions all that much. But basically\nall of these have the same idea, we\u2019ve got"},"850":{"dur":5,"text":"these ways of annotating our data which are\nbased on sophisticated amounts of linguistic"},"855":{"dur":3,"text":"knowledge that have been built into the system\nby the people who created them so we could"},"859":{"dur":4,"text":"use them to collapse our wordform vector down\ninto a more manageable vector of something"},"863":{"dur":1,"text":"else."},"865":{"dur":8,"text":"And this keys into a point that Susan was\nmaking yesterday when she described a topic"},"874":{"dur":5,"text":"model I think was a na\u00efve linguist, a linguistically\nna\u00efve method, so essentially this then would"},"880":{"dur":6,"text":"be your non-na\u00efve approach to addressing\nthe wordform frequencies, whereas the previous"},"887":{"dur":4,"text":"set data mining, factor analysis, cluster\nanalysis, that would be all be na\u00efve because"},"892":{"dur":3,"text":"it\u2019s purely chunked down as I say crunching\nthe numbers."},"895":{"dur":4,"text":"So yes, but let\u2019s assume that we\u2019re putting\naside linguistic knowledge for now, and we"},"900":{"dur":7,"text":"just want to do some non-number crunching,\nwe could do some machine learning, some data"},"907":{"dur":5,"text":"mining and it could be unsupervised, in machine\nlearning, unsupervised has a specialised meaning,"},"912":{"dur":4,"text":"it means that there isn\u2019t a target analysis\nthat we know is correct that we\u2019re trying"},"917":{"dur":5,"text":"to get our machine learning system to produce,\ninstead we\u2019re trying to get it to produce"},"922":{"dur":3,"text":"something and we don\u2019t necessarily know\nwhat until we get it."},"926":{"dur":4,"text":"Topic modelling is an example of this and\nthis is where I have to put in one of my provisos"},"931":{"dur":5,"text":"there\u2019s all kinds of different topic modelling\nthings, today I am not going to attempt to"},"936":{"dur":5,"text":"even address any of them other than Latant\nDirichlet Analysis because there\u2019s just"},"942":{"dur":1,"text":"not enough time."},"943":{"dur":3,"text":"But you may have heard of something called\nSymantec Vector Space models if you\u2019ve been"},"947":{"dur":3,"text":"listening to the kinds of people who talk\nabout these things it\u2019s very similar in"},"951":{"dur":2,"text":"principle but different in detail."},"953":{"dur":6,"text":"So yes Laten Dirichlet Analysis (LDA) what\nis it and why do we care? Well I\u2019m sure"},"960":{"dur":6,"text":"you\u2019ve heard about topic modelling, LDA\nis one form of it as I\u2019ve said, topic modelling"},"966":{"dur":5,"text":"is what I consider to be the epitome of the\n\u201cbig data\u201d approach because it\u2019s data"},"971":{"dur":5,"text":"mining right it\u2019s in we go we just chunk\nwe crunch through our numbers roll, roll,"},"976":{"dur":5,"text":"roll, roll, roll, something comes out at the\nend and you know it\u2019s great, it\u2019s machine"},"982":{"dur":4,"text":"learning it\u2019s number crunching and it was\noriginally developed for computer science"},"987":{"dur":5,"text":"purposes so for instance looking at product\nreviews on sites like Amazon, if you model"},"992":{"dur":3,"text":"the topics you can find out what each review\nis talking about, that\u2019s what they were"},"995":{"dur":7,"text":"doing, and it has become part of the standard\nmachine learning toolkit, but what you see"},"1003":{"dur":4,"text":"is that topic modelling once it gets away\nfrom the people who invented it, you see it"},"1007":{"dur":5,"text":"also being applied to relatively small amounts\nof data, and especially in humanities studies,"},"1013":{"dur":6,"text":"there\u2019s a trend and since I\u2019m at a CL\nConference rather than a humanities conference"},"1019":{"dur":7,"text":"I can say this, there is a trend when humanists,\nhistorians or literary scholars, when they"},"1027":{"dur":4,"text":"get to know, I should be doing something digital,\nso they go across to the computer scientist"},"1032":{"dur":3,"text":"and they say I\u2019ve got all these texts what\ncan I do, and the computer scientists say"},"1035":{"dur":4,"text":"ah ha, use topic modelling, there you are\ndone, problem solved."},"1040":{"dur":7,"text":"I\u2019ve seen this dynamic happen not necessarily\nexactly like that but I\u2019ve seen it happen,"},"1047":{"dur":4,"text":"any computer scientists in this room are excused\nfrom that over broad generalisation simply"},"1051":{"dur":3,"text":"by virtue of turning up at CL you have proven\nthat you\u2019re not the sort of person to do"},"1055":{"dur":6,"text":"that kind of thing, however there are lots\nof computer scientists out there who would."},"1061":{"dur":7,"text":"And so, there\u2019s been a big move in digital\nhumanities to look at topic modelling for"},"1068":{"dur":6,"text":"relatively small amounts of texts, the classic\none which you may have heard of was published"},"1075":{"dur":7,"text":"on line rather than in a journal, a study\nof a early 19th century midwife\u2019s diary"},"1082":{"dur":6,"text":"entries via topic modelling, and I\u2019ve forgotten\nthe surname of the guy who did it unfortunately,"},"1089":{"dur":2,"text":"and of course because it wasn\u2019t published\nI don\u2019t have a reference, if anyone wants"},"1091":{"dur":5,"text":"to find that then just let me know and I\u2019ll\ndig up the blog post for you."},"1096":{"dur":4,"text":"Anyway so it\u2019s spread kind of through the\ndigital humanities in this way for people"},"1101":{"dur":4,"text":"looking at their historical text collections\nit\u2019s also now being used in CL as we\u2019ve"},"1105":{"dur":6,"text":"seen with the Murakami et al paper which Susan\nintroduced yesterday, and I should say at"},"1111":{"dur":4,"text":"this point that if it sounds like I\u2019m being\ncritical I am not, I was, I\u2019ve read the"},"1116":{"dur":4,"text":"Murakami et al paper it\u2019s not out yet but\nI found it on a Birmingham Research Archive"},"1120":{"dur":7,"text":"Survey so I read it there and I thought this\nis the best topic modelling paper I have read"},"1128":{"dur":4,"text":"because all the topic modelling presentations\nthat I\u2019ve seen at digital humanities conferences"},"1132":{"dur":6,"text":"over the past few years I got ooopphh at right,\nbut unfortunately, sorry Susan there is kind"},"1139":{"dur":4,"text":"of a sting in the tale sorry Paul sorry everyone\nbut I think that even getting everything right"},"1144":{"dur":3,"text":"in topic modelling there\u2019s still some hard\nproblems that you run up against at the end"},"1147":{"dur":2,"text":"of the day, I\u2019m going to come to that."},"1150":{"dur":5,"text":"So how does discourse and topics \u201cwork\u201d\nin topic modelling and LDA what is a topic?"},"1155":{"dur":4,"text":"Well Susan said this yesterday it\u2019s not\na topic in the linguistic sense it\u2019s purely"},"1160":{"dur":5,"text":"a model in the sense of a group of weightings\nwhich satisfies a stochastic model, what does"},"1165":{"dur":6,"text":"that mean? Well it means that there is a kind\nof generative theory of how discourse works"},"1172":{"dur":3,"text":"which I shall now explain."},"1175":{"dur":6,"text":"Topic modelling works on the following assumptions,\na writer is someone who sits down and decides"},"1181":{"dur":9,"text":"my text will consist of words from this bin\nof words this bin of words, this bin of words"},"1190":{"dur":4,"text":"and this bin of words, and I\u2019ll take half\nmy text from this bin of words, half my text"},"1195":{"dur":5,"text":"from this bin of words and then the other\nbins not you know just one or two, so they"},"1201":{"dur":6,"text":"walk over to the bin and they pick out from\nthe bin as many words as they need so if they\u2019re"},"1207":{"dur":7,"text":"writing a 1,000 word text and they want 50%\nfrom this bin of words they pick out 500 words"},"1214":{"dur":4,"text":"from the bin at random, now all the words\nyou can imagine them being in a big urn written"},"1218":{"dur":5,"text":"on little scrabble tiles, right, and some\nof the words are duplicated right, so some"},"1224":{"dur":6,"text":"words appear in the bin a huge urn like hundreds\nand hundreds of times and other words occur"},"1230":{"dur":4,"text":"on only one tile so they\u2019re less likely\nto be pulled out, but the writer pulls out"},"1234":{"dur":5,"text":"500 tiles and then goes off to the other bins\nand they pick out 100 here, 300 here whatever,"},"1239":{"dur":4,"text":"and then they take them back to the desk dump\nthem on the table that\u2019s a text, that\u2019s"},"1244":{"dur":7,"text":"how writing works in this view, ok, so we\u2019ve\ngot bins our bins our urns are our topics"},"1251":{"dur":5,"text":"and writing is done by pulling randomly from\nbins in the proportions that were decided"},"1256":{"dur":5,"text":"on at the start, ok that\u2019s the theory of\nhow writing works. That\u2019s the generative"},"1261":{"dur":1,"text":"theory of discourse."},"1263":{"dur":6,"text":"The way topic model system works is alright\ngiven that that\u2019s how writers work we know"},"1270":{"dur":7,"text":"what the text looks like when they\u2019ve finished\nbased on number crunching can we work out"},"1277":{"dur":7,"text":"what words were in the bins to start with,\nso the topic model uses problemolistic algorithms"},"1284":{"dur":5,"text":"to come up with a best guess as to what word\ntiles are in each of these bins that would"},"1290":{"dur":3,"text":"have given rise to the text that the writer\ncame up with yeah."},"1293":{"dur":7,"text":"Now the way, oh dear, I\u2019ve gone passed one,\nyes so it works backwards, right, the idea"},"1301":{"dur":5,"text":"is we know the text and we want to know what\nthe bins are, the bins, I\u2019m saying bins"},"1306":{"dur":3,"text":"because I want to avoid the word topic for\nthe moment."},"1310":{"dur":5,"text":"Now the fact that it\u2019s working backwards\nis interesting it has implications which I\u2019m"},"1316":{"dur":3,"text":"going to come to in a minute but that\u2019s\nwhat topic modelling is, you give it the text,"},"1319":{"dur":4,"text":"you say find out what the bins the urns would\nhave to contain, the computer crunches it"},"1324":{"dur":4,"text":"and says you\u2019re bins would have to contain\nthese things or here\u2019s my best guess as"},"1328":{"dur":4,"text":"to what the bins contain based on the text\nand that list of bins and what they have in"},"1333":{"dur":2,"text":"them is your topic model."},"1335":{"dur":6,"text":"So topic modelling have two problems, the\nfirst is it\u2019s easy to do wrong or to misunderstand"},"1342":{"dur":4,"text":"and it\u2019s like pie charts in Microsoft Excel,\nas some people will know I hate pie charts,"},"1347":{"dur":4,"text":"there\u2019s almost never a good reason to use\na pie chart unfortunately Microsoft Excel\u2019s"},"1351":{"dur":4,"text":"interface is set up so that when you reach\nfor a chart pie charts are right there at"},"1355":{"dur":5,"text":"the top of the list so everyone goes, \u201coh\npie charts poof\u201d pie charts are extraordinarily"},"1361":{"dur":6,"text":"easy to misunderstand or do wrong, similarly\ntopic modelling, again I\u2019m thinking about"},"1367":{"dur":5,"text":"the Murakami et al paper in that paper when\nyou read it, it comes out soon, you\u2019ll see"},"1372":{"dur":5,"text":"that they do things like checking out multiple\ndifferent topic models with different numbers"},"1378":{"dur":6,"text":"of topics i.e. working on 50 bins, 60 bins,\n70 bins and see which one works best, most"},"1385":{"dur":5,"text":"topic modelling analysis don\u2019t do that,\nthey think carefully about the stock list,"},"1390":{"dur":5,"text":"a stock list is when you take out the function\nworks, most topic models just use the built"},"1395":{"dur":3,"text":"in stock lists and say \u201cyeah that\u2019s fine\u201d\nbut the Murakami et al paper actually things"},"1399":{"dur":5,"text":"carefully about it, there\u2019s all these kinds\nof things in that paper which most topic modelling"},"1404":{"dur":2,"text":"analyses do not do."},"1407":{"dur":5,"text":"So you can get past these by being careful\nlike Susan and Paul and crew were, but even"},"1413":{"dur":7,"text":"if you don\u2019t do it wrong there are two hard\ndrawbacks and they are randomness and transparency."},"1421":{"dur":5,"text":"Problems in the concept arise from this transparency\nissue, the issue in interpreting the topic"},"1426":{"dur":3,"text":"modelling and I have to thank Mathew Gillings\nhere who is writing a paper about this but"},"1430":{"dur":4,"text":"he let me read it in advance because he did\na bit study not of the actual topic modelling"},"1435":{"dur":4,"text":"but of how the interpretation of it works,\nso there\u2019s all kinds of issues in what can"},"1439":{"dur":4,"text":"go wrong when you\u2019re sitting there with\nthe output in front of you and trying to interpret"},"1443":{"dur":1,"text":"it."},"1444":{"dur":3,"text":"So that\u2019s one problem, another problem is\nthat you\u2019ve got arbitrary parameters in"},"1448":{"dur":5,"text":"the set-up, that is the topic modelling procedure\ncannot decide how many topics there should"},"1454":{"dur":6,"text":"be you have to tell it as I said before and\nit\u2019s astonishing when I\u2019ve looked in papers"},"1460":{"dur":5,"text":"people recommend kind of standard numbers\nthat are completely different like 20 for"},"1465":{"dur":3,"text":"instance is often recommended as a reasonable\nplace to start but then if you look at another"},"1469":{"dur":5,"text":"sort it will say oh well you should use somewhere\nbetween 100 and 400 topics as I said the Murakami"},"1474":{"dur":4,"text":"et al paper does ranges between I think is\nit about 50 and 200 that you did have some"},"1479":{"dur":5,"text":"feeling that something around that you looked\nat lots of different intervals."},"1484":{"dur":3,"text":"So you know there\u2019s lots of different versions\non this and Murakami et al there\u2019s another"},"1488":{"dur":3,"text":"parameter which is the number of iterations\nagain most people don\u2019t tinker with that"},"1492":{"dur":3,"text":"they just accept the single, the number of\niterations is how many times is the algorithm"},"1496":{"dur":5,"text":"going to go round and round and round in trying\nto work backwards towards the original bins."},"1501":{"dur":4,"text":"And these things range a whole lot more than\nsay a factor analysis, in a factor analysis"},"1506":{"dur":3,"text":"you\u2019ve got to pick the number of factors\nyou want to get out of the factor analysis,"},"1510":{"dur":4,"text":"do you want five or six or seven or eight,\nbut you\u2019ve only got a few numbers to choose,"},"1514":{"dur":4,"text":"we can see that the number of bins that you\nhave in a topic model ranges way more than"},"1518":{"dur":5,"text":"that, right and if you look at 50 and you\nlook at 60 which you might think yeah I\u2019ve"},"1523":{"dur":4,"text":"done well but what about 51, 52, 53, 54, if\nyou don\u2019t look you don\u2019t know but you"},"1527":{"dur":3,"text":"can\u2019t possibly look at that many different\ntopic models."},"1531":{"dur":5,"text":"So it\u2019s very difficult to have a warranted\nvalue, stock lists as I\u2019ve said almost all"},"1537":{"dur":5,"text":"topic modelling uses this, Murakami et al\nas I said is one of the few papers that I\u2019ve"},"1542":{"dur":3,"text":"seen that thinks hard about a stock list,\nI don\u2019t like stock lists but they think"},"1546":{"dur":5,"text":"hard about it. Matt\u2019s paper forthcoming\nshows that it really does make a big difference"},"1552":{"dur":1,"text":"whether you use a stock list or not."},"1553":{"dur":6,"text":"So we\u2019ve got to think again about what does\na topic mean, I kind of skimmed over that"},"1560":{"dur":3,"text":"well when people interpret topics they say\nthings like \u201cwell it\u2019s not necessarily"},"1564":{"dur":4,"text":"a type of content\u201d like topic would suggest\nit could also be a style, it could also be"},"1569":{"dur":6,"text":"a register, in terms of the algorithm as I\nsaid it\u2019s just a bin, a bin of words, and"},"1575":{"dur":3,"text":"I heard this from a CS researcher who I\u2019m\nnot going to name because I can\u2019t remember"},"1579":{"dur":4,"text":"the quote verbatim but it was at a conference\nwe should probably call it discourse modelling"},"1583":{"dur":4,"text":"rather than topic modelling since the groups\nof words it puts together are more like discourses"},"1587":{"dur":6,"text":"than they are like topics and my, I was just\ngoing whaaaaaa if there is a discourse analyst"},"1594":{"dur":4,"text":"in this room at this moment they would be\njust going bonkers and saying no a discourse"},"1598":{"dur":5,"text":"is not words that have been allocated together\nby a machine learning model."},"1604":{"dur":4,"text":"So yeah it would be a low blow however great\nthe temptation, to imply that when many data"},"1608":{"dur":3,"text":"miners say \u201ctopic\u201d or \u201csemantic\u201d what\nthey really mean is \u201cany algorithm that"},"1612":{"dur":3,"text":"spares me from having to understand, or care,\nwhat terms like \u2018topic\u2019 or \u2018semantic\u2019"},"1616":{"dur":6,"text":"actually mean, so I won\u2019t say that."},"1622":{"dur":5,"text":"Randomness, this working backwards procedure\nsomething that people don\u2019t always grasp"},"1627":{"dur":6,"text":"is that it involves randomness, the first\nstep is to randomly make a guess rolling dice"},"1634":{"dur":7,"text":"as to what words are in each bin then that\u2019s\nevaluated the probability weightings are adjusted"},"1641":{"dur":4,"text":"and then it\u2019s randomised again, and then\nagain and then again, and then again, so it\u2019s"},"1645":{"dur":3,"text":"random at each iteration but the assumption\nis that because you\u2019re adjusting the weightings"},"1649":{"dur":6,"text":"the randomness is going to be converging to\nsome kind of underlying reality. What this"},"1655":{"dur":5,"text":"means is that LDA is non-deterministic and\nI have to credit Derek Bridge here who again"},"1660":{"dur":3,"text":"2017 that\u2019s not a paper it was a talk he\nreally did underline for me how important"},"1664":{"dur":5,"text":"that was and in the room at the time I realised\nthat many people were doing topic modelling"},"1670":{"dur":3,"text":"this came as a surprise and for some of them\nit was a nasty surprise, I was in a room full"},"1674":{"dur":3,"text":"of historians and they were like oh topic\nmodelling, topic modelling we\u2019ve got to"},"1677":{"dur":3,"text":"do this it\u2019s digital humanities and then\nDerek Bridge stood up and he gave this talk"},"1680":{"dur":3,"text":"because he\u2019s a topic modelling person he\ngave this talk to explain how it actually"},"1684":{"dur":6,"text":"works and by the end of it the historians\nwere like, well obviously we can\u2019t do topic"},"1690":{"dur":4,"text":"modelling then if it\u2019s non-deterministic\nbut what can we do instead what else is there?"},"1694":{"dur":4,"text":"This was the conversation that was going on\nin that room, what does non-deterministic"},"1699":{"dur":3,"text":"mean in this concept, there\u2019s an element\nof randomness because we start with random"},"1702":{"dur":2,"text":"topics it\u2019s going to get randomised as we\ngo along."},"1705":{"dur":5,"text":"I\u2019m way overtime already so, Gigantopithecus\nsays, \u201cFor scholars in some disciplines,"},"1710":{"dur":5,"text":"a method being non-deterministic involving\nrandomness is an instant, absolute dealbreaker\u201d"},"1716":{"dur":4,"text":"now because we do quantitative stuff in corpus\nlinguistics maybe it\u2019s not such an instant"},"1721":{"dur":3,"text":"dealbreaker for us but it\u2019s certainly something\nthat we should be aware of and I\u2019m going"},"1725":{"dur":1,"text":"to illustrate it later."},"1726":{"dur":6,"text":"Now the practical bit, Andrew does a topic\nmodel, I use Mallet for this I am underlining"},"1732":{"dur":4,"text":"this because again lots of the best papers,\nMurakami et al use the systems built into"},"1737":{"dur":4,"text":"R, but I was thinking about the digital humanities\nalmost all the digital humanists who do this"},"1742":{"dur":5,"text":"sort of thing that I\u2019ve seen they use Mallet\nso I use Mallet, it\u2019s easy to use by the"},"1747":{"dur":1,"text":"way it\u2019s not difficult."},"1748":{"dur":5,"text":"Data, the data that I use for all of this\nwas the J section of FLOB as described by"},"1754":{"dur":7,"text":"Hundt et al and is Mariana here? Oh yes I\njust wanted to say thank you so much for the"},"1762":{"dur":6,"text":"FLOB manual it was such a great help in this,\nanyway so Genre J the learned genre academic"},"1768":{"dur":6,"text":"writing but perhaps more science than humanities\narts because in the FLOB brown scheme humanities"},"1774":{"dur":5,"text":"arts writing also turns up under Belles Lettres.\nThis is 80 texts, each 2,000 words, period"},"1779":{"dur":6,"text":"1991, UK English. Why FLOB J, because it\u2019s\ntractable, it\u2019s a relatively small amount"},"1786":{"dur":4,"text":"of data so I could keep control of what was\ngoing on and do checks and it\u2019s familiarly"},"1791":{"dur":2,"text":"I would guess that there\u2019s probably very\nfew people from this room who\u2019ve never done"},"1794":{"dur":2,"text":"anything with FLOB ever."},"1796":{"dur":5,"text":"So what I decided to do was a simple LDA analysis\nof FLOB using all the default settings, I"},"1801":{"dur":5,"text":"wasn\u2019t going to do what Susan and crew did\nbecause again the point was to work through"},"1806":{"dur":6,"text":"the way that the average user of topic modelling\ndoes. And then I cunningly deleted half of"},"1813":{"dur":6,"text":"my slide so what this did say was that I was\ndoing my analysis by the standard procedure"},"1819":{"dur":4,"text":"of getting the topics, looking at the top\nwords of each topic, eyeballing them and then"},"1824":{"dur":3,"text":"trying to characterise their content, again\nif you read the Murakami et al paper they"},"1828":{"dur":4,"text":"go way beyond that but standard approach is\nthrough interpretation do not."},"1832":{"dur":5,"text":"So here\u2019s my topic model, I asked for 20\ntopics because that\u2019s the default it also"},"1837":{"dur":5,"text":"seems reasonable, 80 texts, 20 topics, seems\na reasonable thing and I said show me the"},"1843":{"dur":7,"text":"top 20 words for every topic, and this is\nwhat it gave me, now I don\u2019t know if you\u2019re"},"1851":{"dur":5,"text":"counting in your head but the slide doesn\u2019t\nactually fit all 20 words so here it is split"},"1856":{"dur":11,"text":"across two slides, topic zero to 9, topics\n10 to 19. Gigantopithecus says, that even"},"1867":{"dur":5,"text":"with just 80 texts and just 20 topics and\njust 20 words per topic and totally ignoring"},"1873":{"dur":3,"text":"everything that\u2019s hiding behind this because\nas we\u2019ll see there\u2019s a huge amount hiding"},"1877":{"dur":6,"text":"behind this, it\u2019s still pretty difficult\nto get all the data into eyeballable shape"},"1883":{"dur":7,"text":"to represent to an audience or indeed a readership\nand again the Murakami et al paper has wonderful"},"1891":{"dur":5,"text":"visualisations to try and get all of this\ninto the paper, most topic modelling studies"},"1896":{"dur":1,"text":"don\u2019t do that."},"1898":{"dur":4,"text":"Anyway, so I went down each one I read the\nwords and I tried to come up with some kind"},"1903":{"dur":5,"text":"of covering term for those 20 words, again\nthis is what people do when they\u2019re interpreting"},"1908":{"dur":5,"text":"topic models so I came up with material physics\noh and I love this number one academic argument"},"1913":{"dur":4,"text":"made, time, work, general, means set, part,\nfat, control common point and then you\u2019ve"},"1918":{"dur":4,"text":"got things later on which are about evidence\nand argument, that was really nice and it\u2019s"},"1922":{"dur":3,"text":"not a topic in the normal sense it\u2019s more\nlike one of these discoursey things."},"1926":{"dur":6,"text":"Economics, then some which were kind of compounds,\nlinguistics and child mortality ended up together,"},"1932":{"dur":5,"text":"transmission electronics, aircraft materials,\nart history, law and finance, chemistry, home"},"1937":{"dur":5,"text":"gender sex, health, that seemed to all be\ntogether, government environment this was"},"1942":{"dur":5,"text":"another argumentative one amounts of data,\nlarge system form, high, found raised, topic,"},"1947":{"dur":4,"text":"I didn\u2019t look at anything beyond this because\nagain that\u2019s not what is done and the digital"},"1952":{"dur":2,"text":"managers they only look at this."},"1954":{"dur":5,"text":"You\u2019ll see that there are three gaps, those\nwere the hard ones, I didn\u2019t think that"},"1959":{"dur":5,"text":"was bad, 17 easy, 3 hard well let\u2019s take\na look at two hard ones topic 11, those are"},"1964":{"dur":5,"text":"the words, language, knowledge, sense, theory,\nprinciple, relationship classes, reality meaning"},"1970":{"dur":5,"text":"play rules, narrative, definition class, fact\nform accept world game object. I was kind"},"1975":{"dur":4,"text":"of getting a sense of a bit of Wittgenstein\nbut also a bit of kind of formal linguistics"},"1980":{"dur":5,"text":"so I was like is this philosophy of semantics\nor something like that, that was my best attempt."},"1985":{"dur":5,"text":"Number 8, design, thalidomide, patients, aircraft,\nweight requirements, patent Searle, I sure"},"1990":{"dur":5,"text":"Searle belonged in 11 but never mind, performance,\nmusic, gvhd, clinical mass, and I\u2019m like"},"1996":{"dur":7,"text":"medicine, music, aircraft design, what, what\nI don\u2019t know."},"2003":{"dur":6,"text":"And topic 17, if you have a suggestion I\u2019d\nbe delighted to hear it I just couldn\u2019t"},"2010":{"dur":6,"text":"come up with anything for that one. Linguistics,\nanimals, discharge, animal and animals, so"},"2017":{"dur":4,"text":"there\u2019s something animals going on there,\nbut everything else doesn\u2019t fit, so yeah"},"2021":{"dur":3,"text":"I just called that hodgepodge."},"2024":{"dur":3,"text":"Now the idea is that you can then relate this\nto the texts so I thought ok I will pick two"},"2028":{"dur":4,"text":"texts at random and I did it by numbers I\ntook J25 and J50 not knowing in advance what"},"2033":{"dur":4,"text":"they were going to be and I did the analysis,\nI looked at because the other thing you get"},"2037":{"dur":4,"text":"from topic modelling is a list of the topics\nmost strongly associated with each text, so"},"2042":{"dur":5,"text":"here\u2019s J25 which is a paper from a Psychology\nJournal called the \u2018The effects of flicker"},"2047":{"dur":5,"text":"on eye movement control\u2019 what did the model\nsay were the bins that that Author had reached"},"2053":{"dur":5,"text":"for most heavily, well unfortunately, you\nknow fate being a cruel mistress, the top"},"2059":{"dur":8,"text":"and strongest topic for that was the Hodgepodge,\nnow let\u2019s go back obviously it\u2019s about"},"2067":{"dur":6,"text":"eye flicker and reading you know experimental\npsychology obviously right, right, ok, then"},"2074":{"dur":5,"text":"we\u2019ve got education, social, psychological\nthat makes more sense and then the academic"},"2079":{"dur":6,"text":"argument one ok, those two made sense, and\nthen I went for 50, and I found unfortunately"},"2086":{"dur":6,"text":"that that also is eeeerrr, so \u2018Rights\u2019\nit\u2019s a paper on Embryology but it\u2019s law"},"2092":{"dur":5,"text":"as well and so we have the academic argument\nand the amounts of data, amounts of data evidence"},"2097":{"dur":3,"text":"so that seemed like academic discourse, but\nthen we also had the one that I would struggle"},"2101":{"dur":4,"text":"for and called it philosophy of semantics\nand then the education one again, that seemed"},"2105":{"dur":4,"text":"to me to be a bit better in terms of getting\nthat text. Incidentally these are the statistically"},"2110":{"dur":2,"text":"strongest topics in each one."},"2112":{"dur":4,"text":"Now often people that do topic modelling they\nsay well if it goes wrong you should have"},"2116":{"dur":3,"text":"used more topics or you should use less topics,\nso I considered maybe I didn\u2019t use enough"},"2120":{"dur":5,"text":"topics so for example Topic 3 has got some\nEnglish and foreign ology in there but it\u2019s"},"2126":{"dur":5,"text":"also got infant mortality in there and words\nlike long which I suppose could be either"},"2131":{"dur":3,"text":"you could be calling it a long period of infant\nmortality or a long vowel so I\u2019m not quite"},"2135":{"dur":4,"text":"sure what\u2019s going on there, so maybe if\nI\u2019d had more topics those two things would"},"2140":{"dur":6,"text":"have been split apart. But then you\u2019ve got\nto think well this is only the top 20 every"},"2146":{"dur":5,"text":"topic has a score for every word in the corpus\nbut you only see the top 20, well maybe the"},"2151":{"dur":5,"text":"things below the top 20 although individually\nnot so strongly associated maybe cumulatively"},"2157":{"dur":3,"text":"they are up to a big difference and maybe\nI should have been looking at that, but again"},"2160":{"dur":4,"text":"that\u2019s not what topic modellers do generally."},"2165":{"dur":3,"text":"Linguistics and Economics seem to end up split\nacross two topics which suggests maybe I\u2019m"},"2168":{"dur":3,"text":"using too many topics of economics is being\nsplit into two places and linguistics is being"},"2172":{"dur":4,"text":"split into two places it sounds like things\nthat belong together have been dragged apart,"},"2177":{"dur":3,"text":"so you can see there I\u2019ve got economics\nin two of my topics and I\u2019ve got linguistics"},"2180":{"dur":4,"text":"in two and then they\u2019re in the Hodgepodge\nas well, so maybe there are actually too many"},"2185":{"dur":3,"text":"topics so, oh I\u2019ve got evidence both ways."},"2188":{"dur":4,"text":"Now as I\u2019ve said the cumulative effect of\nthings below the cut off, my cut off is the"},"2193":{"dur":4,"text":"top 20 words because that\u2019s what\u2019s usually\ndone, but what about what\u2019s below it can"},"2198":{"dur":4,"text":"add up to a lot, we don\u2019t know if the things\nwe haven\u2019t looked at might collectively"},"2202":{"dur":3,"text":"sway the analysis by definition we don\u2019t\nknow what the effect would be of the things"},"2206":{"dur":1,"text":"we haven\u2019t looked at."},"2207":{"dur":4,"text":"Now this problem I should say is one that\nwe as corpus linguists have kind of let slide"},"2212":{"dur":3,"text":"for a long while because people look at the\ntop of frequency lists they look at the top"},"2216":{"dur":3,"text":"of key words lists, they look at the top of\nn-gram lists particularly in lexical bundle"},"2219":{"dur":5,"text":"analysis and the huge problem is that although\neverything below the top is infrequent individually"},"2225":{"dur":4,"text":"it can cumulatively add up to something as\nimportant as something at the top. So that\u2019s"},"2229":{"dur":4,"text":"something that we\u2019ve been kind of lacks\non is corpus linguistics as corpus linguists."},"2234":{"dur":3,"text":"So here\u2019s a little quote from Gillings about\nthe interpretation of that, the labelling"},"2237":{"dur":4,"text":"of topics is inherently a decision made by\nthe researcher alone, for example depending"},"2242":{"dur":3,"text":"on the individual researcher a topic about\nbirds could be labelled either as birds, wildlife"},"2246":{"dur":5,"text":"or animals so if you thought my labels were\nshonky maybe they were but Matthew pointed"},"2251":{"dur":5,"text":"out that this is essentially subjective, there\u2019s\nno agreed taxonomy in fact as Susan was saying"},"2257":{"dur":3,"text":"yesterday the whole point is that you don\u2019t\nhave a taxonomy and it generates one, but"},"2260":{"dur":4,"text":"then that creates problems for interpretation.\nIt\u2019s a judgement call as Underwood noted,"},"2265":{"dur":8,"text":"Underwood, Underwood did the it was Underwood\nwho did the Midwife from the 1800\u2019s, ultimately"},"2273":{"dur":4,"text":"it\u2019s difficult to suggest that a lower number\nof topics results in a broad topic range because"},"2277":{"dur":4,"text":"topic levels are so subjective in the first\nplace, so the Gigantopitecus says, there are"},"2282":{"dur":4,"text":"lots of almost arbitrary parameters here,\nis accepting the default good enough, well"},"2286":{"dur":4,"text":"I just did accept the default, if you didn\u2019t\nthink that that was good enough then you\u2019ve"},"2290":{"dur":1,"text":"got your answer."},"2292":{"dur":4,"text":"Confirmation bias is a risk in interpretation,\nit\u2019s easy to ignore top wordforms or whole"},"2297":{"dur":4,"text":"topics that fail to jump out as salient, you\nmay have noticed in my analysis that there"},"2301":{"dur":3,"text":"are words in some of the topics that don\u2019t\nfit my headings, I was just skimming over"},"2305":{"dur":3,"text":"them, again you\u2019ll see a lot of that in\nthe literature."},"2308":{"dur":3,"text":"The huge number of words and weightings to\nreport makes it easy to sweep intractable"},"2311":{"dur":5,"text":"items under the rug, that\u2019s because the\ntopic model is not those top 20 words the"},"2317":{"dur":4,"text":"topic model is a huge number of association\nstatistics between the words and the topics"},"2322":{"dur":4,"text":"where every word has an association strength\nof every topic."},"2326":{"dur":4,"text":"So here is the actual model which I printed\nout in a text file this is not [38:50] you\u2019ve"},"2330":{"dur":5,"text":"got line numbers down the side, so topic zero,\nexperimental approach nuclear, baa baa baa"},"2336":{"dur":5,"text":"baa baa, right the way down and when you get\nto line 14,000 odd we get to jointly which"},"2341":{"dur":3,"text":"is the last term and then we got to topic\n1 and we start the lexicon again."},"2345":{"dur":4,"text":"And then we start the lexicon again and we\nkeep on going ba da da da da, by the time"},"2349":{"dur":6,"text":"you get to 171,000 lines you\u2019ve got to topic\n11 and finally you get to line 293,080 and"},"2356":{"dur":4,"text":"you get jointly with topic 19 which is your\nvery last and that is the actual topic model,"},"2360":{"dur":5,"text":"that\u2019s the topic model those top 20 words\nare not the topic model they are a slice of"},"2366":{"dur":6,"text":"the very top of the topic model, these 300,000\nnumbers are your actual topic model."},"2372":{"dur":3,"text":"Now you can see that most of the numbers are\n0.01 which is the lowest possible score, it"},"2376":{"dur":6,"text":"means one scrabble tile somewhere at the bottom\nof the urn, but that\u2019s all there. For the"},"2382":{"dur":6,"text":"20 topic model that 200 odd the overwhelming\nmajority 274,000 out of 293 and 0.01. But"},"2389":{"dur":6,"text":"do people ever look at that, should they?\nIf not are we ignoring most of the actual"},"2395":{"dur":1,"text":"topic model?"},"2396":{"dur":4,"text":"Now again model I\u2019ve criticised topic but\nwhat about model, and usually model means"},"2401":{"dur":3,"text":"a description of a phenomenon that is simplified\nin some ways to help us understand how that"},"2405":{"dur":4,"text":"phenomenon actually works, so for instance\nif you talk about the planets going in circles"},"2409":{"dur":4,"text":"around the sun that\u2019s a simplification because\nthey\u2019re not really exact circles they\u2019re"},"2414":{"dur":4,"text":"ellipses but by talking about circles going\naround the sun you can start to understand"},"2418":{"dur":4,"text":"things about orbits you can start to understand\nthings about gravity, that\u2019s a scientific"},"2422":{"dur":5,"text":"model, a topic model is not like this, a topic\nmodel is a description of a phenomenon that"},"2428":{"dur":5,"text":"we know works completely differently, right\nwe know that writers don\u2019t pull tiles out"},"2434":{"dur":4,"text":"of bins, they sit there and they go through\narguments or tell stories, so we know that"},"2438":{"dur":6,"text":"the topic model is not what is happening,\nit\u2019s a model that is not aiming to tell"},"2445":{"dur":5,"text":"us what, how to understand the process of\ndiscourse production rather it\u2019s just creating"},"2451":{"dur":4,"text":"computational systems that can simulate the\noutcome, that\u2019s not the same thing."},"2455":{"dur":5,"text":"Now I had to get POS tagging in her somewhere,\nas Stephan mentioned it\u2019s almost impossible"},"2461":{"dur":4,"text":"for me to talk for more than about 30 minutes\nwithout talking about impart speech tagging."},"2465":{"dur":4,"text":"Impart speech tagging problemastic part speech\ntagging we use something called a Markov model"},"2469":{"dur":4,"text":"which is a model in the same sense, the Markov\nmodel of part speech assumption basically"},"2474":{"dur":5,"text":"says well it\u2019s a random guess based on transition\nprobabilities and emission probabilities which"},"2480":{"dur":5,"text":"I haven\u2019t got time to explain but basically\nthe Markov model is assuming that from one"},"2485":{"dur":5,"text":"word to the next is kind of a random transition\nproducing text as we go and it uses a similar"},"2490":{"dur":4,"text":"kind of backwards working assumptions to try\nand work out the underlying parts of speech"},"2495":{"dur":5,"text":"from the words it sees. So again you know\nit\u2019s not a totally unfamiliar to linguistics"},"2500":{"dur":4,"text":"we\u2019ve been using Markov models for years,\nno-one has ever suggested that the human brain"},"2504":{"dur":4,"text":"produces text by use of a Markov model but\nalso we don\u2019t look at the probabilities"},"2509":{"dur":3,"text":"in the Markov model to give us insights into\ngrammar we know it\u2019s just a clever hack"},"2513":{"dur":4,"text":"it\u2019s not actually telling us anything about\ngrammar in the brain. I wish I had like five"},"2518":{"dur":3,"text":"more slides about Markov models to really\nmake that point but I didn\u2019t want to test"},"2522":{"dur":3,"text":"your patience too much."},"2526":{"dur":4,"text":"Replication since I\u2019ve mentioned scientific\nmodels I should mention replicability because"},"2530":{"dur":6,"text":"scientific respectability requires that analysis\nshould be replicable, this is where the randomness"},"2537":{"dur":5,"text":"at its core of the LDA approach can come back\nto bite us, so I thought well I\u2019ve done"},"2542":{"dur":4,"text":"my analysis I\u2019ll do it again, and when I\nsay I did it again, I didn\u2019t change the"},"2547":{"dur":4,"text":"data I didn\u2019t change the settings, I literally\npasted in the command onto the command line"},"2551":{"dur":5,"text":"again and pressed enter, right so bear in\nmind two key strokes produced what your about"},"2556":{"dur":6,"text":"to see, I repeated the topic modelling run\nof Mallet from that same imported corpus and"},"2562":{"dur":6,"text":"I copied the top 20 words into the top two\nanalysis and I found this time that I had"},"2568":{"dur":6,"text":"19 topics that were easily interpretable so\ninfant mortality plus misc others, nations,"},"2574":{"dur":4,"text":"history, classics, marine biology, theoretical\nlinguistic, circuitry, bio chemists, blah,"},"2579":{"dur":4,"text":"blah, blah blah, and one Hodgepodge that I\ncouldn\u2019t interpret thalidomide patients,"},"2584":{"dur":6,"text":"boundary, so medi medicine but also pyrite\ngrain sites, so anyway."},"2590":{"dur":5,"text":"I found in this case the linguistics instead\nof being you know kind of unsatisfactory linguistics"},"2595":{"dur":4,"text":"was dealt with great this time we had one\nthat was clearly about language theory grammar"},"2600":{"dur":5,"text":"and acquisition topic 6, and we had one that\nwas clearly about phonetics, maybe except"},"2606":{"dur":4,"text":"for game and play but otherwise very obviously\nabout phonetics, and linguistics words weren\u2019t"},"2611":{"dur":5,"text":"really popping up elsewhere, and that linguistics\nphonetics was one of the very few topics that"},"2616":{"dur":5,"text":"appeared both when I did it the first time\nand when I did it the second time, economics"},"2621":{"dur":4,"text":"also was more or less consistent, marine biology\nwas consistent and aircraft manufacturing"},"2626":{"dur":4,"text":"was consistent, what else was consistent between\nthe first time I did it and the second time"},"2630":{"dur":5,"text":"I did it? Crickets chirping, I didn\u2019t have\na sound effect because I didn\u2019t know if"},"2635":{"dur":4,"text":"it would work here but I have a picture of\na cricket."},"2640":{"dur":5,"text":"So the amounts of data evidence discourse\nof you know presenting your evidence gone,"},"2646":{"dur":3,"text":"the academic argument one all the words in\nit seem to be dispersed across the other topics"},"2649":{"dur":5,"text":"it was gone, chemistry wasn\u2019t gone but it\nhad transformed because of changes in the"},"2655":{"dur":6,"text":"words inside it, it was clearly now biochemistry\nwhereas before it hadn\u2019t been clearly biochemistry."},"2661":{"dur":5,"text":"So there\u2019s not a lot of consistency here,\nand this is a slide I added I was inspired"},"2666":{"dur":4,"text":"by Susan here she was talking about how the\nother words in a topic alongside a word can"},"2671":{"dur":4,"text":"disambiguate it, well this is one that I spotted\nit was the word \u201cFormula\u201d when I did my"},"2675":{"dur":3,"text":"first analysis there\u2019s where formula was,\npressure, temperature, reaction, resistance,"},"2679":{"dur":7,"text":"paper, continuum, I thought it\u2019s a physics\nformula, then I did it in the second analysis"},"2687":{"dur":4,"text":"formula popped up in this topic, point, position,\ntext, line, female, part, book, reader, definition,"},"2692":{"dur":4,"text":"points, now this is harder but looking at\nthis and we seem to have maths and reading"},"2696":{"dur":5,"text":"mixed together so formula is more like a mathematical\nformula here, now maybe that\u2019s close enough"},"2701":{"dur":5,"text":"for you it wasn\u2019t close enough for me I\u2019m\nafraid, so Gigantopithecus says, even if you\u2019re"},"2707":{"dur":4,"text":"on board with nondeterministic methods that\ncan produce a different result every time"},"2711":{"dur":4,"text":"you might well find that this amount of variability\nmay give you pause, even if you\u2019re happy"},"2716":{"dur":4,"text":"with a little bit of variability this is a\nlot of variability."},"2721":{"dur":3,"text":"So if we decide we don\u2019t like it what else\ncan we do with these massive lists of word"},"2724":{"dur":5,"text":"frequencies, well again at least I can talk,\nso saying something about Susan and Paul\u2019s"},"2730":{"dur":4,"text":"work that doesn\u2019t involve what Susan talked\nabout yesterday, the Murakami et al paper"},"2734":{"dur":5,"text":"is one of a pair of papers, and in the other\nwhich is due out later this year the same"},"2740":{"dur":4,"text":"approach is applied to multidimensional analysis\nusing Bi9ber\u2019s approach. For those of you"},"2744":{"dur":4,"text":"who don\u2019t know what multidimensional analysis\nis you take 150 frequency counts or features"},"2749":{"dur":4,"text":"that have been designed in advance informed\nby linguistic theory that\u2019s crucial using"},"2754":{"dur":5,"text":"POS tags and word sequences sorry POS tags,\nPOS sequences and word lists that mean something"},"2759":{"dur":6,"text":"semantically and then you do a factor analysis\non them to boil down those 150 variables down"},"2765":{"dur":5,"text":"into 5 or 6, and then the factors you interpret\nthem functionally and you say these are the"},"2771":{"dur":3,"text":"dimensions and variation that\u2019s Bi9ber\u2019s\nmodel in a nutshell."},"2774":{"dur":3,"text":"Now you might say well what\u2019s the difference\nthen if you\u2019ve got statistics in there like"},"2777":{"dur":5,"text":"factor analysis, factor analysis, factor analysis\nis deterministic put in the same inputs you"},"2783":{"dur":2,"text":"will always get the same outputs."},"2786":{"dur":6,"text":"The Hunston et al paper which is in IJCL later\nthis year I believe, the idea was that this"},"2792":{"dur":5,"text":"and the Murakami et al were both about the\nsame data but from two prospectives, multidimensional"},"2797":{"dur":5,"text":"analysis on the one hand, deterministic linguistically\ninformed LDA on the other hand much more bottom"},"2803":{"dur":5,"text":"up, but I thought well, is there not something\nin-between these two, you know is it really"},"2809":{"dur":4,"text":"a choice between embracing LDA with all its\nproblems or else doing something that is very,"},"2813":{"dur":6,"text":"very theory heavy. And I thought um, must\nwe go if we don\u2019t want to do Biber style"},"2820":{"dur":5,"text":"multidimensional is the only alternative to\ngo all the way to LDA, can we get at text"},"2825":{"dur":4,"text":"level co-occurrence patterns without those\nissues? And this was something that was again"},"2830":{"dur":3,"text":"really embracing by Susan\u2019s talk yesterday\nI keep going on about it because I was sitting"},"2834":{"dur":3,"text":"there thinking, please Susan don\u2019t say anything\nthat is going to make my entire talk completely"},"2838":{"dur":4,"text":"pointless tomorrow and she didn\u2019t, once\nor twice she came close, but she didn\u2019t"},"2842":{"dur":4,"text":"anyway and I was thinking Susan really emphasised\nthat topic modelling is in a way it\u2019s like"},"2846":{"dur":5,"text":"collocation but we\u2019re looking at not co-occurrences\ndirectly next to one another but co-occurrence"},"2852":{"dur":4,"text":"within a text and I thought well if that\u2019s\nwhat you\u2019re looking at you can still use"},"2856":{"dur":5,"text":"co-occurrence statistics if you want, they\u2019ll\nstill work. The reason for this Stefan is"},"2861":{"dur":6,"text":"that the basic understanding of collocation\nand here\u2019s Stefan\u2019s version of it there"},"2868":{"dur":4,"text":"are other versions of it but I like this one.\nYou have a generalised contingency table for"},"2872":{"dur":5,"text":"collocation and it\u2019s using this notation\nwhere U is the node and V is the collocate,"},"2878":{"dur":5,"text":"and you look at the collorcurrences of U and\nV and not U and not V. Now basically you say"},"2883":{"dur":5,"text":"how many times do these occur nearby how many\ntimes they occur not nearby? The definition"},"2889":{"dur":6,"text":"of what counts as nearby for this statistical\nmodel is entirely independent of the statistics,"},"2895":{"dur":5,"text":"right, in the same text could be defined as\nnearby and that table will still work all"},"2901":{"dur":5,"text":"the collocation statistics must still work\nwe can still use the stats all of the association"},"2906":{"dur":5,"text":"measures for collocation that we use are deterministic\nsame input leads to same output. I haven\u2019t"},"2911":{"dur":5,"text":"done that because I was interested in something\nelse so that\u2019s where I\u2019m going to finish"},"2916":{"dur":1,"text":"I think."},"2917":{"dur":5,"text":"Factor analysis as I\u2019ve said underlies multidimensional\nanalysis, cluster analysis is the one that"},"2922":{"dur":3,"text":"I\u2019ve talked about least, but it\u2019s where\nI\u2019m going to end, we\u2019ve seen it being"},"2926":{"dur":7,"text":"used in Stephan Gries\u2019 papers, the behavioural\nprofiles method uses cluster analysis across"},"2933":{"dur":5,"text":"feature frequencies derived from detailed\nanalysis of a concordance and if you haven\u2019t"},"2939":{"dur":6,"text":"read those papers on behavioural profiles,\ndo, they\u2019re very, very interesting, they"},"2946":{"dur":2,"text":"have been hugely influential for me."},"2948":{"dur":5,"text":"And also I have an App for that, I have a\nlittle thing on the web which if you want"},"2954":{"dur":4,"text":"to do some cluster analysis it\u2019s all put\non the web anyone can use it called the UCREL"},"2959":{"dur":3,"text":"Clustertool where you just paste in the data\nand you get yourself a nice little cluster"},"2963":{"dur":3,"text":"analysis and it\u2019s all terribly jolly."},"2966":{"dur":5,"text":"And I also thought well I want to use some\nlinguistic knowledge I want to proposal in"},"2971":{"dur":5,"text":"a way of doing it I want to use some of those\nlinguistic knowledge tricks to reduce the"},"2976":{"dur":7,"text":"dimensionality, and this is where I reach\nfor the USAS system, is Paul here? Argh there"},"2984":{"dur":5,"text":"you are, give a cheer for USAS, hooray, so\nI took my 80 texts and I tagged them with"},"2989":{"dur":5,"text":"CLAWS power speech and I tried them with USAS\nsemantic categories, I then simplified the"},"2994":{"dur":5,"text":"semantic tag slightly because the USAS semantic\nsystem is very complex so I boiled it down"},"2999":{"dur":5,"text":"to 209 semantic categories based on that semantic\ntagging, I then wrote a little programme the"},"3005":{"dur":5,"text":"entire run of which can be seen here to scan\nthrough the USAS output and generate semantic"},"3010":{"dur":4,"text":"tag frequencies, so I had my 209 frequencies\nfor the frequencies of the semtags in every"},"3015":{"dur":4,"text":"one of the 80 texts, so again we\u2019re dealing\nwith vector frequencies but already we\u2019ve"},"3019":{"dur":3,"text":"reduced the problem by using the semantic\ntags, basically what that\u2019s doing it providing"},"3023":{"dur":4,"text":"a kind of thesaurus to group together words\nthat are semantically similar in advance before"},"3028":{"dur":7,"text":"we start the analysis. That\u2019s what it looks\nlike when it pops out, so here are the FLOB"},"3035":{"dur":9,"text":"texts, J1, 2, 3, 4, 5, down to 19 and here\nare the semantic tags A10, A11, A12, 13, 14,"},"3044":{"dur":3,"text":"15 the A\u2019s are not terribly interesting\nsemantic tags the interesting ones happen"},"3048":{"dur":3,"text":"between like B and S."},"3051":{"dur":3,"text":"So yes this is a spreadsheet and this is just\nthe top left corner, we\u2019ve got a data matrix,"},"3055":{"dur":4,"text":"we can apply any of the statistical methods\nthat can be applied to a data matrix, I could"},"3059":{"dur":3,"text":"have done factor analysis I could have done\ncollocation stats I decided to do cluster"},"3063":{"dur":1,"text":"analysis."},"3064":{"dur":5,"text":"And that\u2019s what popped out of my tool, all\nI did literally was I copied and pasted that"},"3069":{"dur":6,"text":"spreadsheet into the UCREL cluster tool pressed\none button and that was generated it\u2019s basically"},"3075":{"dur":3,"text":"a little web tool that calls R in the background\nto generate the image."},"3079":{"dur":4,"text":"So that\u2019s the J text clustered by semantic\ntags, and instantly we can see that in terms"},"3084":{"dur":4,"text":"of semantics we\u2019ve got one little outlay\nof blob here and then two big categories,"},"3089":{"dur":5,"text":"now it\u2019s hard to read what the texts actually\nare but thankfully the tool also provides"},"3094":{"dur":7,"text":"a textual version of this laid on its side,\nit looks like this, can you see that this"},"3101":{"dur":5,"text":"is the tree this is the top of the tree and\nthe its laid sideways and you can see what"},"3106":{"dur":4,"text":"texts are close together, now I didn\u2019t have\ntime to go through all 80 texts and cross"},"3111":{"dur":5,"text":"reference them to the manual to work out what\nwas going on in terms of the bibliography"},"3117":{"dur":7,"text":"but if you read through the FLOB J manual\nfile, you will find that similar sounding"},"3124":{"dur":5,"text":"kinds of fields tend to be together in the\nsequence, and I find it very interesting that"},"3130":{"dur":7,"text":"a lot of these units are grouping together\ntexts with similar numbers, 52, 53, 56, 60,"},"3138":{"dur":7,"text":"here\u2019s another one low numbers up there,\nnumbers in the 30\u2019s and 60\u2019s here, 22,"},"3145":{"dur":7,"text":"14, and 15 are there, 70\u2019s, 46, 76, so that\u2019s\nlike 70\u2019s there\u2019s clearly something going"},"3152":{"dur":4,"text":"on here that is reflecting the actual kind\nof disciplines that we\u2019re seeing there."},"3157":{"dur":4,"text":"And there\u2019s another one and you can see\nthis is going right the way through this dendagram,"},"3161":{"dur":9,"text":"the dendagram is the entire output in about\n300\/400 lines of text instead of 300,000 parameters."},"3171":{"dur":5,"text":"One in detail just one, again you can see\nthat they\u2019re all quite close together, and"},"3176":{"dur":5,"text":"so I picked some common words from the title\nso this is again this is from Marianna\u2019s"},"3182":{"dur":5,"text":"manual, so you can see that it\u2019s putting\ntogether 10 and 20, hexagons, conics, cloud"},"3187":{"dur":4,"text":"droplet, deposition and velocity so there\u2019s\nsomething going on with the geometry there,"},"3192":{"dur":4,"text":"17 thalidomide treatment for chronic graft\nversus host disease, that explains thalidomide"},"3197":{"dur":5,"text":"it just kept popping up in weird places in\nthe topic novel, and that\u2019s then associated"},"3203":{"dur":7,"text":"with 9 and 12, you can see there which is\nproteolysis of factor chain polypeptides and"},"3210":{"dur":5,"text":"definition of surface exposed epitopes, this\nis kind of like molecular chemistry of various"},"3215":{"dur":3,"text":"sorts, molecular bio-chemistry, we can see\nwhat\u2019s going on there in terms of these"},"3218":{"dur":5,"text":"semantic tags, and the distances between texts\ndefined by that semantic tag matrix."},"3224":{"dur":5,"text":"Now if I\u2019d had more time I said this was\ntowards I would have done things like switch"},"3229":{"dur":4,"text":"it round and cluster the semantic tags rather\nthan clustering the text to see which semantic"},"3234":{"dur":3,"text":"groupings are related to one another, you\ncan do the cluster analysis both ways, you"},"3238":{"dur":4,"text":"can cluster the features as well as the objects.\nI would have done the same with factor analysis"},"3242":{"dur":5,"text":"to see which semantic tags grouped together\nin factors Biber style and I would in fact"},"3247":{"dur":3,"text":"probably then have tried to do, try being\nthe operative word, because I\u2019ve never done"},"3251":{"dur":4,"text":"it before but I\u2019ve read Biber, I would have\ntried to do a full multidimensional analysis"},"3255":{"dur":3,"text":"there and I\u2019m sure that everyone in this\nroom has already got binging in their mind"},"3259":{"dur":3,"text":"other things that you could do along these\nlines."},"3262":{"dur":4,"text":"So how does this sort of thing stack up against\nLDA this is where I really start being in"},"3267":{"dur":6,"text":"the realm of opinion, well when we start comparing\nthe results that we get out, we see some similarities,"},"3274":{"dur":7,"text":"we see major divisions cutting across disciplines,\nI found in one of the branches of the dendogram"},"3281":{"dur":6,"text":"I found haematology and mathematics together\nnot quite sure why, and a similarity between,"},"3288":{"dur":5,"text":"that I found between doing this topic modelling\nanalysis and this clustering analysis on the"},"3293":{"dur":4,"text":"other hand is that if you want to actually\nget decent interpretation you need to be prepared"},"3297":{"dur":4,"text":"to go digging in the data of Murakami et al\nthere is no way away from that, and there"},"3302":{"dur":4,"text":"are arbitrary parameters, sorry about the\ntypos I forgot to proof read this slide, there"},"3306":{"dur":5,"text":"are arbitrary parameters, the cut is how deep\nin the dendrogram we decide to go and that\u2019s"},"3312":{"dur":4,"text":"arbitrary there\u2019s no mathematics can tell\nyou how deep in the tree you need to go, similarly"},"3316":{"dur":2,"text":"there\u2019s no mathematics can tell you the\nnumber of topics."},"3319":{"dur":5,"text":"So there are similar issues in both these\nkinds of ways of reducing this massive vector"},"3324":{"dur":4,"text":"this massive matrix but the differences are\nthat this cluster analysis I think it\u2019s"},"3329":{"dur":4,"text":"far more transparent, the whole thing fits\non one screen, the text dendrogram few hundred"},"3333":{"dur":5,"text":"lines it\u2019s not 20 things called from the\ntop of this massive set of parameters. It\u2019s"},"3338":{"dur":5,"text":"more reproducible because it\u2019s deterministic\nand we\u2019re directly interpreting text groups"},"3344":{"dur":2,"text":"rather than interpreting topics."},"3346":{"dur":5,"text":"So Gigantopithecus says obviously this is\njust start, more could be done, and more will"},"3351":{"dur":4,"text":"be done, some final thoughts, and this is\nwhere I wax philosophical."},"3356":{"dur":4,"text":"We\u2019ve always been saying to other linguists\ncorpus linguistics is not just number crunching"},"3360":{"dur":4,"text":"it\u2019s essentially qualitative as well as\nquantitative and I said quotes from some textbooks"},"3365":{"dur":3,"text":"including Hunston would be good here if possible\nbecause I wanted to give credit to Susan but"},"3368":{"dur":4,"text":"unfortunately I forgot that I wouldn\u2019t have\naccess to my University Library while I was"},"3373":{"dur":2,"text":"her in Birmingham, so sorry."},"3375":{"dur":3,"text":"And yet here is the field embracing topic\nmodelling which is really blind black box"},"3378":{"dur":4,"text":"number crunching, so did we perhaps not mean\nthat first thing that CL is not just number"},"3382":{"dur":4,"text":"crunching, were we kind of like just covering\nourselves all the time, if we\u2019re serious"},"3387":{"dur":2,"text":"about this we need to be especially on our\nguard with black box methods and statistical"},"3390":{"dur":4,"text":"machine learning without any linguistic knowledge\nin the loop which is the methods like topic"},"3394":{"dur":2,"text":"modelling and LDA."},"3396":{"dur":3,"text":"Data mining research is I think like machine\nlearning methods because they don\u2019t require"},"3400":{"dur":4,"text":"knowledge of the domain. You can have a standard\ntoolkit, if you\u2019re a data mining person"},"3404":{"dur":4,"text":"and you\u2019ve got your toolkit of algorithms\nyou can use it anywhere on any domain but"},"3408":{"dur":4,"text":"we\u2019re linguists we have domain knowledge\nwe probably shouldn\u2019t pretend that we don\u2019t"},"3413":{"dur":5,"text":"and even without that knowledge we already\nhave plenty of tools in our bag for dealing"},"3418":{"dur":2,"text":"with this sort of thing."},"3420":{"dur":4,"text":"Gigantopithecus says, as far as he\u2019s concerned\nthe concordance should still be our weapon"},"3425":{"dur":5,"text":"of first and last resort the place where quantification\nand interpretation meet."},"3431":{"dur":4,"text":"So not a conclusion because there\u2019s more\nto do, obviously more can be done but I\u2019m"},"3435":{"dur":5,"text":"not a fan of LDA and hopefully you now understand\nwhy. Too many things can go wrong in the interpretation"},"3440":{"dur":3,"text":"and we have plenty of exploratory methods\nin our toolkit already."},"3443":{"dur":3,"text":"Most importantly, remember what your Gigantopithecus\ntaught you."}}