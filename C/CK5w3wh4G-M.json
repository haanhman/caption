{"0":{"dur":7,"text":"Translator: Eugenia Barth\nReviewer: Denise RQ"},"22":{"dur":2,"text":"I'm going to talk about my research"},"25":{"dur":2,"text":"on the long term future \nof artificial intelligence."},"28":{"dur":1,"text":"In particular, I want to tell you"},"30":{"dur":3,"text":"about a very important phenomenon \ncalled \"Intelligence Explosion.\""},"35":{"dur":2,"text":"There are two reasons \nthat I work on intelligence explosion"},"37":{"dur":1,"text":"and that I think it's worth sharing."},"39":{"dur":3,"text":"The first is that it's a phenomenon \nof immense theoretical interest"},"43":{"dur":3,"text":"for those who want to understand \nintelligence on a fundamental level."},"46":{"dur":2,"text":"The second reason is practical."},"49":{"dur":3,"text":"It has to do with effects \nthat intelligence explosion could have."},"52":{"dur":1,"text":"Depending on the conditions"},"54":{"dur":2,"text":"under which an intelligence \nexplosion could arise"},"57":{"dur":2,"text":"and on the dynamics that it exhibits"},"59":{"dur":3,"text":"it could mean that AI changes very rapidly"},"62":{"dur":3,"text":"from a safe technology,\nrelatively easy to handle,"},"66":{"dur":3,"text":"to a volatile technology \nthat is difficult to handle safely."},"69":{"dur":2,"text":"In order to navigate this hazard,"},"71":{"dur":2,"text":"we need to understand \nintelligence explosion."},"75":{"dur":3,"text":"Intelligence explosion \nis a theoretical phenomenon."},"79":{"dur":1,"text":"In that sense, it's a bit"},"80":{"dur":2,"text":"like a hypothetical particle\nin particle physics."},"83":{"dur":2,"text":"There are arguments \nthat explain why it should exist,"},"86":{"dur":3,"text":"but we have not been able \nto experimentally comfirm it yet."},"90":{"dur":2,"text":"Nevertheless, the thought experiment"},"92":{"dur":3,"text":"that explains what intelligence \nexplosion would look like"},"96":{"dur":1,"text":"is relatively simple."},"97":{"dur":1,"text":"And it goes like this."},"99":{"dur":1,"text":"Suppose we had a machine"},"101":{"dur":2,"text":"that was much more capable \nthan today's computers."},"103":{"dur":2,"text":"This machine, given a task,"},"106":{"dur":2,"text":"could form hypotheses from observations,"},"109":{"dur":3,"text":"use those hypotheses \nto make plans, execute the plans,"},"112":{"dur":3,"text":"and observe the outcomes \nrelative to the task,"},"116":{"dur":3,"text":"and do it all efficiently \nwithin a reasonable amount of time."},"119":{"dur":3,"text":"This kind of machine could be given\nscience and engineering tasks"},"123":{"dur":2,"text":"to do on its own, autonomously."},"125":{"dur":2,"text":"And this is the key step \nin the thought experiment:"},"128":{"dur":3,"text":"this machine could even be tasked\nwith performing AI research,"},"131":{"dur":1,"text":"designing faster and better machines."},"134":{"dur":3,"text":"Let's say our machine goes to work,\nand after a while,"},"138":{"dur":2,"text":"produces blueprints\nfor a second generation of AI,"},"141":{"dur":4,"text":"that's more efficient, more capable,\nand more general than the first."},"145":{"dur":2,"text":"The second generation \ncan be tasked once again"},"147":{"dur":1,"text":"with designing improved machines,"},"149":{"dur":3,"text":"leading to a third generation, \na fourth, a fifth, and so on."},"154":{"dur":1,"text":"An outside observer would see"},"156":{"dur":4,"text":"a very large and very rapid increase \nin the abilities of these machines,"},"160":{"dur":2,"text":"and it's this large and rapid increase"},"162":{"dur":2,"text":"that we call Intelligence Explosion."},"165":{"dur":2,"text":"If it's the case"},"168":{"dur":4,"text":"that in order to undergo \nan intelligence explosion"},"172":{"dur":3,"text":"many new pieces of hardware \nneed to be build,"},"175":{"dur":2,"text":"or new manufacturing technologies,"},"177":{"dur":2,"text":"then an explosion will be more slow"},"180":{"dur":2,"text":"- although still quite fast \nby historical standards."},"183":{"dur":2,"text":"However, looking at the history\nalgorithmic improvement"},"186":{"dur":2,"text":"it turns out \nthat just as much improvement"},"189":{"dur":3,"text":"tends to come from new software\nas from new hardware."},"193":{"dur":3,"text":"This is true in areas \nlike physics simulation, game playing,"},"196":{"dur":3,"text":"image recognition, and many parts\nof machine learning."},"200":{"dur":3,"text":"What this means is that our outside\nobserver may not see physical changes"},"203":{"dur":3,"text":"in the machines that are undergoing\nan intelligence explosion."},"207":{"dur":2,"text":"They may just see a series of programs"},"209":{"dur":2,"text":"writing successively \nmore capable programs."},"212":{"dur":4,"text":"It stands to reason that this process\ncould give rise to programs"},"216":{"dur":4,"text":"that are much more capable at any number \nof intellectual tasks than any human is."},"221":{"dur":3,"text":"Just as we now build machines that are \nmuch stronger, faster, and more precise"},"225":{"dur":2,"text":"at all kinds of physical tasks,"},"227":{"dur":2,"text":"it's certainly possible to build machines"},"229":{"dur":2,"text":"that are more efficient\nat intellectual tasks."},"232":{"dur":4,"text":"The human brain is not at the upper end\nof computational efficiency."},"237":{"dur":1,"text":"And it goes further than this."},"238":{"dur":1,"text":"There is no particular reason"},"240":{"dur":4,"text":"to define our scale by the abilities \nof a single human or a single brain."},"247":{"dur":2,"text":"The largest thermonuclear bombs \nrelease more energy"},"250":{"dur":1,"text":"in less than a second"},"251":{"dur":3,"text":"than the human population \nof Earth does in a day."},"255":{"dur":2,"text":"It's not ouf of the question to think"},"257":{"dur":3,"text":"that machines designed \nto perform intellectual tasks"},"260":{"dur":2,"text":"and then honed over \nmany generations of improvement"},"263":{"dur":1,"text":"could similarly outperfom"},"265":{"dur":2,"text":"the productive thinking \nof the human race."},"270":{"dur":3,"text":"This is the theoretical phenomenon \ncalled Intelligence Explosion."},"274":{"dur":2,"text":"We don't have a good theory\nof intelligence explosion yet,"},"277":{"dur":3,"text":"but there is reason to think\nthat it could happen at software speed"},"280":{"dur":2,"text":"and could reach a level of capability"},"283":{"dur":2,"text":"that's far greater \nthan any human or group of humans"},"285":{"dur":2,"text":"at any number of intellectual tasks."},"288":{"dur":2,"text":"The first time \nI encountered this argument,"},"290":{"dur":2,"text":"I more or less ignored it."},"293":{"dur":4,"text":"Looking back it seems crazy for me, \nsomeone who takes AI seriously,"},"298":{"dur":2,"text":"to walk away from intelligence explosion."},"300":{"dur":2,"text":"And I'll give you two reasons for that."},"303":{"dur":2,"text":"The first reason is a theorist's reason."},"306":{"dur":4,"text":"A theorist should be interested in \nthe large-scale features of their field"},"310":{"dur":3,"text":"in the contors of their phenomena \nof choice as determined by"},"313":{"dur":4,"text":"the fundamental forces, or interactions, \nor building blocks of their subject."},"319":{"dur":3,"text":"As someone who aspires to be\na good theorist of intelligence,"},"322":{"dur":3,"text":"I can, in good faith, \nignore intelligence explosion"},"325":{"dur":1,"text":"as a major feature"},"327":{"dur":3,"text":"of many simple \nstraightforward theories of intelligence."},"330":{"dur":2,"text":"What intelligence explosion means"},"333":{"dur":3,"text":"is that intelligence improvement \nis not uniform."},"337":{"dur":4,"text":"There is a threshold below \nwhich improvements tend to peter out,"},"341":{"dur":1,"text":"but above that threshold,"},"343":{"dur":3,"text":"intelligence grows like compound interest\nincreasing more and more."},"347":{"dur":2,"text":"This threshold would have to emerge from"},"349":{"dur":2,"text":"any successful theory of intelligence."},"351":{"dur":3,"text":"The way phase transitions \nemerge from thermodynamics,"},"354":{"dur":3,"text":"intelligence would effectively \nhave a boiling point."},"358":{"dur":1,"text":"Seeing this way,"},"360":{"dur":3,"text":"exploring intelligence explosion \nis exactly the kind of thing"},"363":{"dur":2,"text":"a theorist wants to do, \nespecially in a field like AI,"},"366":{"dur":2,"text":"where we are trying to move \nfrom our current state"},"369":{"dur":3,"text":"- partial theories, pseudotheories, \narguments, and thought experiments -"},"372":{"dur":4,"text":"toward a fully-fledged \npredictive theory of intelligence."},"381":{"dur":2,"text":"This is the intelligence explosion."},"385":{"dur":2,"text":"In its most basic form,"},"388":{"dur":2,"text":"it relies on a simple premise"},"390":{"dur":3,"text":"that AI research is not so different \nfrom other intellectual tasks"},"393":{"dur":2,"text":"but can be performed by machines."},"396":{"dur":2,"text":"We don't have a good understanding yet,"},"398":{"dur":3,"text":"but there's reason to think \nthat it can happen at software speed"},"401":{"dur":1,"text":"and reach levels of capability"},"403":{"dur":2,"text":"far exceeding any human\nor group of humans."},"405":{"dur":3,"text":"The second reason which I alluded\nto at the start of the talk"},"409":{"dur":4,"text":"is that intelligence explosion \ncould change AI very suddenly"},"413":{"dur":3,"text":"from being a benign technology\nto being a volatile technology"},"416":{"dur":2,"text":"that requires \nsiginificant thought into safety"},"419":{"dur":2,"text":"before use or even development."},"421":{"dur":3,"text":"Today's AI, by contrast, is not volatile."},"425":{"dur":2,"text":"I don't mean that AI systems\ncan't cause harm."},"428":{"dur":4,"text":"Weaponization of AI is ongoing, \nand accidental harms can arise"},"432":{"dur":4,"text":"from unanticipated systemic effecs \nor from faulty assumptions."},"437":{"dur":3,"text":"But on the whole, these sorts\nof harms should be manageable."},"441":{"dur":3,"text":"Today's AI is not so different\nfrom today's other technologies."},"445":{"dur":4,"text":"Intelligence explosion however \nhighlights an important fact:"},"449":{"dur":4,"text":"AI will become more general,\nmore capable, and more efficient"},"453":{"dur":1,"text":"perhaps very quickly"},"454":{"dur":3,"text":"and could become more so \nthan any human or group of humans."},"458":{"dur":1,"text":"This kind of AI will require"},"459":{"dur":2,"text":"a radically different approach \nto be used safely."},"462":{"dur":4,"text":"And small incidents could plausibly\nescalate to cause large amounts of harm."},"467":{"dur":2,"text":"To understand how AI could be hazardous,"},"470":{"dur":3,"text":"let's consider an analogy\nto microorganisms."},"473":{"dur":1,"text":"There are two traits"},"475":{"dur":5,"text":"that make microorganisms more difficult\nto handle safely than a simple toxin."},"480":{"dur":2,"text":"Microorganisms are goal-oriented,"},"483":{"dur":3,"text":"and they are, what I'm \ngoing to call, chain reactive."},"486":{"dur":1,"text":"Goal-oriented means"},"487":{"dur":1,"text":"that on microorganisms behaviors"},"489":{"dur":2,"text":"tend to push towards some certain result."},"491":{"dur":2,"text":"In their case that's \nmore copies of themselves."},"494":{"dur":1,"text":"Chain reactive means"},"495":{"dur":4,"text":"that we don't expect\na group of microorganisms to stay put."},"500":{"dur":2,"text":"We expect their zone \nof influence to grow,"},"502":{"dur":2,"text":"and we expect their population to spread."},"505":{"dur":3,"text":"Hazards can arise, \nbecause on microorganisms"},"508":{"dur":4,"text":"values don't often align \nwith human goals and values."},"513":{"dur":1,"text":"I don't have particular use"},"514":{"dur":3,"text":"for an infinite number \nof clones of this guy."},"518":{"dur":3,"text":"Chain reactivity \ncan make this problem worse."},"522":{"dur":4,"text":"Since, small releases \nof a microorganism can balloon"},"526":{"dur":3,"text":"into large population spending pandemics."},"530":{"dur":4,"text":"Very advanced AI, such as could arise\nfrom intelligence explosion,"},"534":{"dur":3,"text":"could be quite similar \nin some ways to a microorganism."},"538":{"dur":2,"text":"Most AI systems are task-oriented."},"540":{"dur":3,"text":"They are designed \nby humans to complete a task."},"544":{"dur":2,"text":"Capable AIs will use \nmany different kinds of actions"},"547":{"dur":3,"text":"and many types of plans\nto accomplish their tasks."},"550":{"dur":2,"text":"And flexible AIs will be\nable to learn to thrive,"},"553":{"dur":2,"text":"that is to make accurate predictions\nand effective plans"},"556":{"dur":2,"text":"in a wide variaty of environments."},"558":{"dur":3,"text":"Since AIs will act to accomplish\ntheir tasks as well as possible,"},"562":{"dur":1,"text":"they will also be chain reactive."},"563":{"dur":3,"text":"They'll have use for more resources,\nthey'll want to improve themselves,"},"567":{"dur":4,"text":"to spread to other computer systems,\nto make backup copies of themselves"},"571":{"dur":3,"text":"in order to make sure\nthat their task gets done."},"574":{"dur":4,"text":"Because of their task orientation \nand chain reactivity,"},"578":{"dur":3,"text":"sharing an environment \nwith this kind of AI would be hazardous."},"582":{"dur":2,"text":"They may use some of the things \nwe care about,"},"584":{"dur":4,"text":"our raw materials, and our stuff\nto accomplish their ends."},"588":{"dur":3,"text":"And there is no task \nthat has yet been devised"},"591":{"dur":5,"text":"that is compatible with human safety\nunder these circumstances."},"600":{"dur":3,"text":"This hazard has made worse\nby intelligence explosion."},"603":{"dur":3,"text":"In which very volatile AI \ncould arise quickly from benign AI."},"607":{"dur":2,"text":"Instead of a gradual learning period,"},"609":{"dur":3,"text":"in which we come to terms\nwith the power of very efficient AI,"},"613":{"dur":1,"text":"we could be thrust suddenly into a world"},"615":{"dur":2,"text":"where AI is much more powerful \nthan it is today."},"619":{"dur":2,"text":"This scenario is not inevitable,"},"621":{"dur":1,"text":"it's mostly dependent upon"},"623":{"dur":2,"text":"some research group,\nor company, or government"},"626":{"dur":2,"text":"walking into\nintelligence explosion blindly."},"628":{"dur":2,"text":"If we can understand\nintelligence explosion,"},"631":{"dur":3,"text":"and if we have sufficient will\nand self-control as a society,"},"634":{"dur":2,"text":"then we should be able\nto avoid an AI outbreak."},"637":{"dur":3,"text":"There is still the problem\nof chain reactivity though."},"641":{"dur":3,"text":"It would only take one group\nto release AI into the world"},"645":{"dur":2,"text":"even if nearly all groups are careful."},"647":{"dur":3,"text":"One group walking into intelligence\nexplosion accidently or on purpose"},"651":{"dur":2,"text":"without taking proper precautions,"},"654":{"dur":2,"text":"could release an AI that will self-improve"},"656":{"dur":2,"text":"and cause immense amounts\nof harm to everyone else."},"660":{"dur":1,"text":"I'd like to close with four questions."},"662":{"dur":2,"text":"These are questions\nthat I'd like to see answered"},"664":{"dur":3,"text":"because they'll tell us more about\nthe theory of artificial intelligence"},"668":{"dur":3,"text":"and that theory is what will lead us\nunderstand intelligence explosion"},"671":{"dur":3,"text":"well enough to mitigate\nthe risks that it poses."},"675":{"dur":2,"text":"Some of these questions \nare being actively pursued"},"677":{"dur":2,"text":"by researchers at my home institution,"},"680":{"dur":2,"text":"The Future of Humanity Institute \nat Oxford,"},"683":{"dur":3,"text":"and by others like The Machine \nIntelligence Research Institute."},"686":{"dur":1,"text":"My first question is:"},"688":{"dur":3,"text":"can we get a precise predictive theory \nof intelligence explosion?"},"692":{"dur":3,"text":"What happens when AI starts \nto do AI research?"},"695":{"dur":1,"text":"In particular, I'd like to know"},"697":{"dur":3,"text":"how fast software can improve \nits intellectual capabilities."},"701":{"dur":4,"text":"Many of the most volatile scenarios \nwe've examined include"},"705":{"dur":2,"text":"a rapid self-contained take off,"},"708":{"dur":4,"text":"such as could only happen \nunder a software improvement circumstance."},"712":{"dur":3,"text":"If there is some key resource\nthat limits software improvement"},"716":{"dur":3,"text":"or if it's the case \nthat such improvement isn't possible"},"719":{"dur":2,"text":"below a certain threshold of capability,"},"721":{"dur":3,"text":"these would be very useful facts\nfrom a safety standpoint."},"726":{"dur":1,"text":"Question two:"},"728":{"dur":2,"text":"what are our options, \npolitical or technological,"},"730":{"dur":2,"text":"for dealing with the potential harms"},"732":{"dur":3,"text":"from super efficient \nartificial intelligences?"},"735":{"dur":3,"text":"One option, of course, is\nto not build them in the first place."},"739":{"dur":3,"text":"But this would require \nexceedingly good cooperation"},"743":{"dur":4,"text":"between many governments, commercial\nentities, and even research groups."},"748":{"dur":4,"text":"That cooperation and that level \nof understanding isn't easy to come by."},"752":{"dur":3,"text":"It would also depend, to some extent,\non an answer to question one"},"756":{"dur":3,"text":"so that we know how to prevent\nintelligence explosion."},"760":{"dur":1,"text":"Another option would be to make sure"},"762":{"dur":2,"text":"that everyone knows \nhow to devise safe tasks."},"764":{"dur":2,"text":"It's intuitively plausible \nthat there are some kinds of tasks"},"767":{"dur":2,"text":"that can be assigned \nby a safety conscious team"},"770":{"dur":2,"text":"without posing too much risk."},"772":{"dur":2,"text":"It's another question entirely"},"774":{"dur":3,"text":"how these kinds of safety standards \ncould be applied"},"777":{"dur":2,"text":"uniformly and reliably enough \nall over the world"},"780":{"dur":2,"text":"to prevent serious harm."},"782":{"dur":4,"text":"This leads into question three:\nvery capable AIs,"},"787":{"dur":2,"text":"if they can be programmed correctly,"},"789":{"dur":1,"text":"should be able to determine"},"790":{"dur":1,"text":"what is valuable"},"792":{"dur":3,"text":"by modeling human preferences\nand philosophical arguments."},"795":{"dur":3,"text":"Is it possible to assign a task\nof learning what is valuable"},"799":{"dur":2,"text":"and then acting to pursue that aim?"},"802":{"dur":3,"text":"This turns out to be \na highly technical problem."},"805":{"dur":2,"text":"Some of the ground work \nhas been laid by researchers"},"808":{"dur":2,"text":"like Eliezer Yudkowsky, Nick Bostrom,"},"811":{"dur":1,"text":"Paul Christiano and myself"},"812":{"dur":3,"text":"but we still have a long way to go."},"815":{"dur":6,"text":"My final question, as a machine\nself-improves it may make mistakes."},"821":{"dur":3,"text":"Even if the first AI is programed\nto pursue valuable ends,"},"825":{"dur":1,"text":"later ones may not be."},"827":{"dur":3,"text":"Designing a stable and reliable\nself-improvement process"},"830":{"dur":2,"text":"turns out to involve some open problems"},"832":{"dur":2,"text":"in logic and in decision theory."},"834":{"dur":2,"text":"These problems are being actively\npursued at research workshops"},"837":{"dur":4,"text":"held by The Machine\nIntelligence Research Institute."},"841":{"dur":1,"text":"Those are my four questions."},"843":{"dur":2,"text":"I've only been able to cover\nthe basics in this talk."},"846":{"dur":1,"text":"If you'd like to know more"},"847":{"dur":3,"text":"about the long-term future of AI \nand about the intelligence explosion,"},"851":{"dur":2,"text":"I can recommend \nDavid Chalmers' excellent paper,"},"853":{"dur":2,"text":"\"The Singularity\nof Philosophical Analysis,\""},"856":{"dur":3,"text":"as well as a book\nforthcoming in 2014 called,"},"859":{"dur":2,"text":"\"Super Intelligence\" by Nick Bostrom."},"861":{"dur":3,"text":"And of course there are links \nand references on my website."},"866":{"dur":2,"text":"I believe that managing"},"868":{"dur":2,"text":"and understanding intelligence explosion"},"871":{"dur":1,"text":"will be a critical concern"},"873":{"dur":4,"text":"not just for the theory of AI\nbut for safe use of AI"},"877":{"dur":2,"text":"and possibly, for humanity as a whole."},"880":{"dur":1,"text":"Thank you."},"881":{"dur":1,"text":"(Applause)"}}