{"0":{"dur":7,"text":"[SOUND] Stanford University."},"7":{"dur":3,"text":">> We'll get back started\nagain with CS224N,"},"10":{"dur":3,"text":"Natural Language Processing\nwith Deep Learning."},"14":{"dur":5,"text":"So, you're in for a respite,\nor a change of pace today."},"20":{"dur":5,"text":"So for today's lecture, what we're\nprincipally going to look at is syntax,"},"26":{"dur":2,"text":"grammar and dependency parsing."},"28":{"dur":4,"text":"So my hope today is to teach\nyou in one lecture enough about"},"33":{"dur":4,"text":"dependency grammars and\nparsing that you'll all be able to do"},"38":{"dur":4,"text":"the main part of\nAssignment 2 successfully."},"42":{"dur":4,"text":"So quite a bit of the early part of\nthe lecture is giving a bit of background"},"46":{"dur":2,"text":"about syntax and dependency grammar."},"49":{"dur":4,"text":"And then it's time to talk about\na particular kind of dependency grammar,"},"53":{"dur":5,"text":"transition-based, also dependency parsing,\ntransition-based dependency parsing."},"58":{"dur":4,"text":"And then it's probably only in\nthe last kind of 15 minutes or so"},"63":{"dur":6,"text":"of the lecture that we'll then get back\ninto specifically neural network content."},"69":{"dur":4,"text":"Talking about a dependency parser that\nDanqi and I wrote a couple of years ago."},"74":{"dur":2,"text":"Okay, so for general reminders,"},"76":{"dur":4,"text":"I hope you're all really aware\nthat Assignment 1 is due today."},"81":{"dur":4,"text":"And I guess by this stage you've either\nmade good progress or you haven't."},"86":{"dur":5,"text":"But to give my,\nGood housekeeping reminders,"},"92":{"dur":4,"text":"I mean it seems like every year there\nare people that sort of blow lots of"},"96":{"dur":3,"text":"late days on the first assignment for\nno really good reason."},"99":{"dur":3,"text":"And that isn't such\na clever strategy [LAUGH]."},"103":{"dur":4,"text":"So hopefully [LAUGH] you are well\nalong with the assignment, and"},"107":{"dur":3,"text":"can aim to hand it in before\nit gets to the weekend."},"112":{"dur":6,"text":"Okay, then secondly today is also the day\nthat the new assignment comes out."},"119":{"dur":3,"text":"So maybe you won't look at it\ntill the start of next week but"},"122":{"dur":2,"text":"we've got it up ready to go."},"124":{"dur":5,"text":"And so that'll involve a couple of new\nthings and in some respects probably for"},"130":{"dur":5,"text":"much of it you might not want to start\nit until after next Tuesday's lecture."},"135":{"dur":3,"text":"So two big things will be different for\nthat assignment."},"139":{"dur":5,"text":"Big thing number one is we're gonna do\nassignment number two using TensorFlow."},"145":{"dur":4,"text":"And that's the reason why, quite apart\nfrom exhaustion from assignment one,"},"149":{"dur":4,"text":"why you probably you don't wanna start\nit on the weekend is because on Tuesday,"},"153":{"dur":3,"text":"Tuesday's lecture's gonna be\nan introduction to TensorFlow."},"156":{"dur":3,"text":"So you'll really be more qualified\nthen to start it after that."},"160":{"dur":5,"text":"And then the other big different thing\nin assignment two is we get into"},"165":{"dur":5,"text":"some sort of more substantive\nnatural language processing content."},"170":{"dur":5,"text":"In particular, you guys are going to build\nneural dependency parsers, and the hope"},"175":{"dur":4,"text":"is that you can learn about everything\nthat you need to know to do that today."},"180":{"dur":2,"text":"Or perhaps looking at some of\nthe readings on the website,"},"183":{"dur":3,"text":"if you don't get quite\neverything straight from me."},"186":{"dur":2,"text":"Couple more comments on things."},"188":{"dur":4,"text":"Okay, so for final projects."},"193":{"dur":4,"text":"We're going to sort of post,\nhopefully tomorrow or on the weekend,"},"197":{"dur":4,"text":"a kind of an outline of what's in\nassignment four, so you can have sort of"},"201":{"dur":4,"text":"a more informed meaningful choice between\nwhether you want to do assignment four, or"},"206":{"dur":1,"text":"come up with a final project."},"207":{"dur":3,"text":"The area of assignment four, if you do it,"},"211":{"dur":4,"text":"is going to be question answering\nover the SQuAD dataset."},"215":{"dur":3,"text":"But we've got kind of a page and a half\ndescription to explain what that means, so"},"219":{"dur":1,"text":"you can look out for that."},"220":{"dur":3,"text":"But if you are interested in\ndoing a final project, again,"},"224":{"dur":5,"text":"we'll encourage people to come and meet\nwith one of the final project mentors or"},"230":{"dur":4,"text":"find some other well qualified person\naround here to be a final project mentor."},"235":{"dur":4,"text":"So what we're wanting is that sort of,\neverybody has met with"},"240":{"dur":3,"text":"their final project mentor\nbefore putting in an abstract."},"243":{"dur":2,"text":"And that means it'd be really great for"},"246":{"dur":3,"text":"people to get started doing\nthat as soon as possible."},"249":{"dur":3,"text":"I know some of you have already\ntalked to various of us."},"252":{"dur":5,"text":"For me personally, I've got final\nproject office hours tomorrow"},"258":{"dur":4,"text":"from 1 to 3 pm so\nI hope some people will come by for those."},"262":{"dur":2,"text":"And again, sort of as Richard mentioned,"},"265":{"dur":4,"text":"not everybody can possible have Richard or\nme as the final project mentor."},"270":{"dur":4,"text":"And besides, there's some really big\nadvantages of having some of the PhD"},"274":{"dur":2,"text":"student TAs as final project mentors."},"276":{"dur":4,"text":"Cuz really, for things like spending\ntime hacking on TensorFlow,"},"281":{"dur":2,"text":"they get to do it much more than I do."},"283":{"dur":2,"text":"And so, Danqi, Kevin, Ignacio,"},"285":{"dur":5,"text":"Arun that they've had tons of experience\ndoing NLP research using deep learning."},"291":{"dur":3,"text":"And so that they'd also be great mentors,\nand look them up for"},"295":{"dur":2,"text":"their final project advice."},"298":{"dur":5,"text":"The final thing I just want to touch\non is we clearly had a lot of problems,"},"303":{"dur":4,"text":"I realize, at keeping up and\ncoping with people in office hours,"},"308":{"dur":4,"text":"and queue status has just\nregularly got out of control."},"313":{"dur":3,"text":"I'm sorry that that's\nbeen kind of difficult."},"316":{"dur":5,"text":"I mean honestly we are trying to work and\nwork out ways that we can do this better,"},"321":{"dur":4,"text":"and we're thinking of sort of unveiling\na few changes for doing things for"},"326":{"dur":1,"text":"the second assignment."},"328":{"dur":4,"text":"If any of you peoples have any better\nadvice as to how things could be"},"333":{"dur":4,"text":"organized so that they could work\nbetter feel free to send a message"},"338":{"dur":3,"text":"on Piazza with suggestions\nof ways of doing it."},"341":{"dur":4,"text":"I guess yesterday I ran down\nPercy Liang and said, Percy,"},"346":{"dur":3,"text":"Percy, how do you do it for CS221?"},"349":{"dur":2,"text":"Do you have some big\nsecrets to do this better?"},"352":{"dur":4,"text":"But unfortunately I seem to come away\nwith no big secrets cuz he sort of said:"},"357":{"dur":5,"text":"\"we use queue status and we use the Huang\nbasement\", what else are you meant to do?"},"362":{"dur":3,"text":"So I'm still looking for\nthat divine insight [LAUGH] that"},"365":{"dur":4,"text":"will tell me how to get this\nproblem better under control."},"369":{"dur":2,"text":"So if you've got any good ideas,\nfeel free to share."},"372":{"dur":4,"text":"But we'll try to get\nthis as much better under"},"377":{"dur":4,"text":"control as we can for the following weeks."},"382":{"dur":3,"text":"Okay, any questions, or\nshould I just go into the meat of things?"},"389":{"dur":0,"text":"Okay."},"392":{"dur":5,"text":"All right, so what we're going\nto want to do today is work"},"397":{"dur":5,"text":"out how to put structures over\nsentences in some human language."},"403":{"dur":4,"text":"All the examples I'm going to show is for\nEnglish, but in principle,"},"407":{"dur":5,"text":"the same techniques you can apply for\nany language, where these structures"},"413":{"dur":5,"text":"are going to sort of reveal\nhow the sentence is made up."},"418":{"dur":6,"text":"So that the idea is that sentences and\nparts of sentences have some kind"},"424":{"dur":4,"text":"of structure and there are sort of regular\nways that people put sentences together."},"429":{"dur":5,"text":"So, we can sort of start off with very\nsimple things that aren't yet sentences"},"434":{"dur":5,"text":"like \"the cat\" and \"a dog\", and they\nseem to kind of have a bit of structure."},"439":{"dur":3,"text":"We have an article, or\nwhat linguists often call a determiner,"},"443":{"dur":1,"text":"that's followed by a noun."},"445":{"dur":3,"text":"And then, well, for those kind of phrases,"},"448":{"dur":3,"text":"which get called noun\nphrases that describe things,"},"451":{"dur":5,"text":"you can kind of make them bigger and there\nare sort of rules for how you can do that."},"457":{"dur":4,"text":"So you can put adjectives in\nbetween the article and the noun."},"462":{"dur":6,"text":"You can say the large dog or a barking dog\nor a cuddly dog, and things like that."},"468":{"dur":5,"text":"And, well, you can put things like what I\ncall prepositional phrases after the noun"},"473":{"dur":5,"text":"so you can get things like \"a large dog\nin a crate\" or something like that."},"479":{"dur":4,"text":"And so, traditionally what linguists and\nnatural language processors have"},"484":{"dur":5,"text":"wanted to do is describe\nthe structure of human languages."},"489":{"dur":6,"text":"And they're effectively two key tools\nthat people have used to do this and"},"495":{"dur":2,"text":"one of these key tools and"},"498":{"dur":5,"text":"I think in general the only one\nyou have seen a fraction of is"},"503":{"dur":4,"text":"to use what in computer science terms what\nis most commonly referred to as context"},"508":{"dur":4,"text":"free grammars which are often referred to\nby linguists as phrase structure grammars."},"513":{"dur":3,"text":"And is then referred to as\nthe notion of constituency and so"},"517":{"dur":5,"text":"for that what we are doing is writing\nthese context free grammar rules and"},"522":{"dur":3,"text":"the least if you are Standford\nundergrad or something like that."},"525":{"dur":1,"text":"I know that way back in 103,"},"526":{"dur":5,"text":"you spent a whole lecture learning about\ncontext-free grammars, and their rules."},"532":{"dur":4,"text":"So I could start writing some rules that\nmight start off saying a noun phrase,"},"536":{"dur":2,"text":"and go to a determiner or a noun."},"538":{"dur":3,"text":"Then I realized that noun phrases\nwould get a bit more complicated."},"542":{"dur":3,"text":"And so I came up with this new rule\nthat says- Noun phrase goes to terminal"},"545":{"dur":4,"text":"optional adject of noun and then\noptional prepositional phrase wherefore"},"550":{"dur":3,"text":"prepositional phrase that's a preposition\nfollowed by another noun phrase."},"554":{"dur":3,"text":"Because, I can say a crate,\nor, a large crate."},"558":{"dur":2,"text":"Or, a large crate by the door."},"560":{"dur":5,"text":"And then, well I can go along\neven further, and I could say,"},"565":{"dur":6,"text":"you know a large barking\ndog by the door in a crate."},"571":{"dur":3,"text":"So then I noticed, wow I can put\nin multiple adjectives there and"},"575":{"dur":4,"text":"I can stick on multiple prepositional\nphrases, so I'm using that star,"},"580":{"dur":1,"text":"the kinda clingy star that you also see,"},"581":{"dur":4,"text":"See in regular expressions to say that\nyou can have zero or any number of these."},"586":{"dur":6,"text":"And then I can start making a bigger\nthing like, talk to the cuddly dog."},"592":{"dur":1,"text":"Or, look for the cuddly dog."},"594":{"dur":4,"text":"And, well, now I've got a verb\nfollowed by a prepositional phrase."},"599":{"dur":2,"text":"And so, I can sort of build\nup a constituency grammar."},"603":{"dur":6,"text":"So that's one way of organizing\nthe structure of sentences and,"},"610":{"dur":6,"text":"you know,\nin 20th dragging into 21st century"},"616":{"dur":4,"text":"America, this has been\nthe dominant way of doing it."},"621":{"dur":5,"text":"I mean it's what you see mainly in your\nIntro CS class when you get taught"},"627":{"dur":5,"text":"about regular languages and context free\nlanguages and context sensitive languages."},"633":{"dur":5,"text":"You're working up the Chomsky\nhierarchy where Noam Chomsky"},"639":{"dur":5,"text":"did not actually invent\nthe Chomsky hierarchy to torture"},"644":{"dur":5,"text":"CS under grads With formal content\nto fill the SCS 103 class."},"650":{"dur":3,"text":"The original purpose of the Chomsky\nhierarchy was actually to understand"},"654":{"dur":5,"text":"the complexity of human languages and\nto make arguments about their complexity."},"662":{"dur":4,"text":"If you look more broadly, and\nsorry, it's also dominated,"},"667":{"dur":5,"text":"sorta linguistics in America in the last\n50 years through the work of Noam Chomsky."},"672":{"dur":5,"text":"But if you look more broadly than that,\nthis isn't actually the dominate form"},"677":{"dur":2,"text":"of syntactic description\nthat is being used for"},"679":{"dur":2,"text":"understanding of\nthe structure of sentences."},"682":{"dur":0,"text":"So what else can you do?"},"683":{"dur":4,"text":"So there is this other alternative view of\nlinguistic structure which is referred to"},"687":{"dur":4,"text":"as Dependency structure and\nwhat your doing with dependency structure."},"691":{"dur":5,"text":"Is that you're describing the structure\nof a sentence by taking each word and"},"697":{"dur":2,"text":"saying what it's a dependent on."},"699":{"dur":3,"text":"So, if it's a word that\nkind of modifies or"},"703":{"dur":5,"text":"is an argument of another word that you're\nsaying, it's a dependent of that word."},"708":{"dur":6,"text":"So, barking dog, barking is a dependent\nof dog, because it's of a modifier of it."},"714":{"dur":5,"text":"Large barking dog, large is a modifier of\ndog as well, so it's a dependent of it."},"719":{"dur":5,"text":"And dog by the door, so the by the door\nis somehow a dependent of dog."},"725":{"dur":2,"text":"And we're putting\na dependency between words,"},"727":{"dur":4,"text":"and we normally indicate those\ndependencies with arrows."},"731":{"dur":4,"text":"And so we can draw dependency\nstructures over sentences that say"},"736":{"dur":2,"text":"how they're represented as well."},"739":{"dur":5,"text":"And when right in the first class,\nI gave examples of ambiguous sentences."},"744":{"dur":5,"text":"A lot of those ambiguous sentences, we\ncan think about in terms of dependencies."},"749":{"dur":4,"text":"So do you remember this one,\nscientists study whales from space."},"755":{"dur":2,"text":"Well that was an ambiguous headline."},"758":{"dur":2,"text":"And well, why is it an ambiguous headline?"},"760":{"dur":3,"text":"Well it's ambiguous because\nthere's sort of two possibilities."},"764":{"dur":5,"text":"So in either case there's\nthe main verb study."},"769":{"dur":4,"text":"And it's the scientist that's studying,\nthat's an argument of study, the subject."},"773":{"dur":3,"text":"And it's the whales that are being\nstudied, so that's an argument of study."},"777":{"dur":1,"text":"That's the object."},"778":{"dur":5,"text":"But the big difference is then,\nwhat are you doing with the from space."},"784":{"dur":6,"text":"You saying that it's modifying study,\nor are you saying it's modifying whales?"},"790":{"dur":3,"text":"And like, if you sort of just\nquickly read the headline"},"793":{"dur":1,"text":"It sounds like it's the bottom one, right?"},"795":{"dur":2,"text":"It's whales from space."},"798":{"dur":2,"text":"And that sounds really exciting."},"800":{"dur":3,"text":"But [LAUGH] what the article was meant to\nbe about was, really, that they were being"},"804":{"dur":3,"text":"able to use satellites to\ntrack the movements of whales."},"807":{"dur":4,"text":"And so it's the first one where the,\nfrom space, is modifying."},"811":{"dur":1,"text":"How they're being studied."},"813":{"dur":5,"text":"And so thinking about ambiguities of\nsentences can then be thought about,"},"819":{"dur":4,"text":"many of them, in terms of these dependency\nstructures as to what's modifying what."},"823":{"dur":3,"text":"And this is just a really common thing"},"827":{"dur":4,"text":"in natural language because these kind\nof questions of what modifies what,"},"832":{"dur":4,"text":"really dominate a lot of\nquestions of interpretation."},"836":{"dur":2,"text":"So, here's the kind of sentence"},"838":{"dur":3,"text":"you find when you're reading\nthe Wall Street Journal every morning."},"842":{"dur":5,"text":"The board approved its acquisition by\nRoyal Trustco Limited of Toronto for"},"848":{"dur":2,"text":"$27 a share at its Monthly meeting."},"850":{"dur":4,"text":"And as I've hopefully indicated by\nthe square brackets, if you look at"},"855":{"dur":5,"text":"the structure of this sentence, it sort\nof starts off as subject, verb, object."},"860":{"dur":1,"text":"The board approved its acquisition,"},"861":{"dur":4,"text":"and then everything after that is a whole\nsequence of prepositional phrases."},"866":{"dur":6,"text":"By Royal Trustco Ltd, of Toronto, for\n$27 a share, at its monthly meeting."},"873":{"dur":6,"text":"And well, so then there's the question of,\nwhat's everyone modifying?"},"879":{"dur":5,"text":"So the acquisition is by\nRoyal Trustco Ltd, so that's,"},"885":{"dur":5,"text":"by Royal Trustco Ltd is modifying\nthe thing that immediately precedes that."},"890":{"dur":5,"text":"And of Toronto is modifying the company,\nRoyal Trustco Limited,"},"896":{"dur":3,"text":"so that's modifying the thing that\ncomes immediately preceeding it."},"900":{"dur":2,"text":"So you might think this is easy,"},"902":{"dur":4,"text":"everything just modifies the thing\nthat's coming immediately before it."},"906":{"dur":1,"text":"But that, then stops being true."},"908":{"dur":3,"text":"So, what's for $27 a share modifying?"},"914":{"dur":2,"text":"Yeah so\nthat's modifying the acquisition so"},"916":{"dur":3,"text":"then we're jumping back\na few candidates and"},"919":{"dur":5,"text":"saying is modifying acquisition and\nthen actually at it's monthly meeting."},"924":{"dur":5,"text":"That wasn't the Toronto the Royal\nTrustco Ltd or the acquisition that that"},"930":{"dur":5,"text":"was when the approval was happening so\nthat jumps all the way back up to the top."},"935":{"dur":4,"text":"So in general the situation is that if\nyou've got some stuff like a verb and"},"940":{"dur":4,"text":"a noun phrase, then you start\ngetting these prepositional phrases."},"945":{"dur":4,"text":"Well, the prepositional\nphrase can be modifying,"},"949":{"dur":1,"text":"either this noun phrase or the verb."},"951":{"dur":3,"text":"But then when you get to\nthe second prepositional phrase."},"955":{"dur":3,"text":"Well, there was another noun phrase\ninside this prepositional phrase."},"958":{"dur":0,"text":"So, now there's."},"959":{"dur":0,"text":"Three choices."},"960":{"dur":4,"text":"It can be modifying this noun phrase,\nthat noun phrase or the verb phrase."},"964":{"dur":1,"text":"And then we get to another one."},"966":{"dur":2,"text":"So it's now got four choices."},"968":{"dur":5,"text":"And you don't get sort of\na completely free choice,"},"974":{"dur":2,"text":"cuz you do get a nesting constraint."},"976":{"dur":5,"text":"So once I've had for $27 a share\nreferring back to the acquisition,"},"982":{"dur":3,"text":"the next prepositional phrase has to,\nin general,"},"985":{"dur":3,"text":"refer to either the acquisition or\napproved."},"988":{"dur":3,"text":"I say in general because\nthere are exceptions to that."},"991":{"dur":2,"text":"And I'll actually talk about that later."},"993":{"dur":1,"text":"But most of the time in English,\nit's true."},"995":{"dur":2,"text":"You have to sort of refer to\nthe same one or further back, so"},"998":{"dur":2,"text":"you get a nesting relationship."},"1000":{"dur":5,"text":"But I mean, even if you obey that nesting\nrelationship, the result is that you"},"1005":{"dur":5,"text":"get an exponential number of\nambiguities in a sentence based"},"1011":{"dur":4,"text":"on in the number of prepositional phrases\nyou stick on the end of the sentence."},"1015":{"dur":5,"text":"And so the series of the exponential\nseries you get of these Catalan numbers."},"1021":{"dur":2,"text":"And so Catalan numbers actually show up"},"1023":{"dur":3,"text":"in a lot of places in\ntheoretical computer science."},"1027":{"dur":5,"text":"Because any kind of structure\nthat is somehow sort of similar,"},"1032":{"dur":3,"text":"if you're putting these constraints in,\nyou get Catalan series."},"1035":{"dur":2,"text":"So, are any of you doing CS228?"},"1040":{"dur":0,"text":"Yeah, so"},"1041":{"dur":5,"text":"another place the Catalan series turns up\nis that when you've got a vector graph and"},"1046":{"dur":5,"text":"you're triangulating it, the number of\nways that you can triangulate your vector"},"1052":{"dur":5,"text":"graph is also giving you Catalan numbers."},"1057":{"dur":3,"text":"Okay, so\nhuman languages get very ambiguous."},"1061":{"dur":4,"text":"And we can hope to describe\nthem on the basis of sort of"},"1065":{"dur":2,"text":"looking at these dependencies."},"1068":{"dur":1,"text":"So that's important concept One."},"1070":{"dur":4,"text":"The other important concept I wanted to\nintroduce at this point is this idea of"},"1075":{"dur":6,"text":"full linguistics having annotated\ndata in the form of treebanks."},"1081":{"dur":3,"text":"This is probably a little\nbit small to see exactly."},"1085":{"dur":3,"text":"But what this is, is we've got sentences."},"1089":{"dur":3,"text":"These are actually sentences\nthat come off Yahoo Answers."},"1094":{"dur":5,"text":"And what's happened is,\nhuman beings have sat around and"},"1099":{"dur":5,"text":"drawn in the syntactic structures of\nthese sentences as dependency graphs and"},"1104":{"dur":3,"text":"those things we refer to as treebanks."},"1108":{"dur":4,"text":"And so a really interesting thing\nthat's happened starting around"},"1113":{"dur":4,"text":"1990 is that people have devoted a lot of"},"1118":{"dur":4,"text":"resources to building up these kind\nof annotated treebanks and various"},"1123":{"dur":4,"text":"other kinds of annotated linguistic\nresources that we'll talk about later."},"1127":{"dur":5,"text":"Now in some sense, from the viewpoint of\nsort of modern machine learning in 2017,"},"1132":{"dur":2,"text":"that's completely unsurprising,"},"1135":{"dur":3,"text":"because all the time what we do\nis say we want labelled data so"},"1139":{"dur":4,"text":"we can take our supervised classifier and\nchug on it and get good results."},"1144":{"dur":4,"text":"But in many ways, it was kind of\na surprising thing that happened,"},"1148":{"dur":4,"text":"which is sort of different to the whole\nof the rest of history, right?"},"1153":{"dur":4,"text":"Cuz for the whole of the rest of\nthe history, it was back in this space of,"},"1158":{"dur":3,"text":"well, to describe linguistic\nstructure what we should be doing"},"1162":{"dur":5,"text":"is writing grammar rules that describe\nwhat happens in linguistic structure."},"1167":{"dur":4,"text":"Where here, we're no longer even\nattempting to write grammar rules."},"1171":{"dur":1,"text":"We're just saying, give us some sentences."},"1173":{"dur":3,"text":"And I'm gonna diagram these sentences and\nshow you what their structure is."},"1177":{"dur":4,"text":"And tomorrow give me a bunch more and\nI'll diagram them for you as well."},"1181":{"dur":4,"text":"And if you think about it, in a way,\nthat initially seems kind of"},"1186":{"dur":4,"text":"a crazy thing to do, cuz it seems\nlike just putting structures over"},"1191":{"dur":4,"text":"sentences one by one seems really,\nreally inefficient and slow."},"1196":{"dur":1,"text":"Whereas, if you're writing a grammar,"},"1198":{"dur":2,"text":"you're writing this thing\nthat generalizes, right?"},"1200":{"dur":2,"text":"The whole point of grammar is that\nyou're gonna write this one small,"},"1203":{"dur":3,"text":"And it describes an infinite\nnumber of sentences."},"1206":{"dur":3,"text":"And so surely,\nthat's a big labor saving effort."},"1210":{"dur":5,"text":"But, slightly surprisingly, but maybe it\nmakes sense in terms of what's happened"},"1216":{"dur":5,"text":"in machine learning, that it's just turned\nout to be kind of super successful,"},"1221":{"dur":3,"text":"this building of explicit,\nannotated treebanks."},"1225":{"dur":3,"text":"And it ends up giving us a lot of things."},"1228":{"dur":2,"text":"And I sort of mention a few\nof their advantages here."},"1231":{"dur":2,"text":"First, it gives you\na reusability of labor."},"1234":{"dur":4,"text":"But the problem of human beings\nhandwriting grammars is that they tend to,"},"1238":{"dur":4,"text":"in practice, be almost unreusable,\nbecause everybody does it differently and"},"1243":{"dur":2,"text":"has their idea of the grammar."},"1245":{"dur":4,"text":"And people spend years working on one and\nno one else ever uses it."},"1250":{"dur":4,"text":"Where effectively, these treebanks have\nbeen a really reusable tool that lots of"},"1254":{"dur":4,"text":"people have then built on top of to\nbuild all kinds of natural language"},"1258":{"dur":4,"text":"processing tools, of part of speech\ntaggers and parsers and things like that."},"1263":{"dur":4,"text":"They've also turned out to be a really\nuseful resource, actually, for linguists,"},"1267":{"dur":4,"text":"because they give a kind of real languages\nare spoken, complete with syntactic"},"1272":{"dur":4,"text":"analyses that you can do all kinds of\nquantitative linguistics on top of."},"1277":{"dur":3,"text":"It's genuine data that's broad\ncoverage when people just work"},"1280":{"dur":2,"text":"with their intuitions as to what\nare the grammar rules of English."},"1283":{"dur":2,"text":"They think of some things but\nnot of other things."},"1285":{"dur":3,"text":"And so this is actually a better way to\nfind out all of the things that actually"},"1289":{"dur":0,"text":"happened."},"1290":{"dur":2,"text":"For anything that's sort\nof probabilistic or"},"1293":{"dur":4,"text":"machine learning, it gives some sort\nof not only what's possible, but"},"1297":{"dur":3,"text":"how frequent it is and what other\nthings it tends to co-occur with and"},"1301":{"dur":3,"text":"all that kind of distributional\ninformation that's super important."},"1304":{"dur":4,"text":"And crucially, crucially, crucially,\nand we'll use this for assignment two,"},"1308":{"dur":5,"text":"it's also great because it gives you\na way to evaluate any system that you"},"1314":{"dur":5,"text":"built because this gives us what we treat\nas ground truth, gold standard data."},"1319":{"dur":1,"text":"These are the correct answers."},"1321":{"dur":5,"text":"And then we can evaluate any tool on\nhow good it is at reproducing those."},"1326":{"dur":2,"text":"Okay, so that's the general advertisement."},"1329":{"dur":4,"text":"And what I wanted to do now is sort of\ngo through a bit more carefully for"},"1334":{"dur":5,"text":"sort of 15 minutes, what are dependency\ngrammars and dependency structure?"},"1339":{"dur":1,"text":"So we've sort of got that straight."},"1341":{"dur":3,"text":"I guess I've maybe failed to say, yeah."},"1345":{"dur":3,"text":"I mentioned there was this sort of\nconstituency context-free grammar"},"1349":{"dur":2,"text":"viewpoint and\nthe dependency grammar viewpoint."},"1353":{"dur":2,"text":"Today, it's gonna be all dependencies."},"1356":{"dur":3,"text":"And what we're doing for\nassignment two is all dependencies."},"1359":{"dur":3,"text":"We will get back to some notions of\nconstituency and phrase structure."},"1363":{"dur":4,"text":"You'll see those coming back in\nlater classes in a few weeks' time."},"1368":{"dur":2,"text":"But this is what we're\ngoing to be doing today."},"1370":{"dur":2,"text":"And that's not a completely random choice."},"1373":{"dur":4,"text":"It's turned out that, unlike what's\nhappened in linguistics in most of"},"1377":{"dur":4,"text":"the last 50 years, in the last decade\nin natural language processing,"},"1382":{"dur":3,"text":"it's essentially been swept by\nthe use of dependency grammars,"},"1386":{"dur":4,"text":"that people have found dependency\ngrammars just a really suitable"},"1390":{"dur":4,"text":"framework on which to build semantic\nrepresentations to get out the kind of"},"1394":{"dur":3,"text":"understanding of language that\nthey'd like to get out easily."},"1398":{"dur":2,"text":"They enable the building of very fast,"},"1401":{"dur":3,"text":"efficient parsers,\nas I'll explain later today."},"1404":{"dur":1,"text":"And so in the last sort of ten years,"},"1406":{"dur":4,"text":"you've just sort of seen this huge sea\nchange in natural language processing."},"1410":{"dur":4,"text":"Whereas, if you pick up a conference\nvolume around the 1990s, it was basically"},"1415":{"dur":4,"text":"all phrase structure grammars and one or\ntwo papers on dependency grammars."},"1419":{"dur":1,"text":"And if you pick up a volume now,"},"1421":{"dur":5,"text":"what you'll find out is that of the papers\nthey're using syntactic representations,"},"1426":{"dur":3,"text":"kind of 80% of them are using\ndependency representations."},"1430":{"dur":1,"text":"Okay, yes."},"1432":{"dur":1,"text":">> What's that,\na phrase structure grammar?"},"1433":{"dur":3,"text":"Phrase structure, what's the phrase\nstructure grammar, that's exactly the same"},"1436":{"dur":2,"text":"as the context-free grammar\nwhen a linguist is speaking."},"1439":{"dur":5,"text":"[LAUGH] Yes,\nformerly a context-free grammar."},"1444":{"dur":4,"text":"Okay, so\nwhat does a dependency syntax say?"},"1449":{"dur":5,"text":"So the idea of dependency syntax\nis to say that the sort of model"},"1454":{"dur":4,"text":"of syntax is we have relationships\nbetween lexical items,"},"1459":{"dur":3,"text":"words, and only between lexical items."},"1463":{"dur":5,"text":"They're binary, asymmetric relations,\nwhich means we draw arrows."},"1468":{"dur":3,"text":"And we call those arrows dependencies."},"1471":{"dur":5,"text":"So the whole, there is a dependency\nanalysis of bills on ports and"},"1476":{"dur":4,"text":"immigration were submitted by\nSenator Brownback, Republican of Kansas."},"1481":{"dur":6,"text":"Okay, so that's a start,\nnormally hen we do dependency parsing,"},"1487":{"dur":3,"text":"we do a little bit more than that."},"1490":{"dur":5,"text":"So typically we type the dependencies\nby giving them a name for"},"1496":{"dur":3,"text":"some grammatical relationship."},"1500":{"dur":5,"text":"So I'm calling this the subject, and\nit's actually a passive subject."},"1505":{"dur":3,"text":"And then this is an auxiliary modifier,\nand"},"1508":{"dur":5,"text":"Republican of Kansas is an appositional\nphrase that's coming off of Brownback."},"1514":{"dur":4,"text":"And so we use this kind of\ntyped dependency grammars."},"1518":{"dur":4,"text":"And interestingly,\nI'm not going to go through it, but"},"1523":{"dur":5,"text":"there's sort of some interesting\nmath that if you just have this,"},"1528":{"dur":2,"text":"although it's notationally very different,"},"1531":{"dur":6,"text":"from context-free grammar,\nthese are actually equivalent"},"1537":{"dur":4,"text":"to a restricted kind of context-free\ngrammar with one addition."},"1541":{"dur":4,"text":"But things become sort of a bit more\ndifferent once you put in a typing"},"1546":{"dur":4,"text":"of the dependency labels, where I wont\ngo into that in great detail, right."},"1551":{"dur":3,"text":"So a substantive theory\nof dependency grammar for"},"1555":{"dur":4,"text":"a language,\nwe're then having to make some decisions."},"1559":{"dur":4,"text":"So what we're gonna do is when we,\nwe're gonna draw these arrows"},"1563":{"dur":3,"text":"between two things, and\nI'll just mention a bit more terminology."},"1567":{"dur":6,"text":"So we have an arrow and its got what we\ncalled the tail end of the arrow, I guess."},"1574":{"dur":2,"text":"And the word up here is sort of the head."},"1576":{"dur":6,"text":"So bills is an argument of submitted, were\nis an auxiliary modifier of submitted."},"1583":{"dur":4,"text":"And so this word here is normally referred\nto as the head, or the governor, or"},"1587":{"dur":3,"text":"the superior, or\nsometimes even the regent."},"1591":{"dur":3,"text":"I'll normally call it the head."},"1594":{"dur":3,"text":"And then the word at\nthe other end of the arrow,"},"1597":{"dur":3,"text":"the pointy bit,\nI'll refer to as the dependent,"},"1600":{"dur":5,"text":"but other words that you can sometimes\nsee are modifier, inferior, subordinate."},"1606":{"dur":4,"text":"Some people who do dependency grammar\nreally get into these classist notions"},"1610":{"dur":3,"text":"of superiors and inferiors, but\nI'll go with heads and dependents."},"1615":{"dur":3,"text":"Okay, so the idea is you\nhave a head of a clause and"},"1619":{"dur":2,"text":"then the arguments of the dependence."},"1621":{"dur":3,"text":"And then when you have a phrase like,"},"1625":{"dur":4,"text":"by Senator Brownback, Republican of Texas."},"1630":{"dur":3,"text":"It's got a head which is here\nbeing taken as Brownback and"},"1634":{"dur":2,"text":"then it's got words beneath it."},"1636":{"dur":5,"text":"And so one of the main parts of\ndependency grammars at the end of the day"},"1641":{"dur":4,"text":"is you have to make decisions\nas to which words are heads and"},"1645":{"dur":5,"text":"which words are then the dependents of\nthe heads of any particular structure."},"1651":{"dur":4,"text":"So in these diagrams I'm showing you here,\nand"},"1655":{"dur":4,"text":"the ones I showed you back a few pages,\nwhat I'm actually showing you"},"1659":{"dur":3,"text":"here is analysis according\nto universal dependencies."},"1663":{"dur":3,"text":"So universal dependencies is\na new tree banking effort"},"1667":{"dur":2,"text":"which I've actually been\nvery strongly involved in."},"1670":{"dur":2,"text":"That sort of started\na couple of years ago and"},"1672":{"dur":3,"text":"there are pointers in both\nearlier in the slides and"},"1675":{"dur":4,"text":"on the website if you wanna go off and\nlearn a lot about universal dependencies."},"1679":{"dur":2,"text":"I mean it's sort of\nan ambitious attempt to try and"},"1681":{"dur":4,"text":"have a common dependency representation\nthat works over a ton of languages."},"1686":{"dur":1,"text":"I could prattle on about it for"},"1688":{"dur":4,"text":"ages, and if by some off chance there's\ntime at the end of the class I could."},"1693":{"dur":4,"text":"But probably there won't be so I won't\nactually tell you a lot about that now."},"1697":{"dur":5,"text":"But I will just mention one thing that\nprobably you'll notice very quickly."},"1702":{"dur":3,"text":"And we're also going to be using this\nrepresentation in the assignment that's"},"1706":{"dur":5,"text":"being given out today,\nthe analysis of universal dependencies"},"1712":{"dur":5,"text":"treats prepositions sort of differently\nto what you might have seen else where."},"1717":{"dur":4,"text":"If you've seen any, many accounts of\nEnglish grammar, or heard references in"},"1722":{"dur":4,"text":"some English classroom,\nto have prepositions, having objects."},"1726":{"dur":6,"text":"In universal dependencies,\nprepositions don't have any dependents."},"1732":{"dur":3,"text":"Prepositions are treated kind\nof like they were case markers,"},"1736":{"dur":3,"text":"if you know any language like, German, or"},"1739":{"dur":5,"text":"Latin, or Hindi, or\nsomething that has cases."},"1744":{"dur":5,"text":"So that the by is sort of treated as\nif it were a case marker of Brownback."},"1749":{"dur":4,"text":"So this sort of a bleak modifier\nof by Senator Brownback."},"1753":{"dur":3,"text":"And so it's actually treating\nBrownback here as the head"},"1757":{"dur":4,"text":"with the preposition as sort of like\na case marking dependent of by."},"1761":{"dur":4,"text":"And that was sort of done to get more\nparallelism across different languages"},"1765":{"dur":1,"text":"of the world."},"1766":{"dur":2,"text":"But I'll just mention that."},"1769":{"dur":6,"text":"Other properties of old dependencies,\nnormally dependencies form a tree."},"1775":{"dur":2,"text":"So there are formal properties\nthat goes along with that."},"1777":{"dur":7,"text":"That means that they've got a single-head,\nthey're acyclic, and they're connected."},"1785":{"dur":3,"text":"So there is a sort of graph\ntheoretic properties."},"1789":{"dur":2,"text":"Yeah, I sort of mentioned that really"},"1792":{"dur":3,"text":"dependencies have dominated\nmost of the world."},"1795":{"dur":2,"text":"So just very quickly on that."},"1798":{"dur":5,"text":"The famous first linguist was Panini,"},"1803":{"dur":5,"text":"who wrote his Grammar of Sanskrit\naround the fifth century BCE."},"1808":{"dur":4,"text":"Really most of the work that Panini\ndid was kind of on sound systems and"},"1813":{"dur":2,"text":"make ups of words,\nphonology, and morphology,"},"1816":{"dur":4,"text":"when we mentioned linguistic\nlevels in the first class."},"1820":{"dur":4,"text":"And he only did a little bit of\nwork on the structure of sentences."},"1824":{"dur":1,"text":"But the notation that he used for"},"1826":{"dur":4,"text":"structure of sentences was essentially\na dependency grammar of having word"},"1831":{"dur":3,"text":"relationships being\nmarked as dependencies."},"1837":{"dur":0,"text":"Question?"},"1872":{"dur":4,"text":"Yeah, so the question is,\nwell compare CFGs and PCFGs and"},"1877":{"dur":4,"text":"do they, dependency grammars\nlook strongly lexicalized,"},"1882":{"dur":5,"text":"they're between words and\ndoes that makes it harder to generalize."},"1888":{"dur":2,"text":"I honestly feel I just\ncan't do justice to that"},"1891":{"dur":2,"text":"question right now if I'm gonna get\nthrough the rest of the lecture."},"1893":{"dur":2,"text":"But I will make two comments, so I mean,"},"1896":{"dur":4,"text":"there's certainly the natural way\nto think of dependency grammars,"},"1901":{"dur":4,"text":"they're strongly lexicalized, you're\ndrawing relationships between words."},"1906":{"dur":3,"text":"Whereas the simplest way of thinking of\ncontext-free grammars is you've got these"},"1909":{"dur":2,"text":"rules in terms of categories like."},"1911":{"dur":4,"text":"Noun phrase goes to determiner noun,\noptional prepositional phrase."},"1916":{"dur":4,"text":"And so, that is a big difference."},"1920":{"dur":2,"text":"But it kind of goes both ways."},"1923":{"dur":4,"text":"So, normally, when actually, natural\nlanguage processing people wanna work with"},"1927":{"dur":3,"text":"context-free grammars,\nthey frequently lexicalize them so"},"1931":{"dur":4,"text":"they can do more precise probabilistic\nprediction, and vice versa."},"1935":{"dur":2,"text":"If you want to do generalization and\ndependency grammar,"},"1938":{"dur":3,"text":"you can still use at least\nnotions of parts of speech"},"1941":{"dur":4,"text":"to give you a level of generalization\nas more like categories."},"1945":{"dur":4,"text":"But nevertheless, the kind of natural\nways of sort of turning them into"},"1949":{"dur":3,"text":"probabilities, and machine learning\nmodels are quite different."},"1953":{"dur":2,"text":"Though, on the other hand,\nthere's sort of some results, or"},"1956":{"dur":1,"text":"sort of relationships between them."},"1957":{"dur":2,"text":"But I would think I'd better\nnot go on a huge digression."},"1960":{"dur":1,"text":"But you have another question?"},"1964":{"dur":4,"text":"That means to rather than just have\ncategories like noun phrase to have"},"1969":{"dur":4,"text":"categories like a noun phrase headed\nby dog, and so it's lexicalized."},"1974":{"dur":4,"text":"Let's leave this for\nthe moment though, please, okay."},"1978":{"dur":3,"text":"[LAUGH]\nOkay, so"},"1982":{"dur":3,"text":"that's Panini, and\nthere's a whole big history, right?"},"1985":{"dur":4,"text":"So, essentially for\nLatin grammarians, what they did for"},"1989":{"dur":3,"text":"the syntax of Latin,\nagain, not very developed."},"1993":{"dur":1,"text":"They mainly did morphology, but"},"1995":{"dur":3,"text":"it was essentially a dependency\nkind of analysis that was given."},"1998":{"dur":4,"text":"There was sort of a flowering of Arabic\ngrammarians in the first millennium, and"},"2003":{"dur":2,"text":"they essentially had a dependency grammar."},"2005":{"dur":7,"text":"I mean, by contrast, I mean, really kind\nof context free grammars and constituency"},"2012":{"dur":6,"text":"grammar only got invented almost in\nthe second half of the 20th century."},"2019":{"dur":2,"text":"I mean, it wasn't actually Chomsky\nthat originally invented them,"},"2021":{"dur":4,"text":"there was a little bit of earlier work in\nBritain, but only kind of a decade before."},"2027":{"dur":5,"text":"So, there was this French\nlinguist Lucien Tesniere,"},"2033":{"dur":4,"text":"he is often referred to as the father\nof modern dependency grammar,"},"2037":{"dur":1,"text":"he's got a book from 1959."},"2039":{"dur":6,"text":"Dependency grammars have been very popular\nand more sorta free word order languages,"},"2046":{"dur":4,"text":"cuz notions, sort of like context-free\ngrammars work really well for"},"2050":{"dur":3,"text":"languages like English that\nhave very fixed word order, but"},"2053":{"dur":5,"text":"a lot of other languages of the world\nhave much freer word order."},"2059":{"dur":4,"text":"And that's often more naturally\ndescribed with dependency grammars."},"2064":{"dur":4,"text":"Interestingly, one of the very first\nnatural language parsers developed"},"2068":{"dur":4,"text":"in the US was also a dependency parser."},"2073":{"dur":4,"text":"So, David Hays was one of the first\nUS computational linguists."},"2077":{"dur":4,"text":"And one of the founders of the Association\nfor Computational Linguistics which is our"},"2082":{"dur":4,"text":"main kind of academic association where\nwe publish our conference papers, etc."},"2086":{"dur":7,"text":"And he actually built in 1962,\na dependency parser for English."},"2095":{"dur":2,"text":"Okay, so\na lot of history of dependency grammar."},"2098":{"dur":4,"text":"So, couple of other fine points\nto note about the notation."},"2103":{"dur":4,"text":"People aren't always consistent in\nwhich way they draw the arrows."},"2107":{"dur":4,"text":"I'm always gonna draw the arrows, so\nthey point, go from a head to a dependent,"},"2112":{"dur":2,"text":"which is the direction\nwhich Tesniere drew them."},"2114":{"dur":3,"text":"But there are some other people who\ndraw the arrows the other way around."},"2118":{"dur":2,"text":"So, they point from\nthe dependent to the head."},"2120":{"dur":3,"text":"And so, you just need to look and\nsee what people are doing."},"2123":{"dur":3,"text":"The other thing that's very commonly done,\nand we will do in our parses,"},"2127":{"dur":4,"text":"is you stick this pseudo-word,\nwhich might be called ROOT or"},"2132":{"dur":5,"text":"WALL, or some other name like that,\nat the start of the sentence."},"2137":{"dur":4,"text":"And that kind of makes the math and\nformalism easy,"},"2142":{"dur":5,"text":"because, then, every sentence starts with\nroot and something is a dependent of root."},"2148":{"dur":4,"text":"Or, turned around the other way, if you\nthink of what parsing a dependency grammar"},"2153":{"dur":3,"text":"means is for every word in\nthe sentence you're going to say,"},"2156":{"dur":3,"text":"what is it a dependent of,\nbecause if you do that you're done."},"2159":{"dur":2,"text":"You've got the dependency\nstructure of the sentence."},"2162":{"dur":4,"text":"And what you're gonna want to say is,\nwell, it's either gonna be a dependent of"},"2167":{"dur":3,"text":"some other word in the sentence,\nor it's gonna be a dependent of"},"2170":{"dur":4,"text":"the pseudo-word ROOT, which is meaning\nit's the head of the entire sentence."},"2176":{"dur":5,"text":"And so, we'll go through some\nspecifics of dependency parsing"},"2182":{"dur":1,"text":"the second half of the class."},"2184":{"dur":3,"text":"But the kind of thing that you\nshould think about is well,"},"2187":{"dur":5,"text":"how could we decide which\nwords are dependent on what?"},"2193":{"dur":4,"text":"And there are certain various information\nsources that we can think about."},"2198":{"dur":4,"text":"So yeah, it's sort of totally natural with\nthe dependency representation to just"},"2202":{"dur":2,"text":"think about word relationships."},"2204":{"dur":3,"text":"And that's great, cuz that'll fit super\nwell with what we've done already in"},"2208":{"dur":1,"text":"distributed word representations."},"2209":{"dur":4,"text":"So actually,\ndoing things this way just fits well"},"2213":{"dur":4,"text":"with a couple of tools we\nalready know how to use."},"2217":{"dur":2,"text":"We'll want to say well,\ndiscussion of issues,"},"2220":{"dur":3,"text":"is that a reasonable attachment\nas lexical dependency?"},"2224":{"dur":3,"text":"And that's a lot of the information\nthat we'll actually use, but"},"2227":{"dur":3,"text":"there's some other sources of\ninformation that we'd also like to use."},"2231":{"dur":5,"text":"Dependency distance, so sometimes,\nthere are dependency relationships and"},"2236":{"dur":4,"text":"sentences between words that is 20 words\napart when you got some big long sentence,"},"2241":{"dur":2,"text":"and you're referring that back\nto some previous clause, but"},"2243":{"dur":0,"text":"it's kind of uncommon."},"2244":{"dur":4,"text":"Most of dependencies are pretty short\ndistance, so you want to prefer that."},"2250":{"dur":5,"text":"Many dependencies don't, sort of,\nspan certain kinds of things."},"2255":{"dur":4,"text":"So, if you have the kind of dependencies\nthat occur inside noun phrases,"},"2260":{"dur":3,"text":"like adjective modifier,\nthey're not gonna cross over a verb."},"2264":{"dur":5,"text":"It's unusual for many kinds of\ndependencies to cross over a punctuation,"},"2269":{"dur":3,"text":"so it's very rare to have a punctuation\nbetween a verb and a subject and"},"2273":{"dur":0,"text":"things like that."},"2274":{"dur":3,"text":"So, looking at the intervening\nmaterial gives you some clues."},"2277":{"dur":5,"text":"And the final source of information is\nsort of thinking about heads, and thinking"},"2283":{"dur":6,"text":"how likely they are to have to dependence\nin what number, and on what sides."},"2289":{"dur":4,"text":"So, the kind of information there is,\nright, a word like the,"},"2293":{"dur":4,"text":"is basically not likely to have\nany dependents at all, anywhere."},"2298":{"dur":2,"text":"So, you'd be surprised if it did."},"2301":{"dur":5,"text":"Words like nouns can have dependents, and\nthey can have quite a few dependents,"},"2306":{"dur":4,"text":"but they're likely to have some kinds like\ndeterminers and adjectives on the left,"},"2311":{"dur":3,"text":"other kinds like prepositional\nphrases on the right"},"2314":{"dur":1,"text":"verbs tend to have a lot of dependence."},"2316":{"dur":3,"text":"So, different kinds of words have\ndifferent kinds of patterns of dependence,"},"2320":{"dur":3,"text":"and so there's some information\nthere we could hope to gather."},"2325":{"dur":5,"text":"Okay, yeah,\nI guess I've already said the first point."},"2330":{"dur":2,"text":"How do we do dependency parsing?"},"2332":{"dur":3,"text":"In principle, it's kind of really easy."},"2336":{"dur":5,"text":"So, we're just gonna take every\nword in the sentence and say,"},"2341":{"dur":5,"text":"make a decision as to what word or\nroot this word is a dependent of."},"2347":{"dur":2,"text":"And we do that with a few constraints."},"2350":{"dur":5,"text":"So normally, we require that only\none word can be a dependent of root,"},"2356":{"dur":2,"text":"and we're not going to allow any cycles."},"2359":{"dur":3,"text":"And if we do both of those things,"},"2363":{"dur":4,"text":"we're guaranteeing that we make\nthe dependencies of a tree."},"2367":{"dur":4,"text":"And normally,\nwe want to make out dependencies a tree."},"2372":{"dur":5,"text":"And there's one other property\nI then wanted to mention,"},"2377":{"dur":4,"text":"that if you draw your\ndependencies as I have here, so"},"2382":{"dur":5,"text":"all the dependencies been drawn\nas loops above the words."},"2388":{"dur":4,"text":"It's different if you're allowed to\nput some of them below the words."},"2392":{"dur":4,"text":"There's then a question as to\nwhether you can draw them like this."},"2396":{"dur":4,"text":"So that they have that kind of nice,\nlittle nesting structure, but"},"2400":{"dur":2,"text":"none of them cross each other."},"2402":{"dur":5,"text":"Or whether, like these two that I've\ngot here, where they necessarily"},"2407":{"dur":4,"text":"cross each other, and\nI couldn't avoid them crossing each other."},"2412":{"dur":5,"text":"And what you'll find is in most languages,\ncertainly English,"},"2417":{"dur":4,"text":"the vast majority of dependency\nrelationships have a nesting"},"2422":{"dur":3,"text":"structure relative to the linear order."},"2426":{"dur":3,"text":"And if a dependency tree is fully nesting,"},"2429":{"dur":2,"text":"it's referred to as\na projective dependency tree,"},"2432":{"dur":5,"text":"that you can lay it out in this plane,\nand have sort of a nesting relationship."},"2437":{"dur":3,"text":"But there are few structures"},"2441":{"dur":3,"text":"in English where you'd get things\nthat aren't nested and yet crossing."},"2445":{"dur":2,"text":"And this sentence is\na natural example of one."},"2447":{"dur":2,"text":"So I'll give a talk\ntomorrow on bootstrapping."},"2450":{"dur":4,"text":"So something that you can do with\nnoun modifiers, especially if they're"},"2455":{"dur":4,"text":"kind of long words like bootstrapping or\ntechniques of bootstrapping,"},"2459":{"dur":4,"text":"is you can sort of move them towards\nthe end of the sentence, right."},"2463":{"dur":3,"text":"I could have said I'll give\na talk on bootstrapping tomorrow."},"2467":{"dur":4,"text":"But it sounds pretty natural to say, I'll\ngive a talk tomorrow on bootstrapping."},"2472":{"dur":3,"text":"But this on bootstrapping is\nstill modifying the talk."},"2475":{"dur":4,"text":"And so that's referred to by\nlinguists as right extraposition."},"2480":{"dur":3,"text":"And so when you get that kind of\nrightward movement of phrases,"},"2483":{"dur":3,"text":"you then end up with these crossing lines."},"2486":{"dur":4,"text":"And that gives you what's referred to\nas a non-projective dependency tree."},"2491":{"dur":3,"text":"So, importantly,\nit is still a tree if you sort of"},"2495":{"dur":3,"text":"ignore the constraints of linear order,\nand you're just drawing it out."},"2499":{"dur":4,"text":"There's a graph in theoretical computer\nscience, right, it's still a tree."},"2503":{"dur":4,"text":"It's only when you consider this extra\nthing of the linear order of the words,"},"2508":{"dur":2,"text":"that you're then forced\nto have the lines across."},"2511":{"dur":3,"text":"And so that property which you don't\nactually normally see mentioned in"},"2514":{"dur":3,"text":"theoretical computer science\ndiscussions of graphs"},"2517":{"dur":3,"text":"is then this property that's\nreferred to projectivity."},"2520":{"dur":7,"text":"Yes.\n>> [INAUDIBLE]"},"2528":{"dur":2,"text":">> So the questions is is it possible to"},"2530":{"dur":3,"text":"recover the order of the words\nfrom a dependency tree."},"2534":{"dur":5,"text":"So given how I've defined dependency\ntrees, the strict answer is no."},"2540":{"dur":2,"text":"They aren't giving you the order at all."},"2542":{"dur":5,"text":"Now, in practice, people write down\nthe words of a sentence in order and have"},"2547":{"dur":5,"text":"these crossing brackets, right, crossing\narrows when they're non-projective."},"2553":{"dur":3,"text":"And, of course, it would be a\nstraightforward thing to index the words."},"2557":{"dur":4,"text":"And, obviously, it's a real thing about\nlanguages that they have linear order."},"2561":{"dur":1,"text":"One can't deny it."},"2562":{"dur":4,"text":"But as I've defined dependency structures,\nyeah,"},"2567":{"dur":2,"text":"you can't actually recover\nthe order of words from them."},"2570":{"dur":5,"text":"Okay, one more slide before\nwe get to the intermission."},"2576":{"dur":2,"text":"Yeah, so in the second half of the class,"},"2578":{"dur":5,"text":"I'm gonna tell you about\na method of dependency parsing."},"2584":{"dur":4,"text":"I just wanted to say, very quickly,\nthere are a whole bunch"},"2588":{"dur":3,"text":"of ways that people have gone\nabout doing dependency parsing."},"2592":{"dur":5,"text":"So one very prominent way of doing\ndependency parsing is using dynamic"},"2597":{"dur":1,"text":"programming methods,"},"2598":{"dur":4,"text":"which is normally what people have\nused for constituency grammars."},"2602":{"dur":4,"text":"A second way of doing it\nis to use graph algorithms."},"2607":{"dur":4,"text":"So a common way of doing dependency\nparsing, you're using MST algorithms,"},"2612":{"dur":1,"text":"Minimum Spanning Tree algorithms."},"2613":{"dur":3,"text":"And that's actually a very\nsuccessful way of doing it."},"2616":{"dur":3,"text":"You can view it as kind of\na constraint satisfaction problem."},"2620":{"dur":2,"text":"And people have done that."},"2623":{"dur":4,"text":"But the way we're gonna look at it is\nthis fourth way which is, these days,"},"2627":{"dur":4,"text":"most commonly called transition\nbased-parsing, though when it was first"},"2631":{"dur":5,"text":"introduced, it was quite often called\ndeterministic dependency parsing."},"2636":{"dur":5,"text":"And the idea of this is that\nwe're kind of greedily going to"},"2641":{"dur":6,"text":"decide which word each\nword is a dependent of,"},"2647":{"dur":3,"text":"guided by having a machine\nlearning classifier."},"2651":{"dur":3,"text":"And this is the method you're\ngoing to use for assignment two."},"2654":{"dur":2,"text":"So one way of thinking about this is, so"},"2657":{"dur":4,"text":"far in this class,\nwe only have two hammers."},"2662":{"dur":5,"text":"One hammer we have is word vectors, and\nyou can do a lot with word vectors."},"2667":{"dur":4,"text":"And the other hammer we have is\nhow to build a classifier as"},"2672":{"dur":4,"text":"a feedforward neural network\nwith a softmax on top so"},"2676":{"dur":3,"text":"it classifies between two various classes."},"2680":{"dur":2,"text":"And it turns out that if\nthose are your two hammers,"},"2683":{"dur":3,"text":"you can do dependency parsing this way and\nit works really well."},"2687":{"dur":4,"text":"And so, therefore, that's a great\napproach for using in assignment two."},"2691":{"dur":2,"text":"And it's not just a great approach for\nassignment two."},"2694":{"dur":5,"text":"Actually method four is the dominant\nway these days of doing"},"2699":{"dur":7,"text":"dependency parsing because it has\nextremely good properties of scalability."},"2707":{"dur":5,"text":"That greedy word there is a way of\nsaying this is a linear time algorithm,"},"2712":{"dur":2,"text":"which none of the other methods are."},"2715":{"dur":3,"text":"So in the modern world\nof web-scale parsing,"},"2718":{"dur":3,"text":"it's sort of become most\npeople's favorite method."},"2721":{"dur":2,"text":"So I'll say more about that very soon."},"2724":{"dur":1,"text":"But before we get to that,"},"2725":{"dur":5,"text":"we have Ajay doing our research spotlight\nwith one last look back at word vectors."},"2733":{"dur":1,"text":">> Am I on?\nOkay, awesome, so"},"2735":{"dur":3,"text":"let's take a break from\ndependency parsing and"},"2738":{"dur":5,"text":"talk about something we should\nknow a lot about, word embeddings."},"2743":{"dur":5,"text":"So for today's research highlight, we're\ngonna be talking about a paper titled,"},"2748":{"dur":5,"text":"Improving Distributional Similarity with\nLessons Learned from Word Embeddings."},"2754":{"dur":1,"text":"And it's authored by Levy, et al."},"2758":{"dur":5,"text":"So in class we've learned two major\nparadigms for generating word vectors."},"2763":{"dur":3,"text":"We've learned count-based\ndistributional models,"},"2767":{"dur":6,"text":"which essentially utilize a co-occurrence\nmatrix to produce your word vectors."},"2773":{"dur":3,"text":"And we've learned SVD,\nwhich is Singular Value Decomposition."},"2777":{"dur":2,"text":"And we haven't really talked about PPMI."},"2780":{"dur":1,"text":"But, in effect,"},"2781":{"dur":5,"text":"it still uses that co-occurrence matrix to\nproduce sparse vector encodings for words."},"2786":{"dur":2,"text":"We've also learned neural\nnetwork-based models,"},"2788":{"dur":2,"text":"which you all should have\nlots of experience with now."},"2791":{"dur":5,"text":"And, specifically, we've talked\nabout Skip-Gram Negative Sampling,"},"2797":{"dur":2,"text":"as well as CBOW methods."},"2799":{"dur":3,"text":"And GloVe is also a neural\nnetwork-based model."},"2803":{"dur":4,"text":"And the conventional wisdom is that\nneural network-based models are superior"},"2808":{"dur":1,"text":"to count-based models."},"2811":{"dur":4,"text":"However, Levy et al proposed\nthat hyperparameters and"},"2816":{"dur":3,"text":"system design choices are more important,\nnot the embedding algorithms themselves."},"2820":{"dur":3,"text":"So they're challenging\nthis popular convention."},"2824":{"dur":5,"text":"And so, essentially,\nwhat they do in their paper is"},"2829":{"dur":5,"text":"propose a slew of hyperparameters that,\nwhen implemented and tuned over,"},"2835":{"dur":5,"text":"the count-based distributional models\npretty much approach the performance"},"2841":{"dur":3,"text":"of neural network-based models,\nto the point where there's no consistent,"},"2844":{"dur":2,"text":"better choice across the different\ntasks that they tried."},"2849":{"dur":2,"text":"And a lot of these\nhyperparameters were actually"},"2852":{"dur":5,"text":"inspired by these neural network-based\nmodels such as Skip-Gram."},"2857":{"dur":3,"text":"So if you recall, which you all\nshould be very familiar with this,"},"2860":{"dur":3,"text":"we have two hyperparameters in Skip-Gram."},"2864":{"dur":3,"text":"We have the number of negative samples\nthat we're sampling, as well as"},"2867":{"dur":3,"text":"the unigram distributions smoothing\nexponent, which we fixed at 3 over 4."},"2871":{"dur":3,"text":"But it can be thought of as\nmore of a system design choice."},"2877":{"dur":3,"text":"And these can also be transferred\nover to the account based variants."},"2880":{"dur":2,"text":"And I'll go over those very quickly."},"2883":{"dur":4,"text":"So the single hyper\nparameter that Levy et al.,"},"2887":{"dur":4,"text":"proposed that had the biggest\nimpact in performance was"},"2892":{"dur":4,"text":"Context Distribution Smoothing\nwhich is analogous to"},"2896":{"dur":5,"text":"the unigram distribution\nsmoothing constant 3 over 4 here."},"2903":{"dur":4,"text":"And in effect they both\nachieved the same goal which is"},"2907":{"dur":5,"text":"to sort of smooth out your distribution\nsuch that you're penalizing rare words."},"2913":{"dur":5,"text":"And using this hyperparameter\nwhich interestingly enough,"},"2918":{"dur":4,"text":"the optimal alpha they\nfound was exactly 3 over 4,"},"2923":{"dur":5,"text":"which is the same as\nthe Skip-Gram Unigram smoothing exponent."},"2928":{"dur":3,"text":"They were able to increase performance\nby an average of three points across"},"2932":{"dur":2,"text":"tasks on average which\nis pretty interesting."},"2936":{"dur":1,"text":"And they also propose Shifted PMI,"},"2938":{"dur":2,"text":"which I'm not gonna get\ninto the details of this."},"2940":{"dur":2,"text":"But this is analogous to\nthe negative sampling,"},"2945":{"dur":2,"text":"choosing the number of\nnegative samples in Skip-Gram."},"2950":{"dur":5,"text":"And they've also proposed a total\nof eight hyperparameters in total."},"2956":{"dur":4,"text":"And we've described one of them which\nis the Context Distribution Smoothing."},"2962":{"dur":1,"text":"So here's the results."},"2964":{"dur":4,"text":"And this is a lot of data, and if you're\nconfused, that's actually the conclusion"},"2969":{"dur":6,"text":"that I want you to arrive at because\nclearly there's no trend here."},"2975":{"dur":5,"text":"So, what the authors did was\ntake all four methods, tried"},"2981":{"dur":5,"text":"three different windows, and then test\nall the models across a different task."},"2986":{"dur":3,"text":"And those are split up into word\nsimilarity and analogy task."},"2990":{"dur":4,"text":"And all of these methods are tuned"},"2994":{"dur":3,"text":"to find the best hyperparameters\nto optimize for the performance."},"2997":{"dur":5,"text":"And the best models are bolded, and as you\ncan see there's no consistent best model."},"3003":{"dur":6,"text":"So, in effect, they're challenging\nthe popular convention that"},"3009":{"dur":6,"text":"neural network-based models\nare superior to the count-based models."},"3015":{"dur":2,"text":"However, there's a few\nthings to note here."},"3018":{"dur":5,"text":"Number one, adding hyperparameters\nis never a great thing because"},"3023":{"dur":5,"text":"now you have to train those\nhyperparameters which takes time."},"3028":{"dur":4,"text":"Number two,\nwe still have the issues with count-based"},"3033":{"dur":6,"text":"distributional models specifically\nwith respect to the computational"},"3040":{"dur":5,"text":"issues of storing PPMI counts\nas well as performing SVD."},"3052":{"dur":4,"text":"So the key takeaways here is that the\npaper challenges the conventional wisdom"},"3056":{"dur":4,"text":"that neutral network-based models are in\nfact superior to count-based models."},"3062":{"dur":3,"text":"Number two,\nwhile model design is important,"},"3065":{"dur":3,"text":"hyperparameters are also key for\nachieving good results."},"3069":{"dur":3,"text":"So this implies specifically to\nyou guys especially if you're"},"3073":{"dur":2,"text":"doing a project instead\nof assignment four."},"3076":{"dur":5,"text":"You might implement the model but\nthat might only take you half way there."},"3081":{"dur":4,"text":"Some models to find your optimal\nhyperparameters might take days or"},"3086":{"dur":1,"text":"even weeks to find."},"3087":{"dur":1,"text":"So don't discount their importance."},"3089":{"dur":5,"text":"And, finally, my personal interest within\nML is in deep representation learning."},"3095":{"dur":4,"text":"And this paper specifically excites\nme because I think it sort of"},"3099":{"dur":4,"text":"displays that there's still lots\nof work to be done in the field."},"3104":{"dur":4,"text":"And so, the final takeaway\nis challenge the status quo."},"3109":{"dur":1,"text":"Thank you."},"3110":{"dur":5,"text":">> [APPLAUSE]"},"3115":{"dur":2,"text":">> Okay, thanks a lot Ajay."},"3118":{"dur":4,"text":"Okay and so\nnow we're back to learning about how to"},"3123":{"dur":4,"text":"build a transition based\ndependency parser."},"3127":{"dur":5,"text":"So, maybe in 103 or compilers class,\nformal languages class,"},"3133":{"dur":4,"text":"there's this notion of\nshift reduced parsing."},"3137":{"dur":2,"text":"How many of you have seen shift\nreduced parsing somewhere?"},"3141":{"dur":2,"text":"A minority it turns out."},"3143":{"dur":4,"text":"They just don't teach formal languages the\nway they used to in the 1960s in computer"},"3148":{"dur":1,"text":"science anymore."},"3149":{"dur":4,"text":">> [LAUGH]\n>> You'll just have to spend more time"},"3153":{"dur":0,"text":"with Jeff Ullman."},"3154":{"dur":2,"text":"Okay, well I won't assume that\nyou've all seen that before."},"3157":{"dur":8,"text":"Okay, essentially what\nwe're going to have is,"},"3166":{"dur":5,"text":"I'll just skip these two slides and\ngo straight to the pictures."},"3171":{"dur":1,"text":"Because, they will be\nmuch more understandable."},"3173":{"dur":4,"text":"But before I go on, I'll just\nmention the picture on this page,"},"3177":{"dur":2,"text":"that's a picture of Joakim Nivre."},"3179":{"dur":3,"text":"So Joakim Nivre is a computational\nlinguist in Uppsala,"},"3183":{"dur":5,"text":"Sweden who pioneered this approach of\ntransition based dependency parsing."},"3188":{"dur":2,"text":"He's one of my favorite\ncomputational linguists."},"3191":{"dur":4,"text":"I mean he was also an example,\ngoing along with what Ajay said,"},"3195":{"dur":3,"text":"of sort of doing something unpopular and"},"3199":{"dur":3,"text":"out of the mainstream and\nproving that you can get it to work well."},"3203":{"dur":5,"text":"So at an age when everyone else was trying\nto build sort of fancy dynamic program"},"3208":{"dur":5,"text":"parsers Joakim said no,no, what I'm\ngonna do, is I'm just gonna take each"},"3213":{"dur":5,"text":"successive word and have a straight\nclassifier that says what to do with that."},"3218":{"dur":4,"text":"And go onto the next word completely\ngreedy cuz maybe that's kinda like what"},"3222":{"dur":3,"text":"humans do with incremental\nsentence processing and"},"3225":{"dur":3,"text":"I'm gonna see how well\nI can make that work."},"3228":{"dur":2,"text":"And it turned out you can\nmake it work really well."},"3231":{"dur":4,"text":"So and then sort of transition based\nparsing has grown to this sort of"},"3236":{"dur":2,"text":"really widespread dominant\nway of doing parsing."},"3239":{"dur":5,"text":"So it's good to find something different\nto do If everyone else is doing something,"},"3245":{"dur":2,"text":"it's good to think of something else\nthat might be promising that you"},"3248":{"dur":3,"text":"And I also like Joakim because he's\nactually another person that's really"},"3252":{"dur":1,"text":"interested in human languages and"},"3254":{"dur":3,"text":"linguistics which actually seems\nto be a minority of the field of"},"3258":{"dur":2,"text":"natural language processing\nwhen it comes down to it."},"3260":{"dur":4,"text":"Okay, so here's some more formalism,\nbut I'll skip that as well and"},"3265":{"dur":3,"text":"show it to you afterwards and\nI'll give you the idea of what"},"3269":{"dur":5,"text":"an arc-standard transition-based\ndependency parser does."},"3276":{"dur":5,"text":"So what we're gonna do is were going\nto have a sentence we want to parse,"},"3281":{"dur":5,"text":"I ate fish, and so we've got some rules\nfor parsing which is the transition"},"3286":{"dur":4,"text":"scheme which is written so\nsmall you can't possibly read it."},"3291":{"dur":1,"text":"And this is how we start."},"3293":{"dur":3,"text":"So we have two things,\nwe have a stack, and"},"3296":{"dur":4,"text":"a stack is kinda got the gray\ncartouche around that."},"3300":{"dur":4,"text":"And we start off parsing any\nsentence by putting it on the stack,"},"3305":{"dur":3,"text":"one thing, which is our root symbol."},"3308":{"dur":5,"text":"Okay and\nthe stack has its top towards the right."},"3314":{"dur":3,"text":"And then we have this other thing\nwhich gets referred to as the buffer."},"3318":{"dur":2,"text":"And the buffer is the orange cartouche and"},"3320":{"dur":3,"text":"the buffer is the sentence\nthat we've got to deal with."},"3324":{"dur":5,"text":"And so the thing that we regard as the top\nof the buffer is the thing to the left,"},"3329":{"dur":2,"text":"because we're gonna be taking\noff excessive words right?"},"3332":{"dur":5,"text":"So the top of both of them is sort of at\nthat intersection point between them."},"3337":{"dur":4,"text":"Okay and so,\nto do parsing under this transition-based"},"3342":{"dur":4,"text":"scheme there are three\noperations that we can perform."},"3347":{"dur":6,"text":"We can perform, they're called Shift,\nLeft-Arc and Right-Arc."},"3353":{"dur":3,"text":"So the first one that we're\ngonna do is shift operation."},"3357":{"dur":2,"text":"So shift is really easy."},"3359":{"dur":5,"text":"All we do when we do a shift is we take\nthe word that's on the top of the buffer"},"3364":{"dur":1,"text":"and put it on the top of the stack."},"3367":{"dur":1,"text":"And then we can shift again and"},"3369":{"dur":5,"text":"we take the word that's on the top of the\nbuffer and put it on the top of the stack."},"3374":{"dur":3,"text":"Remember the stack,\nthe top is to the right."},"3377":{"dur":2,"text":"The buffer, the top is to the left."},"3380":{"dur":1,"text":"That's pretty easy, right?"},"3382":{"dur":5,"text":"Okay, so there are two other\noperations left in this arc-standard"},"3388":{"dur":5,"text":"transition scheme which were left arc and\nright arc."},"3393":{"dur":5,"text":"So what left arc and right arc\nare gonna do is we're going to make"},"3398":{"dur":4,"text":"attachment decisions by adding\na word as the dependent,"},"3402":{"dur":2,"text":"either to the left or to the right."},"3405":{"dur":4,"text":"Okay, so what we do for left arc is"},"3410":{"dur":5,"text":"on the stack we say that\nthe second to the top"},"3415":{"dur":5,"text":"of the stack is a dependent of\nthe thing that's the top of the stack."},"3420":{"dur":6,"text":"So, I is a dependent of ate, and we remove\nthat second top thing from the stack."},"3426":{"dur":2,"text":"So that's a left arc operation."},"3429":{"dur":3,"text":"And so now we've got a stack\nwith just [root] ate on it."},"3433":{"dur":4,"text":"But we collect up our decisions, so we've\nmade a decision that I is a dependent of"},"3438":{"dur":4,"text":"ate, and that's that said A that I am\nwriting in small print off to the right."},"3442":{"dur":3,"text":"Okay, so\nwe still had our buffer with fish on it."},"3446":{"dur":7,"text":"So the next thing we're gonna do is\nshift again and put fish on the stack."},"3453":{"dur":2,"text":"And so at that point our buffer is empty,"},"3455":{"dur":2,"text":"we've moved every word on to\nthe stack in our sentence."},"3458":{"dur":3,"text":"And we have on it root ate fish, okay."},"3461":{"dur":4,"text":"So then the third operation we have"},"3466":{"dur":4,"text":"is right arc, and right arc is\njust the opposite of left arc."},"3470":{"dur":5,"text":"So for the right arc operation, we say\nthe thing that's on the top of the stack"},"3476":{"dur":4,"text":"should be made a dependent of the thing\nthat's second to top on the stack."},"3480":{"dur":4,"text":"We remove it from the stack and\nwe add an arc saying that."},"3485":{"dur":3,"text":"So we right arc, so"},"3488":{"dur":5,"text":"we say fish is a dependent of ate,\nand we remove fish from the stack."},"3493":{"dur":5,"text":"We add a new dependency saying\nthat fish is a dependent of ate."},"3499":{"dur":4,"text":"And then we right arc one more time so"},"3504":{"dur":4,"text":"then we're saying that ate is\nthe dependent of the root."},"3508":{"dur":4,"text":"So we pop it off the stack and we're\njust left with root on the stack, and"},"3513":{"dur":5,"text":"we've got one new dependency saying\nthat ate is a dependent of root."},"3518":{"dur":4,"text":"So at this point, And\nI'll just mention, right,"},"3523":{"dur":4,"text":"in reality there's,\nI left out writing the buffer in a few of"},"3527":{"dur":3,"text":"those examples there just because it was\ngetting pretty crowded on the slide."},"3531":{"dur":3,"text":"But really the buffer is always there,\nright, it's not that the buffer"},"3535":{"dur":4,"text":"disappeared and came back again,\nit's just I didn't always draw it."},"3539":{"dur":1,"text":"So but in our end state,"},"3541":{"dur":4,"text":"we've got one thing on the stack,\nand we've got nothing in the buffer."},"3546":{"dur":2,"text":"And that's the good state\nthat we want to be in if we"},"3548":{"dur":2,"text":"finish parsing our sentence correctly."},"3551":{"dur":3,"text":"And so we say, okay,\nwe're in the finished state and we stop."},"3554":{"dur":4,"text":"And so that is almost all there"},"3559":{"dur":5,"text":"is to arc-standard\ntransition based parsing."},"3564":{"dur":4,"text":"So if just sort of go back to\nthese slides that I skipped over."},"3570":{"dur":4,"text":"Right, so we have a stack and our buffer,\nand then on the side we have a set of"},"3575":{"dur":5,"text":"dependency arcs A which starts\noff empty and we add things to."},"3581":{"dur":4,"text":"And we have this sort of set of actions\nwhich are kind of legal moves that we can"},"3585":{"dur":4,"text":"make for parsing, and so\nthis was how things are."},"3590":{"dur":6,"text":"So we have a start condition, ROOT on the\nstack, buffer is the sentence, no arcs."},"3596":{"dur":3,"text":"We have the three operations\nthat we can perform."},"3600":{"dur":3,"text":"Here I've tried to write\nthem out formally, so"},"3603":{"dur":5,"text":"the sort of vertical bar is sort of\nappends an element to a list operation."},"3609":{"dur":6,"text":"So this is sort of having wi as the first\nword on the buffer, it's written"},"3616":{"dur":3,"text":"the opposite way around for the stack\nbecause the head's on the other side."},"3620":{"dur":4,"text":"And so we can sort of do this shift\noperation of moving a word onto the stack"},"3624":{"dur":4,"text":"and these two arc operations\nadd a new dependency."},"3629":{"dur":5,"text":"And then removing one word from the stack\nand our ending condition is one"},"3634":{"dur":4,"text":"thing on the stack which will\nbe the root and an empty buffer."},"3639":{"dur":3,"text":"And so\nthat's sort of the formal operations."},"3642":{"dur":4,"text":"So the idea of transition based\nparsing is that you have this sort of"},"3647":{"dur":5,"text":"set of legal moves to parse a sentence\nin sort of a shift reduced way."},"3652":{"dur":3,"text":"I mean this one I referred to as\narc-standard cuz it turns out there"},"3655":{"dur":3,"text":"are different ways you can define\nyour sets of dependencies."},"3659":{"dur":3,"text":"But this is the simplest one,\nthe one we'll use for the assignment, and"},"3662":{"dur":1,"text":"one that works pretty well."},"3664":{"dur":0,"text":"Question?"},"3666":{"dur":1,"text":"I was gonna get to that."},"3668":{"dur":2,"text":"So I've told you the whole\nthing except for"},"3671":{"dur":4,"text":"one thing which is this just gives\nyou a set of possible moves."},"3675":{"dur":3,"text":"It doesn't say which\nmove you should do when."},"3678":{"dur":3,"text":"And so\nthat's the remaining thing that's left."},"3682":{"dur":1,"text":"And I have a slide on that."},"3684":{"dur":5,"text":"Okay, so the only thing that's left\nis to say, gee, at any point in time,"},"3690":{"dur":5,"text":"like we were here, at any point in time,\nyou're in some configuration, right."},"3696":{"dur":3,"text":"You've got certain things on there,\ncertain things in the stacks,"},"3700":{"dur":4,"text":"certain things in your buffer, you have\nsome set of arcs that you've already made."},"3705":{"dur":5,"text":"And which one of these\noperations do I do next?"},"3710":{"dur":1,"text":"And so that's the final thing."},"3712":{"dur":3,"text":"And the way that you do that,\nthat Nivre proposed,"},"3716":{"dur":5,"text":"is well what we should do is just\nbuild a machine learning classifier."},"3722":{"dur":3,"text":"Since we have a tree bank\nwith parses of sentences,"},"3726":{"dur":3,"text":"we can use those parses\nof sentences to see"},"3729":{"dur":5,"text":"which sequence of operations would\ngive the correct parse of a sentence."},"3734":{"dur":2,"text":"I am not actually gonna go\nthrough that right now."},"3737":{"dur":3,"text":"But if you have the structure\nof a sentence in a tree bank,"},"3740":{"dur":5,"text":"you can sort of work out deterministically\nthe sequence of shifts and"},"3745":{"dur":2,"text":"reducers that you need\nto get that structure."},"3748":{"dur":4,"text":"And it's indeed unique, right, that for\neach tree structure there's a sequence of"},"3752":{"dur":3,"text":"shifts and left arcs and right arcs\nthat will give you the right structure."},"3756":{"dur":3,"text":"So you take the tree, you read off\nthe correct operation sequence, and"},"3760":{"dur":3,"text":"therefore you've got a supervised\nclassification problem."},"3763":{"dur":4,"text":"Say in this scenario, what you\nshould do next is you should shift,"},"3768":{"dur":4,"text":"and so you're then building\na classified to try to predict that."},"3772":{"dur":7,"text":"So in the early work that started off\nwith Nivre and others in the mid 2000s,"},"3780":{"dur":5,"text":"this was being done with conventional\nmachine learning classifiers."},"3785":{"dur":5,"text":"So maybe an SVM, maybe a perceptron,\na kind of maxent \/ soft max classifiers,"},"3791":{"dur":5,"text":"various things, but sort of some\nclassified that you're gonna use."},"3796":{"dur":5,"text":"So if you're just deciding between\nthe operations, shift left arc,"},"3801":{"dur":3,"text":"right arc,\nyou have got at most three choices."},"3804":{"dur":3,"text":"Occasionally you have less because if\nthere's nothing left on the buffer"},"3808":{"dur":3,"text":"you can't shift anymore, so then you'd\nonly have two choices left maybe."},"3812":{"dur":4,"text":"But something I didn't mention\nwhen I was showing this is when"},"3817":{"dur":4,"text":"I added to the arc set, I didn't only\nsay that fish is an object of ate."},"3821":{"dur":3,"text":"I said,\nthe dependency is the object of ate."},"3825":{"dur":3,"text":"And so\nif you want to include dependency labels,"},"3829":{"dur":5,"text":"the standard way of doing that is you just\nhave sub types of left arc and right arc."},"3834":{"dur":2,"text":"So rather than having three choices."},"3837":{"dur":3,"text":"If you have a approximately 40\ndifferent dependency labels."},"3840":{"dur":4,"text":"As we will in assignment two and\nin universal dependencies."},"3844":{"dur":5,"text":"You actually end up with the space\nof 81 way classification."},"3850":{"dur":4,"text":"Because you have classes with\nnames like left arc as an object."},"3855":{"dur":4,"text":"Or left arc as an adjectival modifier."},"3859":{"dur":2,"text":"For the assignment,\nyou don't have to do that."},"3861":{"dur":3,"text":"For the assignment,\nwe're just doing un-type dependency trees."},"3865":{"dur":3,"text":"Which sort of makes it a bit more\nscalable and easy for you guys."},"3868":{"dur":4,"text":"So it's only sort of a three way\ndecision is all you're doing."},"3872":{"dur":4,"text":"In most real applications, it's really\nhandy to have those dependency labels."},"3877":{"dur":1,"text":"Okay."},"3878":{"dur":3,"text":"And then what do we use as features?"},"3881":{"dur":3,"text":"Well, in the traditional model, you sort\nof looked at all the words around you."},"3885":{"dur":2,"text":"You saw what word was on\nthe top of the stack."},"3888":{"dur":2,"text":"What was the part of speech of that word?"},"3890":{"dur":1,"text":"What was the first word in the buffer?"},"3891":{"dur":1,"text":"What was its parts of speech?"},"3893":{"dur":3,"text":"Maybe it's good to look at the thing\nbeneath the top of the stack."},"3897":{"dur":2,"text":"And what word and part of speech it is."},"3900":{"dur":1,"text":"And further ahead in the buffers."},"3901":{"dur":1,"text":"So you're looking at a bunch of words."},"3903":{"dur":3,"text":"You're looking at some attributes of those\nwords, such as their part of speech."},"3907":{"dur":3,"text":"And that was giving you\na bunch of features."},"3910":{"dur":3,"text":"Which are the same kind of classic,\ncategorical,"},"3913":{"dur":3,"text":"sparse features of\ntraditional machine learning."},"3917":{"dur":3,"text":"And people were building\nclassifiers over that."},"3920":{"dur":1,"text":"Yeah, Question?"},"3927":{"dur":4,"text":"So yeah, the question is are most\ntreebanks annotated with part of speech?"},"3932":{"dur":1,"text":"And the answer is yes."},"3933":{"dur":1,"text":"Yeah, so I mean."},"3935":{"dur":3,"text":"We've barely talked about\npart of speech so far,"},"3938":{"dur":3,"text":"things like living things,\nnouns, and verbs."},"3941":{"dur":3,"text":"So the simplest way of doing\ndependency parsing as you're"},"3944":{"dur":4,"text":"first writing a part of speech, tag it or\nassign parts of speech to words."},"3948":{"dur":4,"text":"And then you're doing the syntactic\nstructure of dependency parsing over"},"3953":{"dur":3,"text":"a sequence of word,\npart of speech, tag pairs."},"3956":{"dur":3,"text":"Though there has been other work\nthat's done joint parsing and"},"3960":{"dur":2,"text":"part of speech tag\nprediction at the same time."},"3962":{"dur":3,"text":"Which actually has some advantages,\nbecause you can kind of explore."},"3966":{"dur":3,"text":"Since the two things are associated,"},"3969":{"dur":3,"text":"you can get some advantages\nfrom doing it jointly."},"3973":{"dur":6,"text":"Okay, on the simplest possible model,\nwhich was what Nivre started to explore."},"3979":{"dur":2,"text":"There was absolutely no search."},"3981":{"dur":2,"text":"You just took the next word,\nran your classifier."},"3983":{"dur":4,"text":"And said, that's the object of the verb,\nwhat's the next word?"},"3988":{"dur":1,"text":"Okay, that one's a noun modifier."},"3989":{"dur":3,"text":"And you went along and\njust made these decisions."},"3993":{"dur":4,"text":"Now you could obviously think,\ngee maybe if I did some more searching and"},"3997":{"dur":2,"text":"explore different alternatives\nI could do a bit better."},"3999":{"dur":1,"text":"And the answer is yes, you can."},"4001":{"dur":2,"text":"So there's a lot of work\nin dependency parsing."},"4004":{"dur":4,"text":"Which uses various forms of beam search\nwhere you explore different alternatives."},"4008":{"dur":3,"text":"And if you do that, it gets a ton slower."},"4011":{"dur":3,"text":"And gets a teeny bit better in\nterms of your performance results."},"4017":{"dur":5,"text":"Okay, but especially if you start from the\ngreediest end or you have a small beam."},"4022":{"dur":5,"text":"The secret of this type of parsing\nis it gives you extremely fast"},"4027":{"dur":1,"text":"linear time parsing."},"4029":{"dur":3,"text":"Because you're just going through\nyour corpus, no matter how big."},"4032":{"dur":1,"text":"And say, what's the next word?"},"4034":{"dur":1,"text":"Okay, attach it there."},"4035":{"dur":0,"text":"What's the next word?"},"4036":{"dur":1,"text":"Attach it there."},"4037":{"dur":2,"text":"And you keep on chugging through."},"4039":{"dur":4,"text":"So when people, like prominent search\nengines in suburbs south of us,"},"4044":{"dur":2,"text":"want to parse the entire\ncontent of the Web."},"4046":{"dur":3,"text":"They use a parser like this\nbecause it goes super fast."},"4051":{"dur":0,"text":"Okay."},"4053":{"dur":4,"text":"And so, what was shown was these\nkind of greedy dependencies parses."},"4058":{"dur":5,"text":"Their accuracy is slightly below\nthe best dependency parses possible."},"4064":{"dur":3,"text":"But their performance is\nactually kind of close to it."},"4068":{"dur":3,"text":"And the fact that they're sort of so\nfast and scalable."},"4071":{"dur":3,"text":"More than makes up for\ntheir teeny performance decrease."},"4075":{"dur":1,"text":"So that's kind of exciting."},"4079":{"dur":5,"text":"Okay, so then for the last few minutes\nI now want to get back to neural nets."},"4084":{"dur":1,"text":"Okay so where are we at the moment?"},"4086":{"dur":3,"text":"So at the moment we have a configuration\nwhere we have a stack and"},"4090":{"dur":2,"text":"a buffer and parts of speech or words."},"4092":{"dur":2,"text":"And as we start to build some structure."},"4099":{"dur":3,"text":"The things that we've taken off\nthe stack when we build arcs."},"4102":{"dur":3,"text":"We can kind of sort of think of them as\nstarting to build up a tree as we go."},"4106":{"dur":2,"text":"As I've indicated with that example below."},"4109":{"dur":4,"text":"So, the classic way of doing that\nis you could then say, okay,"},"4113":{"dur":1,"text":"well we've got all of these features."},"4115":{"dur":4,"text":"Like top of stack is word good,\nor top of stack is word bad,"},"4119":{"dur":2,"text":"or top of stack is word easy."},"4121":{"dur":2,"text":"Top of stack's part of\nspeech as adjective."},"4123":{"dur":1,"text":"Top of stack's word is noun."},"4125":{"dur":1,"text":"And if you start doing that."},"4127":{"dur":5,"text":"When you've got a combination of\npositions and words and parts of speech."},"4132":{"dur":4,"text":"You very quickly find that the number\nof features you have in your model"},"4137":{"dur":2,"text":"is sort of order ten million."},"4139":{"dur":1,"text":"Extremely, extremely large."},"4141":{"dur":4,"text":"But you know that's precisely how these\nkinds of parses were standardly made"},"4146":{"dur":1,"text":"in the 2000s."},"4148":{"dur":5,"text":"So you're building these huge machine\nlearning classifiers over sparse features."},"4153":{"dur":3,"text":"And commonly you even had features\nthat were conjunctions of things."},"4156":{"dur":1,"text":"As that helped you predict better."},"4158":{"dur":4,"text":"So you had features like the second\nword on the stack is has."},"4162":{"dur":2,"text":"And its tag is present tense verb."},"4165":{"dur":1,"text":"And the top word on the stack is good."},"4167":{"dur":1,"text":"And things like that would be one feature."},"4168":{"dur":4,"text":"And that's where you easily get\ninto the ten million plus features."},"4173":{"dur":3,"text":"So even doing this already\nworked quite well."},"4177":{"dur":4,"text":"But the starting point\nfrom going on is saying,"},"4181":{"dur":3,"text":"well it didn't work completely great."},"4186":{"dur":1,"text":"That we wanna do better than that."},"4188":{"dur":3,"text":"And we'll go on and\ndo that in just a minute."},"4192":{"dur":4,"text":"But before I do that, I should mention\njust the evaluation of dependency parsing."},"4196":{"dur":3,"text":"Evaluation of dependency\nparsing is actually very easy."},"4200":{"dur":4,"text":"Cuz since for each word we're saying,\nwhat is it a dependent of."},"4204":{"dur":4,"text":"That we're sort of making choices of\nwhat each word is a dependent of."},"4209":{"dur":1,"text":"And then there's a right answer."},"4210":{"dur":3,"text":"Which we get from our tree bank,\nwhich is the gold thing."},"4214":{"dur":4,"text":"We're sort of, essentially,\njust counting how often we are right."},"4218":{"dur":1,"text":"Which is an accuracy measure."},"4220":{"dur":3,"text":"And so, there are two ways\nthat that's commonly done."},"4224":{"dur":5,"text":"One way is that we just look at\nthe arrows and ignore the labels."},"4229":{"dur":4,"text":"And that's often referred to as\nthe UAS measure, unlabeled accuracy."},"4234":{"dur":3,"text":"Or we can also pay\nattention to the labels."},"4237":{"dur":2,"text":"And say you're only right if\nyou also get the label right."},"4240":{"dur":4,"text":"And that's referred to as the LAS,\nthe labelled accuracy score."},"4244":{"dur":0,"text":"Yes?"},"4253":{"dur":4,"text":"So the question is, don't you have\nwaterfall effects if you get something"},"4258":{"dur":4,"text":"wrong high up that'll destroy\neverything else further down?"},"4262":{"dur":2,"text":"You do get some of that."},"4264":{"dur":6,"text":"Because, yes, one decision will\nprevent some other decisions."},"4270":{"dur":1,"text":"It's typically not so bad."},"4272":{"dur":3,"text":"Because even if you mis-attach something\nlike a prepositional phrase attachment."},"4276":{"dur":3,"text":"You can still get right all of\nthe attachments inside noun"},"4279":{"dur":1,"text":"phrase that's inside that\nprepositional phrase."},"4281":{"dur":1,"text":"So it's not so bad."},"4283":{"dur":3,"text":"And I mean actually dependency parsing"},"4286":{"dur":4,"text":"evaluation suffers much less\nbadly from waterfall effects."},"4290":{"dur":4,"text":"Than doing CFG parsing which\nis worse in that respect."},"4295":{"dur":0,"text":"So it's not so bad."},"4298":{"dur":5,"text":"Okay, I had one slide there\nwhich I think I should skip."},"4309":{"dur":5,"text":"Okay I'll skip on to Neural ones."},"4314":{"dur":6,"text":"Okay, so, people could build quite good"},"4320":{"dur":5,"text":"machine learning dependency parsers on\nthese kind of categorical features."},"4325":{"dur":3,"text":"But nevertheless,\nthere was a problems of doing that."},"4329":{"dur":5,"text":"So, Problem #1 is the features\nwere just super sparse."},"4334":{"dur":4,"text":"That if you typically might have a tree\nbank that is an order about a million"},"4339":{"dur":4,"text":"words, and if you're then trying\nto train 15 million features,"},"4344":{"dur":3,"text":"which are kinda different\ncombinations of configurations."},"4347":{"dur":4,"text":"Not surprisingly, a lot of those\nconfigurations, you've seen once or twice."},"4351":{"dur":4,"text":"So, you just don't have any\naccurate model of what happens in"},"4355":{"dur":1,"text":"different configurations."},"4356":{"dur":3,"text":"You just kind of getting these\nweak feature weights, and"},"4360":{"dur":2,"text":"crossing your fingers and\nhoping for the best."},"4362":{"dur":2,"text":"Now, it turns out that\nmodern machine learning,"},"4364":{"dur":1,"text":"crossing your fingers works pretty well."},"4366":{"dur":2,"text":"But, nevertheless,\nyou're suffering a lot from sparsity."},"4370":{"dur":3,"text":"Okay, the second problem is,\nyou also have an incompleteness problem,"},"4374":{"dur":4,"text":"because lots of configurations\nyou'll see it run time, will be"},"4378":{"dur":4,"text":"different configurations that you just\nnever happened to see the configuration."},"4382":{"dur":2,"text":"When exquisite was the second\nword on the stack, and"},"4385":{"dur":5,"text":"the top word of the stack,\nspeech, or something."},"4390":{"dur":3,"text":"Any kind of word pale,\nI've only seen a small fraction of them."},"4393":{"dur":2,"text":"Lot's of things you\ndon't have features for."},"4396":{"dur":2,"text":"The third one is a little bit surprising."},"4398":{"dur":5,"text":"It turned out that when you looked at\nthese symbolic dependency parsers,"},"4403":{"dur":2,"text":"and you ask what made them slow."},"4405":{"dur":4,"text":"What made them slow\nwasn't running your SVM,"},"4409":{"dur":4,"text":"or your dot products in your logistic\nregression, or things like that."},"4414":{"dur":3,"text":"All of those things were really fast."},"4417":{"dur":4,"text":"What these parsers were ending up\nspending 95% of their time doing"},"4422":{"dur":4,"text":"is just computing these features, and\nlooking up their weights because you"},"4426":{"dur":4,"text":"had to sort of walk around the stack and\nthe buffer and sort of put together."},"4430":{"dur":3,"text":"A feature name, and then you had to\nlook it up in some big hash table to"},"4434":{"dur":2,"text":"get a feature number and a weight for it."},"4436":{"dur":2,"text":"And all the time is going on that, so"},"4439":{"dur":4,"text":"even though there are linear time,\nthat slowed them down a ton."},"4444":{"dur":4,"text":"So, in a paper in 2014 Danqi and"},"4448":{"dur":3,"text":"I developed this alternative\nwhere we said well,"},"4452":{"dur":4,"text":"let's just replace that all\nwith a neural net classifier."},"4456":{"dur":4,"text":"So that way, we can have a dense\ncompact feature representation and"},"4460":{"dur":1,"text":"do classification."},"4462":{"dur":4,"text":"So, rather than having our 10\nmillion categorical features,"},"4466":{"dur":4,"text":"we'll have a relatively modest\nnumber of dense features, and"},"4471":{"dur":2,"text":"we'll use that to decide our next action."},"4473":{"dur":3,"text":"And so, I want to spend the last\nfew minutes sort of showing"},"4477":{"dur":4,"text":"you how that works, and this is basically\nquestion two of the assignment."},"4482":{"dur":5,"text":"Okay, and basically, just to give you\nthe headline, this works really well."},"4488":{"dur":3,"text":"So, this was sort of the outcome\nthe first Parser MaltParser."},"4491":{"dur":2,"text":"So, it has pretty good UAS and"},"4494":{"dur":4,"text":"LAS and it had this advantage,\nthat it was really fast."},"4498":{"dur":4,"text":"When I said that's been the preferred\nmethod, I give you some contrast in gray."},"4502":{"dur":2,"text":"So, these are two of\nthe graph base parsers."},"4505":{"dur":4,"text":"So, the graph based parsers have\nbeen somewhat more accurate, but"},"4509":{"dur":2,"text":"they were kind of like two\norders in magnitude slower."},"4512":{"dur":3,"text":"So, if you didn't wanna parse much stuff\nthan you wanted accuracy, you'd use them."},"4515":{"dur":3,"text":"But if you wanted to parse the web,\nno one use them."},"4519":{"dur":4,"text":"And so,\nthe cool thing was that by doing this as"},"4523":{"dur":4,"text":"neural network dependency parser,\nwe were able to get much better accuracy."},"4528":{"dur":4,"text":"We were able to get accuracy that\nwas virtually as good as the best,"},"4532":{"dur":3,"text":"graph-based parsers at that time."},"4535":{"dur":4,"text":"And we were actually about to build\na parser that works significantly"},"4539":{"dur":4,"text":"faster than MaltParser, because of\nthe fact that it wasn't spending"},"4544":{"dur":2,"text":"all this time doing feature combination."},"4547":{"dur":2,"text":"It did have to do more\nvector matrix multiplies,"},"4549":{"dur":2,"text":"of course, but that's a different story."},"4552":{"dur":1,"text":"Okay, so how did we do it?"},"4554":{"dur":3,"text":"Well, so, our starting point was\nthe two tools we have, right?"},"4557":{"dur":1,"text":"Distributed representation."},"4559":{"dur":4,"text":"So, we're gonna use distributed\nrepresentations of words."},"4563":{"dur":4,"text":"So, similar words have close by vectors,\nwe've seen all of that."},"4567":{"dur":4,"text":"We're also going to use part, in our POS,\nwe use part-of-speech tags and"},"4572":{"dur":1,"text":"dependency labels."},"4573":{"dur":3,"text":"And we also learned distributed\nrepresentations for those."},"4577":{"dur":1,"text":"That's kind of a cool idea,"},"4579":{"dur":4,"text":"cuz it's also the case that parts of\nspeech some are more related than others."},"4583":{"dur":3,"text":"So, if you have a fine grain\npart-of-speech set where you have"},"4587":{"dur":3,"text":"plural nouns and proper names as\ndifferent parts of speech from nouns,"},"4591":{"dur":3,"text":"singular, you want to say\nthat they are close together."},"4594":{"dur":5,"text":"So, we also had distributed\nrepresentations for those."},"4599":{"dur":3,"text":"So now,\nwe have the same kind of configuration."},"4603":{"dur":4,"text":"We're gonna run exactly the same\ntransition based dependency parser."},"4607":{"dur":3,"text":"So, the configuration\nis no different at all."},"4611":{"dur":4,"text":"But what we're going to extract\nfrom it is the starting point."},"4615":{"dur":2,"text":"We extract certain positions,"},"4618":{"dur":4,"text":"just like Nivre's MaltParser but\nthen what we're gonna do is,"},"4622":{"dur":5,"text":"for each of these positions, like top of\nstack, second top of stack, buffer etc."},"4628":{"dur":4,"text":"We're then going to look then\nup in our bedding matrix, and"},"4632":{"dur":2,"text":"come up with a dense representation."},"4634":{"dur":3,"text":"So, you might be representing\nwords as sort of a 50 or"},"4637":{"dur":5,"text":"100 dimensional word vector representation\nof the kind that we've talked about."},"4643":{"dur":4,"text":"And so, we get those representations for\nthe different words as vectors, and"},"4647":{"dur":5,"text":"then what we're gonna do is just\nconcatenate those into one longer vector."},"4653":{"dur":3,"text":"So, any configuration of the parser\nis just being represented as"},"4657":{"dur":1,"text":"the longest vector."},"4658":{"dur":1,"text":"Well, perhaps not that long,"},"4659":{"dur":3,"text":"our vectors are sort of more\naround 1,000 not 10 million, yeah."},"4671":{"dur":2,"text":"Sorry, the dependency of, right,"},"4673":{"dur":3,"text":"the question is what's this\ndependency on feeding as an input?"},"4677":{"dur":2,"text":"The dependency I'm feeding\nhere as an import,"},"4679":{"dur":5,"text":"is when I previously built some arcs\nthat are in my arc set, I'm thinking"},"4685":{"dur":5,"text":"maybe it'll be useful to use those arcs as\nwell, to help predict the next decision."},"4690":{"dur":5,"text":"So, I'm using previous decisions on arcs\nas well to predict my follow-up decisions."},"4697":{"dur":1,"text":"Okay, so how do I do this?"},"4699":{"dur":4,"text":"And this is essentially what\nyou guys are gonna build."},"4704":{"dur":4,"text":"From my configuration,\nI take things out of it."},"4708":{"dur":4,"text":"I get there embedding representations, and"},"4713":{"dur":5,"text":"I can concatenate them together,\nand that's my input layer."},"4718":{"dur":4,"text":"I then run that through a hidden\nlayer Is a neural network,"},"4723":{"dur":4,"text":"feedforward neural network,\nI then have, from the hidden layer,"},"4728":{"dur":4,"text":"I've run that through a Softmax layer,\nand I get an output layer,"},"4732":{"dur":6,"text":"which is a probability distribution of my\ndifferent actions in the standard Softmax."},"4739":{"dur":3,"text":"And of course, I don't know what\nany of these numbers are gonna be."},"4742":{"dur":4,"text":"So, what I'm gonna be doing is I'm going\nto be using cross-entropy error, and"},"4746":{"dur":3,"text":"then back-propagating\ndown to learn things."},"4750":{"dur":5,"text":"And this is the whole model,\nand it learns super well,"},"4755":{"dur":4,"text":"and it produces a great dependency parser."},"4760":{"dur":3,"text":"I'm running a tiny bit short of time,\nbut let me just,"},"4763":{"dur":3,"text":"I think I'll have to rush this but\nI'll just say it."},"4767":{"dur":4,"text":"So, non-linearities, we've mentioned\nnon-linearities a little bit."},"4772":{"dur":3,"text":"We haven't said very much about them, and"},"4776":{"dur":4,"text":"I just want to say a couple more\nsentences on non-linearities."},"4780":{"dur":1,"text":"Something like a softmax."},"4781":{"dur":4,"text":"You can say that using a logistic function\ngives you a probability distribution."},"4786":{"dur":3,"text":"And that's kind of what you get in\ngeneralized linear models and statistics."},"4789":{"dur":2,"text":"In general, though, you want to say that."},"4792":{"dur":2,"text":"For neural networks."},"4794":{"dur":4,"text":"Having these non-linearities sort of\nlet's us do function approximation by"},"4799":{"dur":4,"text":"putting together these various\nneurons that have some non-linearity."},"4803":{"dur":4,"text":"We can sorta put together little\npieces like little wavelets to do"},"4807":{"dur":1,"text":"functional approximation."},"4809":{"dur":6,"text":"And the crucial thing to notice is you\nhave to use some non-linearity, right?"},"4815":{"dur":4,"text":"Deep networks are useless unless you put\nsomething in between the layers, right?"},"4820":{"dur":4,"text":"If you just have multiple linear layers\nthey could just be collapsed down into one"},"4824":{"dur":3,"text":"linear layer that the sort of\nproduct of linear transformations,"},"4828":{"dur":3,"text":"affine transformations is just\nan affine transformation."},"4832":{"dur":3,"text":"So deep networks without\nnon-linearities do nothing, okay?"},"4835":{"dur":4,"text":"And so we've talked about\nlogistic non-linearities."},"4840":{"dur":5,"text":"A second very commonly used\nnon-linearity is the tanh non-linearity,"},"4845":{"dur":4,"text":"which is tanh is normally\nwritten a bit differently."},"4850":{"dur":5,"text":"But if you sort of actually do your\nlittle bit of math, tanh is really"},"4855":{"dur":5,"text":"the same as a logistic, just sort of\nstretched and moved a little bit."},"4861":{"dur":5,"text":"And so tanh has the advantage that\nit's sort of symmetric around zero."},"4867":{"dur":4,"text":"And so that often works a lot better\nif you're putting it in the middle"},"4871":{"dur":1,"text":"of a new neural net."},"4872":{"dur":3,"text":"But in the example I showed you earlier,\nand for"},"4875":{"dur":4,"text":"what you guys will be using for\nthe dependency parser,"},"4879":{"dur":5,"text":"the suggestion to use for the first\nlayer is this linear rectifier layer."},"4885":{"dur":3,"text":"And linear rectifier\nnon-linearities are kind of freaky."},"4889":{"dur":2,"text":"They're not some interesting curve at all."},"4892":{"dur":4,"text":"Linear rectifiers just map things\nto zero if they're negative, and"},"4896":{"dur":2,"text":"then linear If they're positive."},"4899":{"dur":4,"text":"And when these were first introduced,\nI thought these were kind of crazy."},"4903":{"dur":3,"text":"I couldn't really believe that these\nwere gonna work and do anything useful."},"4907":{"dur":3,"text":"But they've turned out to\nbe super successful, so"},"4910":{"dur":5,"text":"in the middle of neural networks, these\ndays often the first thing you try and"},"4915":{"dur":6,"text":"often what works the best is what's called\nReLU, which is rectified linear unit."},"4922":{"dur":4,"text":"And they just sort of effectively\nhave these nice properties where"},"4926":{"dur":3,"text":"if you're on the positive\nside the slope is just 1."},"4930":{"dur":5,"text":"Which means that they transmit\nerror in the back propagation step"},"4935":{"dur":3,"text":"really well linearly back\ndown through the network."},"4939":{"dur":3,"text":"And if they go negative that gives enough\nof a non-linearity that they're just"},"4943":{"dur":2,"text":"sort of being turned off\nin certain configurations."},"4946":{"dur":4,"text":"And so these really non-linearities\nhave just been super, super successful."},"4950":{"dur":4,"text":"And that's what we suggest that\nyou use in the dependency parser."},"4956":{"dur":3,"text":"Okay, so I should stop now."},"4959":{"dur":4,"text":"But this kind of putting a neural\nnetwork into a transition based"},"4964":{"dur":3,"text":"parser was just a super successful idea."},"4967":{"dur":5,"text":"So if any of you heard about the Google\nannouncements of Parsey McParseface."},"4972":{"dur":4,"text":"And SyntaxNet for their kind of\nopen source dependency parser."},"4977":{"dur":3,"text":"It's essentially exactly\nthe same idea of this."},"4980":{"dur":3,"text":"Just done with a bigger scaled up,\nbetter optimized neural network."},"4984":{"dur":0,"text":"Okay, thanks a lot."}}