{"0":{"dur":1,"text":"ALAN WINFIELD: Thank\nyou very much indeed."},"4":{"dur":1,"text":"It's really great to be here."},"5":{"dur":2,"text":"And thank you so much\nfor the invitation."},"8":{"dur":2,"text":"So yes, robot intelligence."},"11":{"dur":3,"text":"So I've titled the lecture\n\"The Thinking Robot.\""},"14":{"dur":3,"text":"But of course, that\nimmediately begs the question,"},"18":{"dur":3,"text":"what on earth do we\nmean by thinking?"},"21":{"dur":1,"text":"Well we could, of\ncourse, spend the whole"},"23":{"dur":5,"text":"of the next hour debating\nwhat we mean by thinking."},"28":{"dur":3,"text":"But I should say that I'm\nparticularly interested in"},"32":{"dur":3,"text":"and will focus on\nembodied intelligence."},"36":{"dur":3,"text":"So in other words, the kind\nof intelligence that we have,"},"39":{"dur":3,"text":"that animals including humans\nhave, and that robots have."},"43":{"dur":4,"text":"So of course that\nslightly differentiates"},"48":{"dur":1,"text":"what I'm talking about from AI."},"50":{"dur":3,"text":"But I regard robotics as\na kind of subset of AI."},"55":{"dur":2,"text":"And of course one of the\nthings that we discovered"},"58":{"dur":2,"text":"in the last 60 odd years\nof artificial intelligence"},"61":{"dur":4,"text":"is that the things that we\nthought were really difficult"},"65":{"dur":2,"text":"actually are relatively easy."},"67":{"dur":4,"text":"Like playing chess, or\ngo, for that matter."},"72":{"dur":4,"text":"Whereas the things that\nwe originally thought"},"76":{"dur":4,"text":"were really easy, like making\na cup of tea, are really hard."},"81":{"dur":3,"text":"So it's kind of the opposite\nof what was expected."},"84":{"dur":3,"text":"So embodied intelligence\nin the real world"},"87":{"dur":4,"text":"is really very difficult indeed."},"92":{"dur":2,"text":"And that's what\nI'm interested in."},"94":{"dur":5,"text":"So this is the\noutline of the talk."},"100":{"dur":2,"text":"I'm going to talk initially\nabout intelligence"},"102":{"dur":2,"text":"and offer some\nideas, if you like,"},"105":{"dur":3,"text":"for a way of thinking\nabout intelligence"},"108":{"dur":2,"text":"and breaking it\ndown into categories"},"110":{"dur":3,"text":"or types of intelligence."},"114":{"dur":3,"text":"And then I'm going to choose\na particular one which"},"117":{"dur":4,"text":"I've been really working on\nthe last three or four years."},"121":{"dur":3,"text":"And it's what I call\na generic architecture"},"124":{"dur":4,"text":"for a functional imagination."},"128":{"dur":2,"text":"Or in short, robots\nwith internal models."},"131":{"dur":2,"text":"So that's really what\nI want to focus on."},"134":{"dur":1,"text":"Because I really\nwanted to show you"},"135":{"dur":1,"text":"some experimental\nwork that we've"},"136":{"dur":4,"text":"done the last couple\nof years in the lab."},"141":{"dur":2,"text":"I mean, I'm an\nelectronics engineer."},"143":{"dur":1,"text":"I'm an experimentalist."},"144":{"dur":4,"text":"And so doing experiments\nis really important for me."},"149":{"dur":4,"text":"So the first thing that\nwe ought to realize--"},"153":{"dur":5,"text":"I'm sure we do realize-- is that\nintelligence is not one thing"},"159":{"dur":3,"text":"that we all, animals,\nhumans, and robots"},"162":{"dur":2,"text":"have more or less of."},"164":{"dur":1,"text":"Absolutely not."},"165":{"dur":3,"text":"And you know, there are several\nways of breaking intelligence"},"168":{"dur":2,"text":"down into different\nkind of categories,"},"170":{"dur":2,"text":"if you like, of\nintelligence, different types"},"172":{"dur":1,"text":"of intelligence."},"174":{"dur":3,"text":"And here's one that I came\nup with in the last couple"},"177":{"dur":2,"text":"of years."},"179":{"dur":2,"text":"It's certainly not the only way\nof thinking about intelligence."},"182":{"dur":3,"text":"But this really breaks\nintelligence into four,"},"185":{"dur":4,"text":"if you like, types, four\nkinds of intelligence."},"190":{"dur":4,"text":"You could say kinds\nof minds, I guess."},"195":{"dur":2,"text":"The most fundamental\nis what we call"},"197":{"dur":1,"text":"morphological intelligence."},"199":{"dur":2,"text":"And that's the intelligence\nthat you get just"},"202":{"dur":3,"text":"from having a physical body."},"205":{"dur":2,"text":"And there are some\ninteresting questions"},"207":{"dur":3,"text":"about how you design\nmorphological intelligence."},"210":{"dur":4,"text":"You've probably all seen\npictures of or movies of robots"},"215":{"dur":2,"text":"that can walk, but in\nfact don't actually"},"218":{"dur":3,"text":"have any computing, any\ncomputation whatsoever."},"222":{"dur":4,"text":"In other words, the\nbehavior of walking"},"226":{"dur":2,"text":"is an emergent property\nof the mechanics,"},"229":{"dur":4,"text":"if you like, the springs and\nlevers and so on in the robot."},"234":{"dur":3,"text":"So that's an example of\nmorphological intelligence."},"237":{"dur":3,"text":"Individual intelligence is\nthe kind of intelligence"},"240":{"dur":4,"text":"that you get from\nlearning individually."},"245":{"dur":1,"text":"Social intelligence,\nI think, is really"},"246":{"dur":1,"text":"interesting and important."},"248":{"dur":1,"text":"And that's the\none that I'm going"},"249":{"dur":1,"text":"to focus on most in this talk."},"251":{"dur":1,"text":"Social intelligence\nis the intelligence"},"252":{"dur":5,"text":"that you get from learning\nsocially, from each other."},"258":{"dur":3,"text":"And of course, we\nare a social species."},"262":{"dur":1,"text":"And the other one\nwhich I've been"},"263":{"dur":2,"text":"working on a lot in\nthe last 20 odd years"},"266":{"dur":1,"text":"is swarm intelligence."},"267":{"dur":2,"text":"So this is the kind\nof intelligence"},"270":{"dur":5,"text":"that we see most particularly\nin social animals, insects."},"279":{"dur":3,"text":"The most interesting properties\nof swarm intelligence"},"282":{"dur":2,"text":"tend to be emergent\nor self-organizing."},"285":{"dur":2,"text":"So in other words,\nthe intelligence"},"288":{"dur":4,"text":"is typically manifest as\na collective behavior that"},"292":{"dur":3,"text":"emerges from the, if you\nlike, the micro interactions"},"296":{"dur":2,"text":"between the individuals\nin that population."},"299":{"dur":1,"text":"So emergence and\nself-organization"},"300":{"dur":2,"text":"are particularly\ninteresting to me."},"303":{"dur":4,"text":"But I said this is\nabsolutely not the only way"},"308":{"dur":1,"text":"to think about intelligence."},"310":{"dur":3,"text":"And I'm going to\nshow you another way"},"313":{"dur":3,"text":"of thinking about intelligence\nwhich I particularly like."},"317":{"dur":5,"text":"And this is Dan Dennett's\ntower of generate and test."},"322":{"dur":4,"text":"So in Darwin's Dangerous\nIdea, and several other books,"},"326":{"dur":5,"text":"I think, Dan Dennett\nsuggests that a good way"},"331":{"dur":3,"text":"of thinking about intelligence\nis to think about the fact"},"335":{"dur":4,"text":"that all animals,\nincluding ourselves, need"},"339":{"dur":2,"text":"to decide what actions to take."},"341":{"dur":5,"text":"So choosing the next action is\nreally critically important."},"347":{"dur":4,"text":"I mean it's critically important\nfor all of us, including"},"351":{"dur":0,"text":"humans."},"352":{"dur":2,"text":"Even though the wrong\naction may not kill us,"},"354":{"dur":1,"text":"as it were, for humans."},"356":{"dur":1,"text":"But for many animals,\nthe wrong action"},"358":{"dur":2,"text":"may well kill that animal."},"360":{"dur":4,"text":"And Dennett talks\nabout what he calls"},"365":{"dur":3,"text":"the tower of generate and test\nwhich I want to show you here."},"368":{"dur":3,"text":"It's a really cool\nbreakdown, if you like, way"},"371":{"dur":2,"text":"of thinking about intelligence."},"373":{"dur":4,"text":"So at the bottom of his tower\nare Darwinian creatures."},"378":{"dur":2,"text":"And the thing about\nDarwinian creatures"},"380":{"dur":3,"text":"is that they have\nonly one way of,"},"383":{"dur":5,"text":"as it were, learning\nfrom, if you like,"},"389":{"dur":2,"text":"generating and testing\nnext possible actions."},"391":{"dur":3,"text":"And that is natural selection."},"395":{"dur":4,"text":"So Darwinian creatures in\nhis schema cannot learn."},"400":{"dur":2,"text":"They can only try out an action."},"402":{"dur":2,"text":"If it kills them, well\nthat's the end of that."},"405":{"dur":3,"text":"So by the laws of\nnatural selection,"},"408":{"dur":2,"text":"that particular action\nis unlikely to be"},"410":{"dur":4,"text":"passed on to descendants."},"415":{"dur":3,"text":"Now, of course, all\nanimals on the planet"},"418":{"dur":2,"text":"are Darwinian creatures,\nincluding ourselves."},"421":{"dur":3,"text":"But a subset of what Dennett\ncalls Skinnerian creatures."},"425":{"dur":5,"text":"So Skinnerian creatures\nare able to generate"},"430":{"dur":2,"text":"a next possible candidate\naction, if you like,"},"433":{"dur":3,"text":"a next possible\naction and try it out."},"436":{"dur":3,"text":"And here's the thing,\nif it doesn't kill them"},"440":{"dur":3,"text":"but it's actually a bad action,\nthen they'll learn from that."},"443":{"dur":2,"text":"Or even if it's a good\naction, a Skinnerian creature"},"446":{"dur":2,"text":"will learn from\ntrying out an action."},"449":{"dur":5,"text":"So really, Skinnerian creatures\nare a subset of Darwinians,"},"455":{"dur":3,"text":"actually a small subset\nthat are able to learn"},"458":{"dur":4,"text":"by trial and error, individually\nlearn by trial and error."},"463":{"dur":4,"text":"Now, the third layer,\nor story, if you"},"468":{"dur":3,"text":"like, in Dennett's tower, he\ncalls Popperian creatures,"},"471":{"dur":3,"text":"after, obviously, the\nphilosopher, Karl Popper."},"475":{"dur":3,"text":"And Popperian creatures\nhave a big advantage"},"478":{"dur":3,"text":"over Darwinians and\nSkinnerians in that they"},"481":{"dur":3,"text":"have an internal model of\nthemselves in the world."},"485":{"dur":1,"text":"And with an internal\nmodel, it means"},"487":{"dur":3,"text":"that you can try out\nan action, a candidate"},"490":{"dur":5,"text":"next possible action, if\nyou like, by imagining it."},"495":{"dur":2,"text":"And it means that you\ndon't have to actually have"},"497":{"dur":3,"text":"to put yourself to the\nrisk of trying it out"},"501":{"dur":2,"text":"for real physically\nin the world,"},"503":{"dur":5,"text":"and possibly it killing you,\nor at least harming you."},"508":{"dur":4,"text":"So Popperian creatures have\nthis amazing invention,"},"513":{"dur":1,"text":"which is internal modeling."},"515":{"dur":4,"text":"And of course, we are examples\nof Popperian creatures."},"519":{"dur":2,"text":"But there are plenty of\nother animals-- again,"},"521":{"dur":2,"text":"it's not a huge proportion."},"524":{"dur":2,"text":"It's rather a small proportion,\nin fact, of all animals."},"527":{"dur":1,"text":"But certainly there\nare plenty of animals"},"528":{"dur":5,"text":"that are capable in some\nform of modeling their world"},"534":{"dur":4,"text":"and, as it were, imagining\nactions before trying them out."},"538":{"dur":2,"text":"And just to complete\nDennett's tower,"},"541":{"dur":5,"text":"he adds another layer that\nhe calls Gregorian creatures."},"546":{"dur":3,"text":"Here's he's naming this\nlayer after Richard Gregory,"},"549":{"dur":5,"text":"the British psychologist."},"554":{"dur":3,"text":"And the thing that\nGregorian creatures have"},"558":{"dur":3,"text":"is that in addition\nto internal models,"},"561":{"dur":4,"text":"they have mind tools like\nlanguage and mathematics."},"565":{"dur":5,"text":"Especially language because it\nmeans that Gregorian creatures"},"571":{"dur":2,"text":"can share their experiences."},"574":{"dur":3,"text":"In fact, a Gregorian\ncreature could, for instance,"},"577":{"dur":8,"text":"model in its brain, in its\nmind, the possible consequences"},"585":{"dur":3,"text":"of doing a particular thing,\nand then actually pass"},"588":{"dur":0,"text":"that knowledge to you."},"589":{"dur":2,"text":"So you don't even have\nto model it yourself."},"592":{"dur":3,"text":"So the Gregorian\ncreatures really"},"596":{"dur":2,"text":"have the kind of\nsocial intelligence"},"598":{"dur":4,"text":"that we probably--\nperhaps not uniquely,"},"602":{"dur":1,"text":"but there are obviously\nonly a handful"},"604":{"dur":6,"text":"of species that are able to\ncommunicate, if you like,"},"610":{"dur":3,"text":"traditions with each other."},"613":{"dur":5,"text":"So I think internal models are\nreally, really interesting."},"619":{"dur":2,"text":"And as I say, I've been\nspending the last couple"},"622":{"dur":5,"text":"of years thinking about\nrobots with internal models."},"627":{"dur":2,"text":"And actually doing\nexperiments with"},"629":{"dur":1,"text":"robots with internal models."},"631":{"dur":4,"text":"So are robots with\ninternal models self-aware?"},"635":{"dur":3,"text":"Well probably not in the\nsense that-- the everyday"},"639":{"dur":3,"text":"sense that we mean by\nself-aware, sentient."},"642":{"dur":1,"text":"But certainly internal\nmodels, I think,"},"644":{"dur":1,"text":"can provide a\nminimal level of kind"},"646":{"dur":2,"text":"of functional self-awareness."},"648":{"dur":5,"text":"And absolutely enough to allow\nus to ask what if questions."},"653":{"dur":4,"text":"So with internal models, we have\npotentially a really powerful"},"658":{"dur":2,"text":"technique for robots."},"660":{"dur":2,"text":"Because it means that\nthey can actually ask"},"662":{"dur":3,"text":"themselves questions\nabout what if I take this"},"666":{"dur":2,"text":"or that next possible action."},"668":{"dur":2,"text":"So there's the action\nselection, if you like."},"673":{"dur":4,"text":"So really, I'm kind of\nfollowing Dennett's model."},"678":{"dur":3,"text":"I'm really interested in\nbuilding Popperian creatures."},"681":{"dur":2,"text":"Actually, I'm interested in\nbuilding Gregorian creatures."},"684":{"dur":5,"text":"But that's another, if you\nlike, another step in the story."},"689":{"dur":2,"text":"So really, here I'm\nfocusing primarily"},"691":{"dur":1,"text":"on Popperian creatures."},"693":{"dur":1,"text":"So robots with internal models."},"698":{"dur":3,"text":"And what I'm talking\nabout in particular"},"701":{"dur":4,"text":"is a robot with a\nsimulation of itself"},"706":{"dur":3,"text":"and it's currently perceived\nenvironment and of the actors"},"709":{"dur":1,"text":"inside itself."},"711":{"dur":2,"text":"So it takes a bit of\ngetting your head around."},"713":{"dur":2,"text":"The idea of a robot with\na simulation of itself"},"716":{"dur":0,"text":"inside itself."},"717":{"dur":3,"text":"But that's really what\nI'm talking about."},"720":{"dur":3,"text":"And the famous, the late\nJohn Holland, for instance,"},"724":{"dur":5,"text":"rather perceptively\nwrote an internal model"},"729":{"dur":1,"text":"that allows a\nsystem to look ahead"},"731":{"dur":2,"text":"to the future\nconsequences of actions"},"733":{"dur":2,"text":"without committing\nitself to those actions."},"736":{"dur":1,"text":"I don't know\nwhether John Holland"},"738":{"dur":2,"text":"was aware of Dennett's tower."},"740":{"dur":0,"text":"Possibly not."},"741":{"dur":4,"text":"But really saying the same\nkind of thing as Dan Dennett."},"746":{"dur":4,"text":"Now before I come on to the\nwork that I've been doing,"},"751":{"dur":3,"text":"I want to show you some\nexamples of-- a few examples,"},"754":{"dur":4,"text":"there aren't many, in fact--\nof robots with self-simulation."},"761":{"dur":3,"text":"The first one, as\nfar as I'm aware,"},"765":{"dur":2,"text":"was by Richard\nVaughan and his team."},"767":{"dur":2,"text":"And he used a simulation\ninside a robot"},"770":{"dur":6,"text":"to allow it to plan a safe\nroute with incomplete knowledge."},"777":{"dur":3,"text":"So as far as I'm aware, this\nis the world's first example"},"780":{"dur":2,"text":"of robots with self-simulation."},"786":{"dur":3,"text":"Perhaps an example that you\nmight already be familiar with,"},"790":{"dur":4,"text":"this is Josh Bongard\nand Hod Lipson's work."},"794":{"dur":2,"text":"Very notable, very\ninteresting work."},"796":{"dur":4,"text":"Here, self-simulation, but\nfor a different purpose."},"801":{"dur":2,"text":"So this is not\nself-simulation to choose,"},"803":{"dur":2,"text":"as it were, gross\nactions in the world."},"806":{"dur":2,"text":"But instead,\nself-simulation to learn"},"808":{"dur":2,"text":"how to control your own body."},"810":{"dur":5,"text":"So that the idea here is that\nif you have a complex body, then"},"815":{"dur":2,"text":"a self-simulation is a really\ngood way of figuring out"},"818":{"dur":1,"text":"how to control\nyourself, including"},"820":{"dur":2,"text":"how to repair yourself\nif parts of you"},"822":{"dur":6,"text":"should break or fail or\nbe damaged, for instance."},"828":{"dur":3,"text":"So that's a really\ninteresting example"},"832":{"dur":2,"text":"of what you can do\nwith self-simulation."},"834":{"dur":5,"text":"And a similar idea,\nreally, was tested"},"839":{"dur":3,"text":"by my old friend, Owen Holland."},"843":{"dur":3,"text":"He built this kind of\nscary looking robot."},"846":{"dur":2,"text":"Initially it was called\nChronos, but but then it"},"848":{"dur":2,"text":"became known as ECCE-robot."},"851":{"dur":6,"text":"And this robot is deliberately\ndesigned to be hard to control."},"858":{"dur":2,"text":"In fact, Owen refers to\nit as anthropomimetic."},"864":{"dur":3,"text":"Which means anthropic\nfrom the inside out."},"867":{"dur":4,"text":"So most humanoid robots are\nonly humanoid on the outside."},"871":{"dur":3,"text":"But here, we have a robot\nthat has a skeletal structure,"},"875":{"dur":3,"text":"it has tendons,\nit's very-- and you"},"879":{"dur":1,"text":"can see from the\nlittle movie clip"},"880":{"dur":3,"text":"there, if any part\nof the robot moves,"},"884":{"dur":3,"text":"then the whole of\nthe rest of the robot"},"887":{"dur":6,"text":"tends to flex, rather like\nhuman bodies or animal bodies."},"894":{"dur":5,"text":"So Owen was particularly\ninterested in a robot"},"899":{"dur":2,"text":"that is difficult to control."},"901":{"dur":4,"text":"And the idea then of using an\ninternal simulation of yourself"},"906":{"dur":2,"text":"in order to be able to\ncontrol yourself or learn"},"909":{"dur":1,"text":"to control yourself."},"910":{"dur":4,"text":"And he was the first to\ncome up with this phrase,"},"915":{"dur":2,"text":"functional imagination."},"918":{"dur":2,"text":"Really interesting work,\nso do check that out."},"923":{"dur":1,"text":"And the final example\nI want to give"},"925":{"dur":5,"text":"you is from my own lab,\nwhere-- this is swarm robotics"},"931":{"dur":5,"text":"work-- where in fact we're doing\nevolutionary swarm robotics"},"936":{"dur":1,"text":"here."},"937":{"dur":6,"text":"And we've put a simulation\nof each robot and the swarm"},"944":{"dur":2,"text":"inside each robot."},"947":{"dur":3,"text":"And in fact, we're using\nthose internal simulations"},"950":{"dur":1,"text":"as part of a genetic algorithm."},"951":{"dur":4,"text":"So each robot, in fact, is\nevolving its own controller."},"956":{"dur":3,"text":"And in fact, it actually\nupdates its own controller"},"959":{"dur":1,"text":"about once a second."},"961":{"dur":4,"text":"So again, it's a bit an odd\nthing to get your head around."},"965":{"dur":3,"text":"So about once a\nsecond, each robot"},"968":{"dur":3,"text":"becomes its own great, great,\ngreat, great grandchild."},"972":{"dur":3,"text":"In other words, its\ncontroller is a descendant."},"977":{"dur":4,"text":"But the problem with this is\nthat the internal simulation"},"982":{"dur":1,"text":"tends to be wrong."},"984":{"dur":2,"text":"And we have what we\ncall the reality gap."},"986":{"dur":3,"text":"So the gap between the\nsimulation and the real world."},"990":{"dur":3,"text":"And so we got round that-- my\nstudent Paul O'Dowd came up"},"993":{"dur":1,"text":"with the idea that\nwe could co-evolve"},"995":{"dur":4,"text":"the simulators, as well as\nthe controllers in the robots."},"999":{"dur":2,"text":"So we have a\npopulation of robots"},"1002":{"dur":3,"text":"inside each individual\nphysical robot, as it were,"},"1005":{"dur":1,"text":"simulated robots."},"1007":{"dur":3,"text":"But then you also have\na swarm of 10 robots."},"1010":{"dur":4,"text":"And therefore, we have a\npopulation of 10 simulators."},"1015":{"dur":2,"text":"So we actually co-evolve\nhere, the simulators"},"1018":{"dur":3,"text":"and the robot controllers."},"1021":{"dur":3,"text":"So I want to now\nshow you the newer"},"1025":{"dur":4,"text":"work I've been doing on\nrobots with internal models."},"1029":{"dur":5,"text":"And primarily-- I was\ntelling [? Yan ?] earlier"},"1035":{"dur":3,"text":"that, you know, I'm kind of\nan old fashioned electronics"},"1038":{"dur":0,"text":"engineer."},"1039":{"dur":3,"text":"Spent much of my career\nbuilding safety systems,"},"1042":{"dur":1,"text":"safety critical systems."},"1043":{"dur":3,"text":"So safety is something\nthat's very important to me"},"1046":{"dur":1,"text":"and to robotics."},"1048":{"dur":2,"text":"So here's a kind of\ngeneric internal modeling"},"1051":{"dur":2,"text":"architecture for safety."},"1053":{"dur":6,"text":"So this is, in fact, Dennett's\nloop of generate and test."},"1059":{"dur":2,"text":"So the idea is that we have\nan internal model, which"},"1062":{"dur":4,"text":"is a self-simulation, that\nis initialized to match"},"1066":{"dur":2,"text":"the current real world."},"1068":{"dur":4,"text":"And then you try out,\nyou run the simulator"},"1073":{"dur":3,"text":"for each of your next\npossible actions."},"1076":{"dur":4,"text":"To put it very simply,\nimagine that you're a robot,"},"1080":{"dur":3,"text":"and you could either turn left,\nturn right, go straight ahead,"},"1083":{"dur":1,"text":"or stand still."},"1084":{"dur":2,"text":"So you have four\npossible next actions."},"1087":{"dur":3,"text":"And therefore, you'd loop\nthrough this internal model"},"1091":{"dur":2,"text":"for each of those\nnext possible actions."},"1093":{"dur":3,"text":"And then moderate the\naction selection mechanism"},"1097":{"dur":1,"text":"in your controller."},"1098":{"dur":2,"text":"So this is not part\nof the controller."},"1101":{"dur":2,"text":"It's a kind of\nmoderator, if you like."},"1103":{"dur":4,"text":"So you could imagine\nthat the regular robot"},"1108":{"dur":3,"text":"controller, the thing in\nred, has a set of four"},"1112":{"dur":2,"text":"next possible actions."},"1114":{"dur":3,"text":"But your internal\nmodel determines"},"1118":{"dur":3,"text":"that only two of them are safe."},"1121":{"dur":2,"text":"So it would effectively,\nif you like,"},"1124":{"dur":3,"text":"moderate or govern the\naction selection mechanism"},"1128":{"dur":2,"text":"of the robot's controller,\nso that the robot"},"1130":{"dur":4,"text":"controller, in fact, will not\nchoose the unsafe actions."},"1138":{"dur":5,"text":"Interestingly, if you have\na learning controller,"},"1143":{"dur":5,"text":"then that's fine because we\ncan effectively extend or copy"},"1149":{"dur":4,"text":"the learned behaviors\ninto the internal model."},"1153":{"dur":1,"text":"That's fine."},"1154":{"dur":2,"text":"So in principle-- we\nhaven't done this."},"1156":{"dur":2,"text":"But we're starting to do\nit now-- in principle,"},"1159":{"dur":5,"text":"we can extend this architecture\nto, as it were, to adaptive"},"1164":{"dur":1,"text":"or learning robots."},"1168":{"dur":2,"text":"Here's a simple\nthought experiment."},"1170":{"dur":5,"text":"Imagine a robot with several\nsafety hazards facing it."},"1176":{"dur":2,"text":"It has four next\npossible actions."},"1178":{"dur":7,"text":"Well, your internal\nmodel can figure out"},"1185":{"dur":4,"text":"what the consequence of each\nof those actions might be."},"1190":{"dur":7,"text":"So two of them-- so either\nturn right or stay still"},"1198":{"dur":1,"text":"are safe actions."},"1199":{"dur":1,"text":"So that's a very simple\nthought experiment."},"1203":{"dur":4,"text":"And here's a slightly more\ncomplicated thought experiment."},"1208":{"dur":1,"text":"So imagine that\nthe robot, there's"},"1210":{"dur":1,"text":"another actor in\nthe environment."},"1211":{"dur":1,"text":"It's a human."},"1213":{"dur":2,"text":"The human is not looking\nwhere they're going."},"1215":{"dur":2,"text":"Perhaps walking down the\nstreet peering at a smartphone."},"1217":{"dur":2,"text":"That never happens,\ndoes it, of course."},"1220":{"dur":4,"text":"And about to walk into\na hole in the pavement."},"1224":{"dur":5,"text":"Well, of course, if\nit were you noticing"},"1230":{"dur":2,"text":"that human about to walk\ninto a hole in the pavement,"},"1232":{"dur":2,"text":"you would almost certainly\nintervene, of course."},"1234":{"dur":2,"text":"And it's not just because\nyou're a good person."},"1237":{"dur":2,"text":"It's because you have\nthe cognitive machinery"},"1240":{"dur":4,"text":"to predict the consequences of\nboth your and their actions."},"1244":{"dur":1,"text":"And you can figure\nout that if you"},"1246":{"dur":2,"text":"were to rush over\ntowards them, you"},"1248":{"dur":2,"text":"might be able to prevent them\nfrom falling into the hole."},"1250":{"dur":3,"text":"So here's the same kind of idea."},"1254":{"dur":2,"text":"Imagine it's not\nyou, but a robot."},"1257":{"dur":3,"text":"And imagine now that\nyou are modeling"},"1261":{"dur":3,"text":"the consequences of\nyours and the human's"},"1264":{"dur":4,"text":"actions for each one of\nyour next possible actions."},"1268":{"dur":2,"text":"And you can see\nthat now this time,"},"1271":{"dur":2,"text":"we've given a kind\nof numerical scale."},"1273":{"dur":5,"text":"So 0 is perfectly safe, whereas\n10 is seriously dangerous,"},"1279":{"dur":3,"text":"kind of danger of\ndeath, if you like."},"1283":{"dur":4,"text":"And you can see that\nthe safest outcome"},"1287":{"dur":2,"text":"is if the robot turns right."},"1289":{"dur":2,"text":"In other words, the\nsafest for the human."},"1292":{"dur":2,"text":"I mean, clearly the\nsafest for the robot"},"1294":{"dur":2,"text":"is either turn\nleft or stay still."},"1297":{"dur":4,"text":"But in both cases, the human\nwould fall into the hole."},"1301":{"dur":1,"text":"So you can see that\nwe could actually"},"1303":{"dur":4,"text":"invent a rule which\nwould represent"},"1308":{"dur":3,"text":"the best outcome for the human."},"1311":{"dur":2,"text":"And this is what it looks like."},"1313":{"dur":3,"text":"So if all robot actions,\nthe human is equally safe,"},"1317":{"dur":3,"text":"then that means that we don't\nneed to worry about the human,"},"1320":{"dur":7,"text":"so the internal model will\noutput the safest actions"},"1327":{"dur":2,"text":"for the robot."},"1329":{"dur":3,"text":"Else, then output\nthe robot actions"},"1333":{"dur":3,"text":"for the least unsafe\nhuman outcomes."},"1337":{"dur":3,"text":"Now remarkably-- and\nwe didn't intend this,"},"1341":{"dur":2,"text":"this actually is\nan implementation"},"1343":{"dur":2,"text":"of Asimov's first\nlaw of robotics."},"1346":{"dur":3,"text":"So a robot may not\ninjure a human being,"},"1349":{"dur":1,"text":"or through inaction--\nthat's important,"},"1351":{"dur":4,"text":"the or through inaction-- allow\na human being to come to harm."},"1356":{"dur":5,"text":"So we kind of ended up\nbuilding Asimovian robot,"},"1361":{"dur":4,"text":"simple Asimovian ethical robot."},"1366":{"dur":2,"text":"So what does it look like?"},"1368":{"dur":3,"text":"Well, we've now extended\nto humanoid robots."},"1372":{"dur":1,"text":"But we started with\nthe e-puck robots,"},"1374":{"dur":5,"text":"these little-- they're about\nthe size of a salt shaker,"},"1379":{"dur":5,"text":"I guess, about seven\ncentimeters tall."},"1384":{"dur":5,"text":"And this is the little\narena in the lab."},"1389":{"dur":6,"text":"And what we actually have\ninside the ethical robot is--"},"1396":{"dur":2,"text":"this is the internal\narchitecture."},"1398":{"dur":4,"text":"So so you can see that we have\nthe robot controller, which"},"1403":{"dur":4,"text":"is, in fact, a mirror of\nthe real robot controller,"},"1407":{"dur":2,"text":"a model of the robot, and\na model of the world, which"},"1410":{"dur":2,"text":"includes others in the world."},"1412":{"dur":1,"text":"So this is the simulator."},"1413":{"dur":4,"text":"This is a more or less a\nregular robot simulator."},"1418":{"dur":2,"text":"So you probably know\nthat robot simulators"},"1420":{"dur":1,"text":"are quite commonplace."},"1422":{"dur":5,"text":"We roboticists use them all\nthe time to test robots in,"},"1428":{"dur":1,"text":"as it were, in\nthe virtual world,"},"1429":{"dur":2,"text":"before then trying\nout the code for real."},"1431":{"dur":1,"text":"But what we've done\nhere is we've actually"},"1433":{"dur":6,"text":"put an off the shelf\nsimulator inside the robot"},"1439":{"dur":2,"text":"and made it work in real time."},"1441":{"dur":3,"text":"So the output of the\nsimulator for each"},"1444":{"dur":4,"text":"of those next possible actions\nis evaluated and then goes"},"1449":{"dur":1,"text":"through a logic layer."},"1451":{"dur":3,"text":"Which is essentially the\nrule, the if then else rule"},"1454":{"dur":3,"text":"that I showed you a\ncouple of slides ago."},"1458":{"dur":3,"text":"And that effectively\ndetermines or moderates"},"1462":{"dur":4,"text":"the action selection\nmechanism of the real robot."},"1469":{"dur":1,"text":"So this is the\nsimulation budget."},"1471":{"dur":3,"text":"So we're actually using\nthe open source simulator"},"1475":{"dur":4,"text":"Stage, a well-known simulator."},"1479":{"dur":3,"text":"And in fact, we managed to get\nStage to run about 600 times"},"1482":{"dur":2,"text":"real time."},"1484":{"dur":2,"text":"Which means that\nwe're actually cycling"},"1487":{"dur":2,"text":"through our internal\nmodel twice a second."},"1490":{"dur":3,"text":"And for each one\nof those cycles,"},"1493":{"dur":5,"text":"we're actually modeling not four\nbut 30 next possible actions."},"1498":{"dur":3,"text":"And we're modeling about\n10 seconds into the future."},"1502":{"dur":5,"text":"So every half a second, our\nrobot with an internal model"},"1507":{"dur":7,"text":"is looking ahead 10 seconds for\nabout 30 next possible actions,"},"1514":{"dur":2,"text":"30 of its own next\npossible actions."},"1517":{"dur":3,"text":"But of course, it's also\nmodeling the consequences"},"1520":{"dur":3,"text":"of each of the other\nactors, dynamic actors"},"1523":{"dur":1,"text":"in its environment."},"1525":{"dur":5,"text":"So this is quite nice to\nactually do this in real time."},"1530":{"dur":2,"text":"And let me show you some of the\nresults that we got from that."},"1533":{"dur":3,"text":"So ignore the kind\nof football pitch."},"1537":{"dur":3,"text":"So what we have here is\nthe ethical robot, which we"},"1540":{"dur":3,"text":"call the A-robot, after Asimov."},"1544":{"dur":1,"text":"And we have a hole\nin the ground."},"1546":{"dur":2,"text":"It's not a real hole, it's a\nvirtual hole in the ground."},"1548":{"dur":3,"text":"We don't need to be digging\nholes into the lab floor."},"1551":{"dur":4,"text":"And we're using another\ne-perk as a proxy human"},"1556":{"dur":1,"text":"we call this the H-robot."},"1560":{"dur":2,"text":"So let me show\nyou what happened."},"1562":{"dur":6,"text":"Well we ran it, first of\nall, with no H-robot at all,"},"1568":{"dur":1,"text":"as a kind of baseline."},"1570":{"dur":3,"text":"And you can see on\nthe left, in 26 runs,"},"1573":{"dur":2,"text":"those are the traces\nof the A-robot."},"1576":{"dur":1,"text":"So you can see the\nA-robot, in fact,"},"1578":{"dur":2,"text":"is maintaining its own safety."},"1580":{"dur":3,"text":"Its avoiding, its\nskirting around the edge"},"1583":{"dur":4,"text":"almost optimally skirting the\nedge of the hole in the ground."},"1588":{"dur":1,"text":"But then when we\nintroduce the H-robot,"},"1590":{"dur":3,"text":"you get this wonderful\nbehavior here."},"1593":{"dur":3,"text":"Where as soon as the A-robot\nnotices that the H-robot is"},"1597":{"dur":2,"text":"heading towards the hole,\nwhich is about here, then"},"1600":{"dur":4,"text":"it deflects, it diverts\nfrom its original course."},"1604":{"dur":4,"text":"And in fact, more\nor less collides."},"1609":{"dur":1,"text":"They don't actually\nphysically collide"},"1610":{"dur":3,"text":"because they have low\nlevel collision avoidance."},"1613":{"dur":1,"text":"So they don't actually collide."},"1615":{"dur":2,"text":"But nevertheless, the\nA-robot effectively"},"1617":{"dur":5,"text":"heads off the H-robot, but\nthen bounces off safely,"},"1622":{"dur":2,"text":"goes off in another direction."},"1624":{"dur":5,"text":"And the A-robot then resumes its\ncourse to its target position."},"1630":{"dur":2,"text":"Which is really nice."},"1632":{"dur":4,"text":"And interestingly, even though\nour simulator is rather low"},"1637":{"dur":2,"text":"fidelity, it doesn't matter."},"1640":{"dur":1,"text":"Surprisingly, it doesn't matter."},"1641":{"dur":3,"text":"Because the closer the\nA-robot to the H-robot"},"1644":{"dur":4,"text":"gets, then the better its\npredictions about colliding."},"1649":{"dur":3,"text":"So this is why, even with a\nrather low fidelity simulator,"},"1653":{"dur":6,"text":"we can collide with really good\nprecision with the H-robot."},"1659":{"dur":7,"text":"So let me show you the movies of\nthis trial with a single proxy"},"1667":{"dur":1,"text":"human."},"1669":{"dur":5,"text":"And I think the movie starts\nin-- so this is real time."},"1675":{"dur":4,"text":"And you can see the\nA-robot nicely heading off"},"1679":{"dur":3,"text":"the H-robot which\nthen disappears off"},"1683":{"dur":1,"text":"towards the left."},"1687":{"dur":3,"text":"I think then we've\nspeeded it four times."},"1691":{"dur":2,"text":"And this is a\nwhole load of runs."},"1694":{"dur":3,"text":"So you can see that\nit really does work."},"1697":{"dur":3,"text":"And also notice that every\nexperiment is a bit different."},"1700":{"dur":2,"text":"And of course, that's what\ntypically happens when"},"1703":{"dur":2,"text":"you have real physical robots."},"1705":{"dur":2,"text":"Simply because of the\nnoise in the system,"},"1708":{"dur":3,"text":"the fact that these are real\nrobots with imperfect motors"},"1711":{"dur":1,"text":"and sensors and what have you."},"1715":{"dur":5,"text":"So we wrote the paper\nand were about to submit"},"1721":{"dur":2,"text":"the paper, when we\nkind of thought, well,"},"1723":{"dur":1,"text":"this is a bit boring, isn't it?"},"1725":{"dur":4,"text":"We built this\nrobot and it works."},"1729":{"dur":7,"text":"So we had the idea to put a\nsecond human in the-- oh sorry."},"1737":{"dur":1,"text":"I've forgotten one slide."},"1738":{"dur":2,"text":"So before I get to\nthat, I just wanted"},"1740":{"dur":8,"text":"to show you a little animation\nof-- these little filaments"},"1748":{"dur":6,"text":"here are the traces of the\nA-robot and its prediction"},"1754":{"dur":1,"text":"of what might happen."},"1756":{"dur":2,"text":"So at the point\nwhere this turns red,"},"1759":{"dur":2,"text":"the A-robot then\nstarts to intersect."},"1761":{"dur":4,"text":"And each one of those\ntraces is its prediction"},"1766":{"dur":4,"text":"of the consequences of both\nitself and the H-robot."},"1771":{"dur":6,"text":"This is really nice because you\ncan kind of look into the mind,"},"1777":{"dur":3,"text":"to put it that\nway, of the robot,"},"1780":{"dur":2,"text":"and actually see\nwhat it's doing."},"1782":{"dur":2,"text":"Which is very nice, very cool."},"1785":{"dur":6,"text":"But I was about to say we tried\nthe same experiment, in fact,"},"1791":{"dur":3,"text":"identical code,\nwith two H-robots."},"1795":{"dur":3,"text":"And this is the robot's dilemma."},"1799":{"dur":2,"text":"This may be the first time\nthat a real physical robot"},"1802":{"dur":3,"text":"has faced an ethical dilemma."},"1805":{"dur":3,"text":"So you can see the two\nH-robots are more or less"},"1808":{"dur":2,"text":"equidistant from the hole."},"1810":{"dur":3,"text":"And there is the\nA-robot which, in fact,"},"1814":{"dur":4,"text":"fails to save either of them."},"1818":{"dur":3,"text":"So what's going on there?"},"1822":{"dur":4,"text":"We know that it can save\none of them every time."},"1826":{"dur":2,"text":"But in fact, it's just\nfailed to save either."},"1831":{"dur":3,"text":"And oh, yeah, it does\nactually save one of them."},"1834":{"dur":3,"text":"And has a look at the other\none, but it's too late."},"1838":{"dur":3,"text":"So this is really\nvery interesting."},"1841":{"dur":1,"text":"And not at all what we expected."},"1851":{"dur":2,"text":"In fact, let me show\nyou the statistics."},"1854":{"dur":11,"text":"So in 33 runs, the\nethical robot failed"},"1865":{"dur":5,"text":"to save either of the H-robots\njust under half the time."},"1871":{"dur":3,"text":"So about 14 times, it\nfailed to save either."},"1874":{"dur":4,"text":"It saved one of them just\nover 15, perhaps 16 times."},"1879":{"dur":2,"text":"And amazingly, saved\nboth of them twice,"},"1882":{"dur":3,"text":"which is quite surprising."},"1885":{"dur":2,"text":"It really should perform\nbetter than that."},"1890":{"dur":3,"text":"And in fact, when we started\nto really look at this,"},"1893":{"dur":2,"text":"we discovered that\nthe-- so here's"},"1896":{"dur":3,"text":"a particularly good\nexample of dithering."},"1899":{"dur":4,"text":"So we realized\nthat we made a sort"},"1904":{"dur":2,"text":"of pathologically\nindecisive ethical robot."},"1907":{"dur":1,"text":"So I'm going to save this\none-- oh no, no, that"},"1909":{"dur":1,"text":"one-- oh no, no,\nthis one-- that one."},"1911":{"dur":3,"text":"And of course, by the\ntime our ethical robot"},"1914":{"dur":2,"text":"has changed its mind\nthree or four times,"},"1916":{"dur":1,"text":"well, it's too late."},"1918":{"dur":1,"text":"So this is the problem."},"1919":{"dur":5,"text":"The problem, fundamentally,\nis that our ethical robot"},"1925":{"dur":3,"text":"doesn't make a decision\nand stick to it."},"1928":{"dur":2,"text":"In fact, it's a\nconsequence of the fact"},"1930":{"dur":4,"text":"that we are running\nour consequence engine,"},"1934":{"dur":1,"text":"as I mentioned, twice a second."},"1936":{"dur":2,"text":"So every half a second,\nour ethical robot"},"1939":{"dur":2,"text":"has the opportunity\nto change its mind."},"1941":{"dur":1,"text":"That's clearly a bad strategy."},"1943":{"dur":3,"text":"But nevertheless, it\nwas an interesting kind"},"1946":{"dur":5,"text":"of unexpected consequence\nof the experiment."},"1952":{"dur":3,"text":"We've now transferred the\nwork to these humanoid robots."},"1956":{"dur":2,"text":"And we get the same thing."},"1958":{"dur":2,"text":"So here, there\nare two red robots"},"1961":{"dur":1,"text":"both heading toward danger."},"1963":{"dur":3,"text":"The blue one, the ethical\nrobot, changes its mind,"},"1966":{"dur":1,"text":"and goes and saves\nthe one on the left,"},"1968":{"dur":2,"text":"even though it could have\nsaved the one on the right."},"1971":{"dur":8,"text":"So another example of our\ndithering ethical robot."},"1979":{"dur":4,"text":"And as I've just\nhinted at, the reason"},"1983":{"dur":2,"text":"that there our ethical\nrobot is so indecisive"},"1986":{"dur":3,"text":"is because it's essentially\na memory-less architecture."},"1989":{"dur":5,"text":"So you could say that the\nrobot has a-- again, borrowing"},"1995":{"dur":4,"text":"Owen Holland's description, it\nhas a functional imagination."},"1999":{"dur":2,"text":"But it has no\nautobiographical memory."},"2001":{"dur":1,"text":"So it doesn't\nremember the decision"},"2003":{"dur":2,"text":"it made half a second ago."},"2005":{"dur":3,"text":"Which is clearly\nnot a good strategy."},"2009":{"dur":3,"text":"Really, an ethical\nrobot, just like you"},"2013":{"dur":3,"text":"if you are acting in\na similar situation,"},"2016":{"dur":2,"text":"it's probably a\ngood idea for you"},"2018":{"dur":2,"text":"to stick to the first\ndecision that you made."},"2020":{"dur":2,"text":"But probably not forever."},"2023":{"dur":2,"text":"So you know, I think\nthe decisions probably"},"2025":{"dur":1,"text":"need to be sticky somehow."},"2027":{"dur":3,"text":"So decisions like this\nmay need a half life."},"2031":{"dur":3,"text":"You know, sticky but not\nbut not absolutely rigid."},"2034":{"dur":3,"text":"So actually, at this\npoint, we decided"},"2038":{"dur":2,"text":"that we're not going to worry\ntoo much about this problem."},"2041":{"dur":3,"text":"Because in a sense, this is\nmore of a problem for ethicists"},"2044":{"dur":1,"text":"than engineers, perhaps."},"2046":{"dur":1,"text":"But maybe we could\ntalk about that."},"2051":{"dur":2,"text":"Before finishing,\nI want to show you"},"2054":{"dur":2,"text":"another experiment that we did\nwith the same architecture,"},"2057":{"dur":1,"text":"exactly the same architecture."},"2059":{"dur":3,"text":"And this is what we call\nthe corridor experiment."},"2062":{"dur":5,"text":"So here we have a robot\nwith this internal model."},"2067":{"dur":4,"text":"And it has to get from the\nleft hand to the right hand"},"2072":{"dur":3,"text":"of a crowded corridor\nwithout bumping"},"2075":{"dur":3,"text":"into any of the other robots\nthat are in the same corridor."},"2078":{"dur":4,"text":"So imagine you're walking\ndown a corridor in an airport"},"2082":{"dur":2,"text":"and everybody else is coming\nin the opposite direction."},"2085":{"dur":2,"text":"And you want to try and get to\nthe other end of the corridor"},"2087":{"dur":1,"text":"without crashing\ninto any of them."},"2089":{"dur":3,"text":"But in fact, you have a\nrather large body space."},"2093":{"dur":2,"text":"You don't want to get\neven close to any of them."},"2096":{"dur":5,"text":"So you want to maintain\nyour private body space."},"2101":{"dur":4,"text":"And what the blue\nrobot here is doing"},"2106":{"dur":4,"text":"is, in fact, modeling the\nconsequences of its actions"},"2111":{"dur":2,"text":"and the other ones within\nthis radius of attention."},"2114":{"dur":2,"text":"So this blue circle is\na radius of attention."},"2116":{"dur":4,"text":"So here, we're looking at a\nsimple attention mechanism."},"2121":{"dur":3,"text":"Which is only worry about\nthe other dynamic actors"},"2124":{"dur":3,"text":"within your radius of attention."},"2127":{"dur":2,"text":"In fact, we don't even worry\nabout ones that are behind us."},"2130":{"dur":2,"text":"It's only the ones that are\nmore or less in front of us."},"2133":{"dur":5,"text":"And you can see that the\nrobot does eventually make it"},"2138":{"dur":1,"text":"to the end of the corridor."},"2140":{"dur":4,"text":"But with lots of kind\nof stops and back tracks"},"2144":{"dur":2,"text":"in order to prevent it\nfrom-- because it's really"},"2147":{"dur":3,"text":"frightened of any kind of\ncontact with the other robots."},"2153":{"dur":5,"text":"And here, we're not\nshowing all of the sort"},"2159":{"dur":1,"text":"of filaments of prediction."},"2161":{"dur":1,"text":"Only the ones that are chosen."},"2168":{"dur":4,"text":"And here are some results\nwhich interestingly show us--"},"2173":{"dur":6,"text":"so perhaps the best one to\nlook at is this danger ratio."},"2179":{"dur":7,"text":"And dumb simply means robots\nwith no internal model at all."},"2186":{"dur":4,"text":"And intelligent means\nrobots with internal models."},"2191":{"dur":4,"text":"So here, the danger ratio\nis the number of times"},"2195":{"dur":3,"text":"that you actually come\nclose to another robot."},"2198":{"dur":2,"text":"And of course it's very high."},"2201":{"dur":2,"text":"This is simulated\nin real robots."},"2204":{"dur":3,"text":"Very good correlation between\nthe real and simulated."},"2207":{"dur":2,"text":"And with the intelligent\nrobot, the robot"},"2210":{"dur":3,"text":"with the internal model, we\nget a really very much safer"},"2213":{"dur":1,"text":"performance."},"2215":{"dur":3,"text":"Clearly there is some\ncost in the sense"},"2219":{"dur":4,"text":"that, for instance,\nthe intelligent robot"},"2223":{"dur":3,"text":"runs with internal models\ntend to cover more ground."},"2226":{"dur":3,"text":"But surprisingly, not that\nmuch further distance."},"2230":{"dur":1,"text":"It's less than you'd expect."},"2232":{"dur":2,"text":"And clearly, there's\na computational cost."},"2234":{"dur":2,"text":"Because the computational\ncost of simulating clearly"},"2237":{"dur":4,"text":"is zero for the dumb robots,\nwhereas it's quite high for"},"2242":{"dur":3,"text":"the intelligent robot, the\nrobot with internal models."},"2245":{"dur":3,"text":"But again, computation is\nrelatively free these days."},"2249":{"dur":2,"text":"So actually, we're\ntrading safety"},"2251":{"dur":4,"text":"for computation, which I\nthink is a good trade off."},"2255":{"dur":5,"text":"So really, I want\nto conclude there."},"2260":{"dur":2,"text":"I've not, of course,\ntalked about all aspects"},"2263":{"dur":1,"text":"of robot intelligence."},"2265":{"dur":1,"text":"That would be a\nthree hour seminar."},"2267":{"dur":3,"text":"And even then, I wouldn't\nbe able to cover it all."},"2270":{"dur":3,"text":"But what I hope I've shown\nyou in the last few minutes"},"2273":{"dur":4,"text":"is that with internal\nmodels, we have"},"2278":{"dur":2,"text":"a very powerful generic\narchitecture which we could"},"2281":{"dur":3,"text":"call a functional imagination."},"2284":{"dur":3,"text":"And this is where I'm being\na little bit speculative."},"2288":{"dur":2,"text":"Perhaps this moves\nus in the direction"},"2290":{"dur":4,"text":"of artificial theory of mind,\nperhaps even self-awareness."},"2294":{"dur":2,"text":"I'm not going to use the\nword machine consciousness."},"2296":{"dur":0,"text":"Well, I just have."},"2297":{"dur":6,"text":"But that's a very much more\ndifficult goal, I think."},"2303":{"dur":2,"text":"And I think there\nis practical value,"},"2306":{"dur":4,"text":"I think there's real practical\nvalue in robotics of robots"},"2311":{"dur":3,"text":"with self and other simulation."},"2314":{"dur":2,"text":"Because as I hope\nI've demonstrated,"},"2317":{"dur":3,"text":"at least in a kind of prototype\nsense, proof of concept,"},"2321":{"dur":4,"text":"that such simulation moves\nus towards safer and possibly"},"2325":{"dur":4,"text":"ethical systems in\nunpredictable environments"},"2330":{"dur":2,"text":"with other dynamic actors."},"2332":{"dur":2,"text":"So thank you very much\nindeed for listening."},"2335":{"dur":3,"text":"I'd obviously be delighted\nto take any questions."},"2338":{"dur":1,"text":"Thank you."},"2339":{"dur":3,"text":"[APPLAUSE]"},"2345":{"dur":2,"text":"HOST: Thank you very much for\nthis very fascinating view"},"2348":{"dur":1,"text":"on robotics today."},"2349":{"dur":1,"text":"We have time for questions."},"2351":{"dur":2,"text":"Please wait until you've\ngot a microphone so we have"},"2354":{"dur":1,"text":"the answer also on the video."},"2360":{"dur":5,"text":"AUDIENCE: The game playing\ncomputers-- or perhaps more"},"2365":{"dur":3,"text":"accurately would be saying\ngame playing algorithms,"},"2369":{"dur":4,"text":"predated the examples\nyou listed as computers"},"2373":{"dur":1,"text":"with internal models."},"2375":{"dur":2,"text":"Still, you didn't mention those."},"2377":{"dur":4,"text":"Is there a particular\nreason why you didn't?"},"2381":{"dur":2,"text":"ALAN WINFIELD: I guess I\nshould have mentioned them."},"2384":{"dur":0,"text":"You're quite right."},"2385":{"dur":2,"text":"I mean, the-- what\nI'm thinking of here"},"2387":{"dur":4,"text":"is particularly robots\nwith explicit simulations"},"2392":{"dur":2,"text":"of themselves and the world."},"2394":{"dur":3,"text":"So I was limiting my examples\nto simulations of themselves"},"2397":{"dur":0,"text":"in the world."},"2398":{"dur":1,"text":"I mean, you're quite\nright that of course"},"2400":{"dur":4,"text":"game-playing algorithms need to\nhave a simulation of the game."},"2405":{"dur":2,"text":"And quite likely,\nof the-- in fact,"},"2407":{"dur":5,"text":"certainly, of the possible\nmoves of the opponent,"},"2413":{"dur":4,"text":"as well as the game-playing\nAI's own moves."},"2417":{"dur":0,"text":"So you're quite right."},"2418":{"dur":2,"text":"I mean, it's a different\nkind of simulation."},"2421":{"dur":1,"text":"But I should include that."},"2423":{"dur":0,"text":"You're right."},"2426":{"dur":1,"text":"AUDIENCE: Hi there."},"2427":{"dur":6,"text":"In your simulation, you had\nthe H-robot with one goal,"},"2434":{"dur":2,"text":"and the A-robot with\na different goal."},"2436":{"dur":2,"text":"And they interacted\nwith each other halfway"},"2439":{"dur":1,"text":"through the goals."},"2441":{"dur":3,"text":"What happens when they\nhave the same goal?"},"2445":{"dur":1,"text":"ALAN WINFIELD: The same goal."},"2446":{"dur":3,"text":"AUDIENCE: Reaching the\nsame spot, for example."},"2450":{"dur":2,"text":"ALAN WINFIELD: I don't\nknow is the short answer."},"2455":{"dur":4,"text":"It depends on whether that\nspot is a safe spot or not."},"2459":{"dur":4,"text":"I mean, if it's a safe spot,\nthen they'll both go toward it."},"2464":{"dur":3,"text":"They'll both reach it,\nbut without crashing"},"2467":{"dur":1,"text":"into each other."},"2468":{"dur":2,"text":"Because the A-robot\nwill make sure"},"2471":{"dur":2,"text":"that it avoids the H-robot."},"2474":{"dur":1,"text":"In fact, that's\nmore or less what's"},"2475":{"dur":2,"text":"happening in the\ncorridor experiment."},"2478":{"dur":0,"text":"Yeah."},"2479":{"dur":2,"text":"But it's a good question,\nwe should try that."},"2493":{"dur":1,"text":"AUDIENCE: The\nsimulation that you"},"2495":{"dur":2,"text":"did for the corridor experiment,\nthe actual real world"},"2498":{"dur":4,"text":"experiment, the simulation\ntrack the other robots"},"2502":{"dur":0,"text":"movements as well?"},"2503":{"dur":1,"text":"Meaning what information\ndid the simulation"},"2504":{"dur":3,"text":"have that it began with,\nversus what did it perceive?"},"2508":{"dur":1,"text":"Because, I mean, the\nother robots are moving."},"2509":{"dur":1,"text":"And in the real\nworld, they might not"},"2511":{"dur":1,"text":"move as you predict them to be."},"2513":{"dur":4,"text":"How did the blue robot\nactually know for each step"},"2517":{"dur":1,"text":"where the robots were?"},"2518":{"dur":1,"text":"ALAN WINFIELD: Sure."},"2520":{"dur":1,"text":"That's a very good question."},"2522":{"dur":2,"text":"In fact we cheated,\nin the sense that we"},"2525":{"dur":2,"text":"used-- for the real\nrobot experiments,"},"2527":{"dur":3,"text":"we used a tracking system."},"2530":{"dur":6,"text":"Which means that essentially\nthe robot with an internal model"},"2536":{"dur":2,"text":"has access to the position."},"2539":{"dur":4,"text":"It's like a GPS,\ninternal GPS system."},"2544":{"dur":2,"text":"But in a way, that's\nreally just a kind"},"2546":{"dur":6,"text":"of-- it's kind of cheating,\nbut even a robot with a vision"},"2553":{"dur":3,"text":"system would be able\nto track all the robots"},"2557":{"dur":2,"text":"in its field of vision."},"2559":{"dur":4,"text":"And as for the second\npart of your question,"},"2563":{"dur":3,"text":"our kind of model of what\nthe other robot that would do"},"2567":{"dur":2,"text":"Which is it's kind\nof a ballistic model."},"2570":{"dur":3,"text":"Which is if a robot is\nmoving at a particular speed"},"2573":{"dur":1,"text":"in a particular\ndirection, then we"},"2575":{"dur":3,"text":"assume it will continue to\ndo so until it encounters"},"2579":{"dur":0,"text":"an obstacle."},"2582":{"dur":3,"text":"So very simple kind\nof ballistic model."},"2586":{"dur":5,"text":"Which even for humans is useful\nfor very simple behaviors,"},"2591":{"dur":1,"text":"like moving in a crowded space."},"2600":{"dur":2,"text":"Oh hi."},"2602":{"dur":2,"text":"AUDIENCE: In the\nsame experiment--"},"2605":{"dur":2,"text":"it's a continuation of\nthe previous question."},"2607":{"dur":2,"text":"So in between some of\nthe red robots, how"},"2610":{"dur":4,"text":"changed their direction\nrandomly-- I guess so."},"2615":{"dur":4,"text":"Does the internal model of\nthe blue robot consider that?"},"2619":{"dur":1,"text":"ALAN WINFIELD: Not explicitly."},"2620":{"dur":5,"text":"But it does in the\nsense that because it's"},"2625":{"dur":3,"text":"pre- or re-initializing it's\ninternal model every half"},"2629":{"dur":4,"text":"a second, then if the positions\nand directions of the actors"},"2634":{"dur":2,"text":"in its environment\nare changed, then they"},"2636":{"dur":3,"text":"will reflect the new positions."},"2640":{"dur":2,"text":"AUDIENCE: Not exactly\nthe positions."},"2642":{"dur":3,"text":"But as you said, you have\nconsidered the ballistic motion"},"2646":{"dur":1,"text":"of the objects."},"2647":{"dur":4,"text":"So if there is any randomness\nin the environment-- so"},"2651":{"dur":2,"text":"does the internal\nmodel of the blue robot"},"2654":{"dur":2,"text":"consider the\nrandomness, and change"},"2656":{"dur":2,"text":"the view of the red robots?"},"2659":{"dur":3,"text":"It's like it views the red\nrobot as a ballistic motion."},"2662":{"dur":3,"text":"So does it change its\nview of the red robot"},"2666":{"dur":4,"text":"that red robots more in\nthe ballistic motion?"},"2670":{"dur":2,"text":"ALAN WINFIELD: Well, it's\na very good question."},"2673":{"dur":1,"text":"I think the answer is no."},"2674":{"dur":5,"text":"I think we're probably assuming\na more or less deterministic"},"2680":{"dur":3,"text":"model of the world."},"2683":{"dur":3,"text":"Deterministic, yes, I think\npretty much deterministic."},"2686":{"dur":2,"text":"But we're relying\non the fact that we"},"2688":{"dur":3,"text":"are updating and\nrerunning the model,"},"2692":{"dur":3,"text":"reinitializing and rerunning\nthe model every half a second,"},"2695":{"dur":3,"text":"to, if you like, track\nthe stochasticity which is"},"2699":{"dur":3,"text":"inevitable in the real world."},"2702":{"dur":4,"text":"We probably do need to\nintroduce some stochasticity"},"2707":{"dur":2,"text":"into the internal model, yes."},"2709":{"dur":1,"text":"But not yet."},"2711":{"dur":0,"text":"AUDIENCE: Thank you."},"2712":{"dur":1,"text":"ALAN WINFIELD: But\nvery good question."},"2717":{"dur":1,"text":"AUDIENCE: Hello."},"2718":{"dur":2,"text":"With real life applications\nwith this technology,"},"2720":{"dur":3,"text":"like driverless\ncars, for example,"},"2724":{"dur":3,"text":"I think it becomes a lot more\nimportant how you program"},"2727":{"dur":2,"text":"the robots in terms of ethics."},"2729":{"dur":4,"text":"So I mean, there could be\ndilemma like if the robot has"},"2733":{"dur":2,"text":"a choice between saving\na school bus full of kids"},"2736":{"dur":5,"text":"versus one driver, that logic\nneeds to be programmed, right?"},"2741":{"dur":3,"text":"And you made a distinction\nbetween being an engineer"},"2744":{"dur":2,"text":"yourself and then had\nbeen an ethicist earlier."},"2747":{"dur":3,"text":"So to what extent\nis the engineer"},"2751":{"dur":1,"text":"responsible in that case?"},"2753":{"dur":4,"text":"And also does a project\nlike this in real life"},"2757":{"dur":1,"text":"always require the ethicist?"},"2759":{"dur":2,"text":"How do you see this field\nin real life applications"},"2762":{"dur":1,"text":"evolving?"},"2763":{"dur":0,"text":"ALAN WINFIELD: Sure."},"2764":{"dur":2,"text":"That's a really great question."},"2766":{"dur":4,"text":"I mean, you're right that\ndriverless cars will-- well,"},"2771":{"dur":4,"text":"it's debatable whether they will\nhave to make such decisions."},"2775":{"dur":4,"text":"But many people think they will\nhave to make such decisions."},"2779":{"dur":3,"text":"Which are kind of the driverless\ncar equivalent of the trolley"},"2783":{"dur":4,"text":"problem, which is a well-known\nkind of ethical dilemma thought"},"2787":{"dur":1,"text":"experiment."},"2789":{"dur":5,"text":"Now my view is\nthat the rules will"},"2795":{"dur":3,"text":"need to be decided not by the\nengineers, but if you like,"},"2799":{"dur":2,"text":"by the whole of society."},"2801":{"dur":3,"text":"So ultimately, the\nrules that decide"},"2805":{"dur":3,"text":"how the driverless\ncar should behave"},"2809":{"dur":3,"text":"under these difficult\ncircumstances, impossible,"},"2813":{"dur":3,"text":"in fact,\ncircumstances-- and even"},"2816":{"dur":3,"text":"if we should, in fact, program\nthose rules into the car."},"2819":{"dur":4,"text":"So some people argue that the\ndriverless cars should not"},"2824":{"dur":6,"text":"attempt to, as it were,\nmake a rule driven decision."},"2830":{"dur":2,"text":"But just leave it to chance."},"2833":{"dur":2,"text":"And again, I think\nthat's an open question."},"2835":{"dur":5,"text":"But this is really why I\nthink this dialogue and debate"},"2840":{"dur":7,"text":"and conversations with\nregulators, lawyers, ethicists,"},"2847":{"dur":3,"text":"and the general public,\nusers of driverless cars,"},"2851":{"dur":2,"text":"I think is why we need\nto have this debate."},"2853":{"dur":2,"text":"Because whatever those\nrules are, and even"},"2856":{"dur":2,"text":"whether we have them\nor not, is something"},"2858":{"dur":5,"text":"that should be decided,\nas it were, collectively."},"2864":{"dur":3,"text":"I mean, someone\nasked me last week,"},"2867":{"dur":3,"text":"should you be able to alter the\nethics of your own driverless"},"2871":{"dur":0,"text":"car?"},"2872":{"dur":2,"text":"My answer is absolutely not."},"2874":{"dur":1,"text":"I mean, that should be illegal."},"2876":{"dur":3,"text":"So I think that if\ndriverless cars were"},"2879":{"dur":1,"text":"to have a set of\nrules, and especially"},"2881":{"dur":3,"text":"if those rules had numbers\nassociated with them."},"2884":{"dur":3,"text":"I mean, let's think of\na less emotive example."},"2888":{"dur":6,"text":"Imagine a driverless car and\nan animal runs into the road."},"2894":{"dur":6,"text":"Well, , the driverless car\ncan either ignore the animal"},"2901":{"dur":5,"text":"and definitely kill the animal,\nor it could try and brake,"},"2906":{"dur":4,"text":"possibly causing harm to the\ndriver or the passengers."},"2910":{"dur":3,"text":"But effectively\nreducing the probability"},"2914":{"dur":1,"text":"of killing the animal."},"2915":{"dur":3,"text":"So there's an example\nwhere you have some numbers"},"2918":{"dur":3,"text":"to tweak if you\nlike, parameters."},"2922":{"dur":3,"text":"So if these rules are\nbuilt into driverless cars,"},"2926":{"dur":1,"text":"they'll be parameterized."},"2928":{"dur":2,"text":"And I think it\nshould be absolutely"},"2930":{"dur":6,"text":"illegal to hack those\nparameters, to change them."},"2936":{"dur":2,"text":"In the same way that it's\nprobably illegal right now"},"2939":{"dur":4,"text":"to hack an aircraft autopilot."},"2943":{"dur":2,"text":"I suspect that\nprobably is illegal."},"2946":{"dur":1,"text":"If it isn't, it should be."},"2948":{"dur":2,"text":"So I think that you\ndon't need to go"},"2951":{"dur":3,"text":"far down this line of\nargument before realizing"},"2954":{"dur":4,"text":"that the regulation\nand legislation has"},"2959":{"dur":1,"text":"to come into play."},"2960":{"dur":6,"text":"In fact, I saw a piece in\njust this morning in Wired"},"2967":{"dur":5,"text":"that, I think, in the US,\nregulation for driverless cars"},"2972":{"dur":2,"text":"is now on the table."},"2974":{"dur":1,"text":"Which is absolutely right."},"2976":{"dur":3,"text":"I mean, we need to have\nregulatory framework,"},"2980":{"dur":4,"text":"or what I call governance\nframeworks for driverless cars."},"2984":{"dur":2,"text":"And in fact, lots of\nother autonomous systems."},"2986":{"dur":1,"text":"Not just driverless cars."},"2987":{"dur":3,"text":"But great question, thank you."},"2991":{"dur":2,"text":"AUDIENCE: In the experiment\nwith the corridor,"},"2994":{"dur":2,"text":"you always assume-- even in the\nother experiments-- you always"},"2997":{"dur":2,"text":"assume that the main actor\nis the most intelligent"},"2999":{"dur":1,"text":"and the others are not."},"3000":{"dur":1,"text":"Like they're dumb,\nor like they're"},"3001":{"dur":2,"text":"ballistic models\nor linear models."},"3003":{"dur":2,"text":"Have you tried doing\na similar experiment"},"3006":{"dur":5,"text":"in which still each actor\nis intelligent but assumes"},"3012":{"dur":1,"text":"that the others are not,\nbut actually everyone"},"3013":{"dur":0,"text":"is intelligent?"},"3014":{"dur":2,"text":"So like everyone is a\nblue dot in the experiment"},"3017":{"dur":1,"text":"with the model that you have."},"3019":{"dur":2,"text":"And also, have you considered\nchanging the model so that he"},"3021":{"dur":2,"text":"assumes that the others\nhave the same model"},"3023":{"dur":2,"text":"that that particular\nactor has, as well."},"3026":{"dur":1,"text":"[INTERPOSING VOICES]"},"3027":{"dur":2,"text":"ALAN WINFIELD: No, we're\ndoing it right now."},"3029":{"dur":2,"text":"So we're doing that\nexperiment right now."},"3032":{"dur":2,"text":"And if you ask me\nback in a year,"},"3035":{"dur":4,"text":"perhaps I can tell you what\nhapp-- I mean, it's really mad."},"3039":{"dur":2,"text":"But it does take us\ndown this direction"},"3041":{"dur":4,"text":"of artificial theory of mind."},"3046":{"dur":3,"text":"So if you have several\nrobots or actors,"},"3049":{"dur":3,"text":"each of which is modeling\nthe behavior of the other,"},"3053":{"dur":5,"text":"then you get-- I\nmean, some of the--"},"3059":{"dur":2,"text":"I don't even have a\nmovie to show you."},"3061":{"dur":3,"text":"But in simulation\nwe've tried this"},"3065":{"dur":5,"text":"where we have two robots which\nare kind of like-- imagine,"},"3070":{"dur":1,"text":"this happens to all of\nus, you're walking down"},"3072":{"dur":4,"text":"the pavement and you do\nthe sort of sidestep dance"},"3077":{"dur":2,"text":"with someone who's\ncoming towards you."},"3079":{"dur":1,"text":"And so the research\nquestion that we're"},"3081":{"dur":2,"text":"asking ourselves is do\nwe get the same thing."},"3084":{"dur":2,"text":"And it seems to be that we do."},"3086":{"dur":3,"text":"So if the robots are\nsymmetrical, in other words,"},"3090":{"dur":2,"text":"they're each modeling\nthe other, then"},"3092":{"dur":6,"text":"we can get these kind of little\ninteresting dances, where each"},"3098":{"dur":2,"text":"is trying to get out of the\nway of the other, but in fact,"},"3100":{"dur":2,"text":"choosing in a\nsense the opposite."},"3103":{"dur":2,"text":"So one chooses to step right,\nthe other chooses to step left,"},"3106":{"dur":3,"text":"and they still can't\ngo past each other."},"3109":{"dur":1,"text":"But it's hugely interesting."},"3111":{"dur":1,"text":"Yes, hugely interesting."},"3117":{"dur":2,"text":"AUDIENCE: Hi."},"3119":{"dur":1,"text":"I think it's really\ninteresting how"},"3121":{"dur":2,"text":"you point out the\nimportance of simulations"},"3123":{"dur":1,"text":"and internal models."},"3125":{"dur":2,"text":"But I feel that one thing\nthat is slightly left out"},"3128":{"dur":4,"text":"there is a huge gap from going\nfrom simulation to real world"},"3132":{"dur":1,"text":"robots, for example."},"3133":{"dur":3,"text":"And I assume that\nin these simulations"},"3136":{"dur":4,"text":"you kind of assume that the\nsensors are 100% reliable."},"3141":{"dur":2,"text":"And that's obviously\nnot the case in reality."},"3143":{"dur":3,"text":"And especially if we're talking\nabout autonomous cars or robots"},"3147":{"dur":1,"text":"and safety."},"3149":{"dur":2,"text":"How do you calculate\nthe uncertainty"},"3152":{"dur":1,"text":"that comes with the\nsensors in the equation?"},"3154":{"dur":0,"text":"ALAN WINFIELD: Sure."},"3155":{"dur":2,"text":"I mean, this is a deeply\ninteresting question."},"3157":{"dur":2,"text":"And the short answer\nis I don't know."},"3159":{"dur":1,"text":"I mean, this is all future work."},"3165":{"dur":3,"text":"I mean, my instinct\nis that a robot"},"3168":{"dur":3,"text":"with a simulation,\ninternal simulation,"},"3171":{"dur":6,"text":"even if that simulation\nin a sense is idealized,"},"3178":{"dur":2,"text":"is still probably going to be\nsafer than a robot that has"},"3180":{"dur":2,"text":"no internal simulation at all."},"3183":{"dur":6,"text":"And you know, I think we humans\nhave multiple simulations"},"3189":{"dur":1,"text":"running all the time."},"3191":{"dur":3,"text":"So I think we have sort of quick\nand dirty, kind of low fidelity"},"3194":{"dur":4,"text":"simulations when we\nneed to move fast."},"3199":{"dur":4,"text":"But clearly when you need\nto plan something, plan"},"3203":{"dur":4,"text":"some complicated\naction, like where"},"3207":{"dur":1,"text":"you are going to go\non holiday next year,"},"3209":{"dur":4,"text":"you clearly don't use the same\ninternal model, same simulation"},"3213":{"dur":3,"text":"as for when you try\nand stop someone"},"3216":{"dur":2,"text":"from running into the road."},"3219":{"dur":3,"text":"So I think that future\nintelligent robots"},"3223":{"dur":3,"text":"will need also to have\nmultiple simulators."},"3226":{"dur":3,"text":"And also strategies\nfor choosing which"},"3230":{"dur":4,"text":"fidelity simulator to\nuse at a particular time."},"3234":{"dur":3,"text":"And if a particular\nsituation requires"},"3238":{"dur":3,"text":"that you need high\nfidelity, then for instance,"},"3242":{"dur":1,"text":"one of the things\nthat you could do,"},"3244":{"dur":2,"text":"which actually I think\nhumans probably do,"},"3246":{"dur":3,"text":"is that you simply move more\nslowly to give your self time."},"3249":{"dur":2,"text":"Or even stop to\ngive yourself time"},"3252":{"dur":1,"text":"to figure out what's going on."},"3254":{"dur":2,"text":"And in a sense,\nplan your strategy."},"3257":{"dur":8,"text":"So I think even with the\ncomputational power we have,"},"3266":{"dur":3,"text":"there will still be a\nlimited simulation budget."},"3269":{"dur":2,"text":"And I suspect that\nthat simulation budget"},"3271":{"dur":1,"text":"will mean that in\nreal time, when you're"},"3273":{"dur":5,"text":"doing this in real time, you\nprobably can't run your highest"},"3278":{"dur":1,"text":"fidelity simulator."},"3280":{"dur":7,"text":"And taking into account all\nof those probabilistic, noisy"},"3287":{"dur":3,"text":"sensors and actuators\nand so on, you probably"},"3290":{"dur":2,"text":"can't run that\nsimulator all the time."},"3293":{"dur":2,"text":"So I think we're\ngoing to have to have"},"3295":{"dur":3,"text":"a nuanced approach where we\nhave perhaps multiple simulators"},"3299":{"dur":1,"text":"with multiple fidelities."},"3300":{"dur":2,"text":"Or maybe a sort of\ntuning, where you can tune"},"3303":{"dur":3,"text":"the fidelity of your simulator."},"3306":{"dur":2,"text":"So this is kind of a\nnew area of research."},"3308":{"dur":2,"text":"I don't know anybody who's\nthinking about this yet, apart"},"3311":{"dur":1,"text":"from ourselves."},"3312":{"dur":2,"text":"So great."},"3314":{"dur":2,"text":"AUDIENCE: [INAUDIBLE]."},"3317":{"dur":3,"text":"ALAN WINFIELD: It\nis pretty hard, yes."},"3320":{"dur":0,"text":"Please."},"3321":{"dur":2,"text":"AUDIENCE: [INAUDIBLE]."},"3323":{"dur":1,"text":"ALAN WINFIELD: Do you\nwant the microphone?"},"3325":{"dur":0,"text":"Sorry."},"3328":{"dur":2,"text":"AUDIENCE: Have you considered\nthis particular situation"},"3330":{"dur":1,"text":"where there are\ntwo Asimov robots--"},"3332":{"dur":4,"text":"and that would be an extension\nof the question that he asked."},"3336":{"dur":2,"text":"So for example, if there\nare two guys walking"},"3339":{"dur":2,"text":"on a pavement and there\ncould be a possibility"},"3342":{"dur":1,"text":"of mutual cooperation."},"3344":{"dur":2,"text":"As in one might communicate\nwhether that I might step out"},"3347":{"dur":1,"text":"of this place and you might go."},"3348":{"dur":1,"text":"And then I'll go after that."},"3350":{"dur":2,"text":"So if there are\ntwo Asimov robots,"},"3352":{"dur":1,"text":"will there be a\npossibility, and have"},"3353":{"dur":2,"text":"you considered this fact\nthat both will communicate"},"3356":{"dur":2,"text":"with each other, and they will\neventually come to a conclusion"},"3359":{"dur":4,"text":"that I will probably walk,\nand the other will get out"},"3363":{"dur":1,"text":"of the way."},"3364":{"dur":1,"text":"And the second part\nof this question"},"3365":{"dur":2,"text":"would be what if\none of the robots"},"3368":{"dur":3,"text":"actually does not\nagree to cooperate?"},"3372":{"dur":2,"text":"I mean, since they both would\nhave different simulators."},"3375":{"dur":1,"text":"They could have\ndifferent simulators."},"3376":{"dur":2,"text":"And one might actually try to\ncommunicate that you step out"},"3379":{"dur":3,"text":"of the way so that I might go."},"3382":{"dur":2,"text":"And the other one\ndoesn't agree with that."},"3384":{"dur":2,"text":"I mean, what would\nthe [INAUDIBLE]."},"3386":{"dur":1,"text":"ALAN WINFIELD: Yeah,\nit's a good question."},"3388":{"dur":3,"text":"In fact, we've actually\ngotten a new paper which"},"3391":{"dur":3,"text":"we're just writing right now."},"3395":{"dur":5,"text":"And the sort of working\ntitle is \"The Dark Side"},"3400":{"dur":2,"text":"of Ethical Robots.\""},"3402":{"dur":4,"text":"And one of the things that we\ndiscovered-- it's actually not"},"3407":{"dur":4,"text":"surprising-- is that you only\nneed to change one line of code"},"3412":{"dur":5,"text":"for a co-operative robot to\nbecome a competitive robot,"},"3417":{"dur":2,"text":"or even an aggressive robot."},"3420":{"dur":5,"text":"So it's fairly obvious, when\nyou start to think about it,"},"3425":{"dur":4,"text":"if your ethical rules\nare very simply written,"},"3429":{"dur":1,"text":"and are a kind of\nlayer, if you like,"},"3431":{"dur":2,"text":"on top of the rest\nof the architecture,"},"3434":{"dur":2,"text":"then it's not that difficult\nto change those rules."},"3439":{"dur":1,"text":"And yes, we've done\nsome experiments."},"3440":{"dur":2,"text":"And again, I don't have\nany videos to show you."},"3443":{"dur":5,"text":"But they're pretty\ninteresting, showing"},"3448":{"dur":4,"text":"how easy it is to make a\ncompetitive robot, or even"},"3452":{"dur":3,"text":"an aggressive robot\nusing this approach."},"3456":{"dur":3,"text":"In fact, on the BBC\nsix months ago or so,"},"3460":{"dur":3,"text":"I was asked surely if you\ncan make an ethical robot,"},"3463":{"dur":3,"text":"doesn't that mean you can\nmake an unethical robot?"},"3466":{"dur":2,"text":"And the answer,\nI'm afraid, is yes."},"3469":{"dur":3,"text":"It does mean that."},"3472":{"dur":3,"text":"But this really goes back\nto your question earlier,"},"3475":{"dur":2,"text":"which is that it should\nbe-- we should make"},"3478":{"dur":3,"text":"sure it's illegal\nto convert, to turn,"},"3481":{"dur":3,"text":"if you like, to recode\nan ethical robot"},"3484":{"dur":1,"text":"as an unethical robot."},"3486":{"dur":3,"text":"Or even it should be illegal\nto make unethical robots."},"3489":{"dur":0,"text":"Something like that."},"3490":{"dur":1,"text":"But it's a great question."},"3492":{"dur":4,"text":"And short answer, yes."},"3496":{"dur":2,"text":"And yes, we have some\ninteresting new results,"},"3498":{"dur":5,"text":"new paper on, as it\nwere, unethical robots."},"3504":{"dur":1,"text":"Yeah."},"3505":{"dur":2,"text":"HOST: All right, we are\nrunning out of time now."},"3508":{"dur":1,"text":"Thanks everyone\nfor coming today."},"3509":{"dur":1,"text":"Thanks, Professor Alan Winfield."},"3511":{"dur":1,"text":"ALAN WINFIELD: Thank you."},"3512":{"dur":1,"text":"[APPLAUSE]"}}